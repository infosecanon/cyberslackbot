Every day, all around the world, thousands of IT systems are compromised.
Some are attacked purely for the kudos of doing so, others for political motives,
but most commonly they are attacked to steal money or commercial secrets.
Are you confident that your cyber security governance regime
minimizes the risks of this happening to your business?
And if your company doesn't have it right, your IT systems may have already
been compromised, attackers could already have your new product plans,
bidding positions or research; they may already be running your process
control systems. Are you confident that this has not already happened to
your business?
Whatever business we are in, we all rely on the internet. We research and
develop on it; bid and sell on it; communicate with our customers on it and rely
upon it for our logistical support. Put simply, the internet brings immeasurable
benefits. But, we cannot escape the fact that it also brings new risks.
About 80 per cent of known attacks would be defeated by embedding basic
information security practices for your people, processes and technology.
This guidance is about getting those basics right; where companies adopt
these steps it has made a tangible difference to their vulnerability to cyber
attack.
We've seen determined and successful efforts to: steal intellectual property;
take commercially sensitive data, such as key negotiating positions; access government and defense related information; disrupt government and industry service; and, exploit information security weaknesses through the targeting of partners, subsidiaries and supply chains at home and abroad. The magnitude and tempo of these attacks, basic or sophisticated, on UK and
global networks pose a real threat to the UK's economic security. The mitigation of these risks and management of these threats - in other words, cyber security - is one of the biggest challenges we all face today.

The technical level of cyber attacks is growing exponentially. What was
considered a sophisticated cyber attack only a year ago might now be
incorporated into a downloadable and easy to deploy internet application,
requiring little or no expertise to use.
Responsibility to manage your company's cyber risks starts and stops at
Board level. You can never be totally safe. Risks will, at times, become reality.
To that end this guidance is designed to offer some practical steps which you,
as leaders, can direct to be taken to improve the protection of your networks
and the information carried upon them. Value, Revenue and Credibility are
at stake. Don't let cyber security become the agenda - put it on the
agenda.

Many different groups may pose a threat to a company's information assets:
criminals interested in making money through fraud, operating not just as
individuals but often in well-organized groups based in hard-to-reach
jurisdictions; industrial competitors and foreign intelligence services
interested in gaining competitive advantage for their own companies or
countries; hackers who revel in the challenge of penetrating and disrupting
computer systems and hacktivists who cause disruption for political or
ideological reasons. And the insider threat should not be overlooked:
potentially the actions of a company's own employees pose a risk, whether
careless or malicious.

“Cyberspace is an interactive domain made up of digital networks that is
used to store, modify and communicate information. It includes the
internet, but also the other information systems that support our
businesses, infrastructure and services.”
Common usage of the term also refers to the virtual environment of information and
interactions between people.

Information is critical to today's business.
Information and the ICT (Information and Communication Technologies) that
store and process it are critical to business success. Your intellectual property,
confidential or sensitive information provide competitive advantage, whether
in the form of a product design, a manufacturing process or a negotiating
strategy. At the same time the need to access and share information more
widely, using a broad range of connecting technologies is increasing the risk to
the corporate information base. Compromise of information assets can damage companies.
Compromise of information through, for example, staff error or the deliberate
actions of an outsider could have a permanent or at least long-term impact on a
business. A single successful attack could destroy a company's financial
standing or reputation. Information compromise can lead to material financial
loss through loss of productivity, of intellectual property, reputational damage,
recovery costs, investigation time, regulatory and legal costs. This could lead
to reduced competitive advantage, lower market share, impact on profits,
adverse media coverage, bankruptcy, or even, where safety-critical systems
may be concerned, loss of life.
In addition to an accurate picture of those information assets that are critical to
business success, Boards will wish to reassure themselves that they have
regular up to date information on the threats and known business
vulnerabilities to make informed information risk decisions.
We can all name companies whose cyber security has been very publicly
compromised: where it has happened, this has caused tangible damage.
What makes your business immune to such attacks?

The threat is not only technical.
Many attempts to compromise information involve what is known as social
engineering, or the skillful manipulation of people and human nature. It is often
easier to trick someone into clicking on a malicious link in an email that they
think is from a friend or colleague than it is to hack into a system, particularly if
the recipient of the email is busy or distracted. And there are many well
documented cases of hackers persuading IT support staff to open up areas of
a network or reset passwords, simply by masquerading as someone else over
the phone.
The key is effective enterprise-wide risk management and awareness
Being aware of potential threats is a normal part of risk management across
the private sector. Alongside financial, legal, HR and other business risks,
companies need to consider what could threaten their critical information
assets and what the impact would be if those assets were compromised in
some way. The key is mitigating the majority of risks to critical information
assets and being better able to reduce the impact of and recover from
problems as they arise.
What is information?
Information, whether financial or about people and systems, is the
lifeblood of any organization. Yet, with increasing automation and
interconnectivity of information systems, a compromise in one area
could impact the entire organization and its customers. Information is
everywhere from customer facing systems (ATMs, points of sale, mobile
phones), to business systems (research data and other intellectual
property, management and customer relationship information) and
operational systems (safety, protection, process control). When
identifying information assets, all of these different areas need to be
taken into consideration.

Many players pose a risk to information
There are many types of people who pose a risk to business information
assets: cyber criminals interested in making money through fraud or from the
sale of valuable information; industrial competitors and foreign intelligence services, interested in gaining an economic advantage for their own companies or countries;
hackers who find interfering with computer systems an enjoyable
challenge; hacktivists who wish to attack companies for political or ideological
motives; employees, or those who have legitimate access, either by accident or
deliberate misuse.

A major cyber attack may feel like the stuff of popular culture. It's not. Although
many never hit the headlines, such attacks are increasing in prevalence and
scale all the time. The impact of not recognizing and pre-empting cyber risks
can be long term.
Risks to all forms of information should be treated in the same way as
other financial or business risks, especially where threats and
vulnerabilities are constantly changing. Ultimate responsibility for cyber
security rests at Board level, with the correct governance, management
and culture throughout the business. The Board should seek assurance
that key information risks are both assessed and prioritized, and that
there is regular monitoring where threats and vulnerabilities are
constantly changing. The Board should also set the value the company
places on its various information assets such as company pricing data,
business strategies, online services and process control systems, and
communicate this throughout the business.

What information should you protect?
All business activity relies on information in a number of forms; it may be
supporting corporate management decisions, user access to information,
operational networks or process control systems. Review your information
assets and agree which are the most critical to the success and competitive
advantage of your company.
What are the risks to your information and how much risk can you
accept?
Identify the risks to information assets. Assess who has access to those assets
and who may wish to target the company. Consider the circumstances in which
the risks have or could become a reality. Quantify the level of risk to those
assets that the business is willing to accept and communicate your risk
appetite across the business, especially to those who implement and manage
the company's security. Ensure your assessments keep pace with
technological advances, such as Cloud Computing, which may affect the
balance of risk over time.

Ensure that your governance framework encompasses information risk
across the business and apply the same degree of rigor to these risks as
financial and other risk management regimes. Implement security controls
and supporting policies that are commensurate with the level of risk that the
business is willing to tolerate. To support this process we have set out at page
8, ten steps that support a robust information risk and cyber security regime. If
any of these areas are not covered in your framework, is there a sound
business rationale?
Do the security measures work?
Regularly review and test the effectiveness of, and adherence to, current
controls, and investigate any anomalies. Often well intentioned policies
believed to be critical to preventing disaster are not being followed in practice.
This could be down to a lack of training, a culture of complacency or simply
because they are not usable.
What would happen to the business if one of your risks became a reality?
Plan for a worst case scenario. Have robust, regularly tested, incident
management processes and contingency planning in place to recover from
and reduce the impact of any compromises to the business. Understanding
why an attack occurred and what was compromised is critical to recovering
successfully and protecting the business in the future.
How do you embed risk management within your company?
Effectively managing the process of assessing risks and implementing
controls is essential - both in the business and supply chain. The appropriate
people need to be accountable for information and its protection, and should
have the right authority, tools and training to achieve this.
How can you ensure that you have the best possible understanding of
the threat to your business?
Empower selected senior staff to share appropriate information with others in
your and related business sectors, both to help build best practice and warn of
potential upcoming attacks. Report crime to Action Fraud or other relevant
agencies to help law enforcement build a fuller picture of the threat to
business, share the findings and deploy appropriate support.

Basic information risk management can stop up to 80% of the cyber attacks
seen today, allowing companies to concentrate on managing the impact of the
other 20%. We recommend that as a business you take steps to review, and
invest where necessary, to improve security in the following key areas:

Develop a mobile working
policy & train staff to
adhere to it. Apply the
secure baseline build to
all devices. Protect data
both in transit & at rest.

Produce user security
policies covering
acceptable & secure
use of the
organization’s
systems. Establish a staff
training program.
Maintain user awareness
of the cyber risks.

Establish an incident
response & disaster
recovery capability.
Produce & test incident
management plans.
Provide specialist training
to the incident
management team. Report
criminal incidents to law
enforcement.

Information Risk Management Regime
Establish an effective governance structure and determine your risk appetite - just like
you would for any other risk. Maintain the Board’s engagement with the cyber risk.
Produce supporting information risk management policies.

Establish account
management processes
& limit the number of
privileged accounts. Limit
user privileges & monitor
user activity. Control access
to activity & audit logs.

Produce a policy to control
all access to removable
media. Limit media types
& use. Scan all media for
malware before importing
on to corporate system.

Apply security patches &
ensure that the secure
configuration of all ICT
systems is maintained.
Create a system inventory
& define a baseline build
for all ICT devices.

Produce relevant policy &
establish anti-malware
defense that are
applicable & relevant
to all business areas.
Scan for malware across
the organization.

Establish a monitoring
strategy & produce
supporting policies.
Continuously monitor
all ICT systems &
networks. Analyse logs
for unusual activity that
could indicate an attack.

Protect your networks
against external and
internal attack.Manage
the network perimeter.
Filter out unauthorized
access & malicious
content. Monitor & test
security controls.

Many companies across different industry sectors will already have
experienced some form of cyber attack. Whilst the scenarios on the following
pages are illustrative, they are based on events that had real impact on the
companies that experienced them. They are just three examples of the many
hundreds of incidents we see occurring regularly. Application of the 10 steps
provides a comprehensive information risk management framework; however,
for each scenario we have suggested those of particular relevance.
The suggested cyber controls in this booklet cannot prevent all of the most
sophisticated cyber attacks, but they can greatly hinder the vast majority of
attackers; reduce the vulnerability of a particular company; and limit any
impact in the event of a breach. Engagement with peers across your sector,
the wider business community and law enforcement can help you to maintain
an understanding of current and emerging threats.
CPNI facilitates information exchanges which allow one company to learn
from the experiences, mistakes and successes of another, without fear of
exposing company sensitivities. Information exchanges are free to join and
their membership is determined by the existing members.

What happened?
A sales director of a global telecoms company was targeted and had a
corporate laptop stolen whilst attending an overseas conference. Given his
position, he had access to key corporate information, including bid information
for an upcoming major tender. A local copy of a database was stored
unencrypted on the stolen laptop. The perpetrator retrieved this information
easily and was able to pass it on to an unscrupulous competitor.
What was the impact?
The company lost the tender and market share to an overseas competitor and
experienced a steady fall in share price. It was only after a lengthy internal
investigation that the company became aware of the information breach, by
which time it was too late.
How could this have been prevented?
For this scenario we suggest four of the ten steps are of particular relevance to
mitigate the information risk:

A robust mobile working
policy and an encrypted
Virtual Private Network
(VPN) would allow people to
access the office network
without the need for local
copies of databases.

As a business heavily reliant on large industry contracts, the company should
have recognized and valued its contract bid information assets and protected
that data appropriately.

A world leading Biotech company developing the next generation of
pharmaceuticals was ready for a product launch following five years of
research and development representing a £1 billion investment. However,
vulnerable systems and processes allowed it to become the victim of a
sophisticated and targeted cyber attack allowing the attacker to steal the
research.
Eight months before the product launch, the research director received an
email that appeared to be from a colleague with a PDF of a relevant scientific
paper. The email was actually a fake and the PDF attachment contained
malware which exploited a software vulnerability for which the patch, although
available for months, had not been applied. This well-fabricated social
engineering attack allowed the attacker to steal the research and other
sensitive information enabling a foreign competitor to release a cheaper
version of the product onto the market ahead of the UK company.
What was the impact?
The company suffered material financial loss; they faced setbacks in securing
further funding for research and development and lost major contracts to
foreign competitors, who beat them to market with additional products based
on the stolen research.
How could this have been prevented?
For this scenario we suggest five of the ten steps are of particular relevance to
mitigate the information risk:

Active internal
monitoring of valued
assets, log analysis and
web filtering all could
have detected the
attacker's activities.

Staff should have been
trained and understood
the importance of
not clicking on links
or opening suspicious
documents in unsolicited
email.

The Board should have been aware of the value of their Intellectual Property
and the cyber risks to the organization of this type of attack. Appropriate
security controls could then have been put in place.

A security firm with large Government contracts became the victim of a cyber
attack by politically and ideologically motivated hacktivists.
A security flaw in a poorly designed public facing website enabled an attacker
to gain access to an internal database which held passwords. A combination of
weak passwords and poor password security management enabled the
attacker to gain administrator privileges and as a result access to the
company's entire IT estate. The attacker was then further able to exploit
unpatched systems until they had access to the company's internal emails and
sensitive data.
What was the impact?
After a number of sensitive internal emails and data were published publicly,
there was severe reputational damage and a loss of customer confidence in
the company. This led to problems with retaining both current contracts and
securing new ones, ultimately leading to bankruptcy.

How could this have been prevented?
For this scenario we suggest five of the ten steps are of particular relevance to
mitigate the information risk:

Information Risk Management Regime
The board was complacent in assuring that the IT department was
implementing a compliance regime to required standards. A robust corporate
governance regime would have spotted areas of concern before the attack.

Incident management
procedures would have
alerted security to the
intrusion and triggered
procedures to mitigate the
damage and limit future
damage in the event of
further attacks.

Like other corporate risks, cyber risks need to be managed proactively by the
Board, led by senior management and assured by corporate governance. A
model for managing cyber risks is suggested below. Implementation will
clearly need to reflect the nature of your business and your appetite for risk.
Board

Make protecting your
information a Board
responsibility
Have you agreed as a Board what
information assets are critical to your
business?
Have you decided your appetite for
risk with regard to these assets?
Are you proactively managing these
risks as a Board?

Have you ensured that your existing
risk management regime encompasses
information and cyber risk?
Have you implemented appropriate
security policies & controls which you
maintain?
Do you maintain an up-to-date
awareness of threats & vulnerabilities?

Have you assured yourself that you are
managing the right information & cyber
risks for your business?
Are you implementing the right security
controls at the right pace to meet your
risk?
Are you regularly testing, monitoring
and reviewing your security controls?

If you are uncertain about your company's ability to manage its information
risks, here are some practical steps that can be taken through Corporate
Governance mechanisms:
Confirm that you have identified your key information assets and the
impact on your business if they were to be compromised;
Confirm that you have clearly identified the key threats to your
information assets and set an appetite for the associated risks;
Confirm that you are appropriately managing the cyber risks to your
information and have the necessary security policies in place.
Companies may not have all the expertise needed to implement some of these
steps and assure themselves that the measures they have in place meet
today's threats; in the first instance audit partners should be able to provide
assistance. For information risk management expertise, organizations should
seek advice from members of appropriate professional bodies or those who
have attained industry recognized qualifications. Whilst these are primarily aimed at the
public sector, they may be of assistance to the private sector.

Malware is an abbreviation of the words malicious and software. The term refers to software that is deployed with malicious intent. Malware is easy to deploy remotely, and tracking the source of
malware is hard. This combination has enabled commercial malware providers to supply
sophisticated black markets for both malware and the information that it collects. Demand for
sophisticated malware is created primarily by organized crime syndicates and state-sponsored
espionage agents. The financial services industry is a primary target for malware-enabled cyber
attacks because financial institutions (Financial institutions) operate software that tracks ownership of monetary
assets. Cybercriminals also directly target FI customers and business partners using malware-enabled
attacks. This paper is intended to assist financial institutions by promoting awareness and
understanding of the risks and the mitigation activities associated with the use of malware in the
financial industry.

Software-enabled crime is not a new concept. Computer-enabled fraud and service theft evolved
in parallel with the information technology that enabled it. Since the advent of mainframe-based
automated bank account systems, Financial institutions have been victims of malware-based cyber attacks. Criminals
altered software to transfer other people’s money to accounts they controlled, and emptied the
accounts anonymously. As computers were shared on networks, these services experienced service
theft, wherein criminals altered system software to hide reconnaissance activities which enabled theft
of both valuable services and valuable information.

This co-evolution of technology services and cybercrime may have created some confusion in the
general population, for whom attacks on technology do not seem to be as significant as attacks on
physical assets. Those not familiar with the emerging technology itself find it difficult to understand
the implications of software compromise. General confusion over cybercrime objectives is
exacerbated by the element of opportunism in some types of cybercrime, wherein attackers do not
select specific victims, but simply let rogue software loose to find its own targets. This type of
cybercrime appears to some segments of the public as bad luck for the victim rather than as a direct
result of adversarial intent.
Nevertheless, even opportunistic cybercriminals select their targets, if only by selecting the operating
system platform on which malware may be processed. Where the platform is the latest version of an
emerging technology, the selected victim class may be assumed to be those financially able to afford
that new technology. Another selection made by cybercriminals is the specification of data that
malware processes. Where data concerning credit card numbers is sought, the target victim class
includes all credit card holders and associated institutions. Where the data sought is bank account
numbers, all financial firms are targets. The attraction of cybercrime lies in the high return on
investment, low-to-no-risk operating environments, and proliferation of vulnerable computing
resources. The ubiquitous connectedness provided by the Internet has allowed for multiple elements
of the criminal community to operate in tandem to pursue profit driven crime as well as other
malicious activities, using malware.
To the casual observer, headlines about cyber attacks may seem unrelated. Attacks are scattered
across geography and technology. They involve different companies and nationalities. As recently as
five years ago, security standards publications identified malware and phishing attacks as separate
threats. However, today security analysts agree that various types of malware are used in
conjunction. Cooperation and collaboration among cybercriminals have created crime patterns
that evolve in concert with emerging technology, and all users of emerging technology are victims.
There is also evidence that cybercriminals operate in geopolitically-identifiable groups. As one
analyst put it, “the phrase ‘campaign’ is more appropriate than ‘adversary’.”
Malware is typically used to steal information that can be readily monetized, such as login
credentials, credit card and bank account numbers, and intellectual property such as computer
software, financial algorithms, and trade secrets. Although many cybercriminal groups are trafficking
in commodities shared by multiple industry sectors, such as credit card numbers, there are some
situations wherein a single company is obviously the target of a single adversary, whether it be an
organized crime syndicate, nation-state, or a single operative. For example, the work of a single
nation-state adversary was evident to Google upon analysis of its 2009 cyber attack [6]. The extent
to which any given attack lands on one set of companies or customers rather than another depends
on a variety of factors.
Malware creation and distribution channels are described in detail in Section 3. The remainder of
this section describes in general how malware works and how it accomplishes crime.

Malware may take as many forms as software. It may be deployed on desktops, servers, mobile
phones, printers, and programmable electronic circuits. Sophisticated attacks have confirmed data
can be stolen through well written malware residing only in system memory without leaving any
footprint in the form of persistent data. Malware has been known to disable information security
protection mechanisms such as desktop firewalls and anti-virus programs. Some even have the
ability to subvert authentication, authorization, and audit functions. It has configured initialization
files to maintain persistence even after an infected system is rebooted. Upon execution,
sophisticated malware may self-replicate and/or lie dormant until summoned via its command
features to extract data or erase files.
A single piece of malware is generally described by four attributes of its operation:
Propagation: The mechanism that enables malware to be distributed to multiple systems
Infection: The installation routine used by the malware, as well as its ability to remain
installed despite disinfection attempts
Self-Defense: The method used to conceal its presence and resist analysis, these techniques
may also be called anti-reversing capabilities
Capabilities: Software functionality available to malware operator

However, alluded to in the description of a bot is the fact that a typical cybercrime will
require multiple different types of software acting in coordination in order to achieve the full crime
capability. For example, a criminal may use email spamming software (a form of flaw exploit) to
trick a user into downloading a keylogger from an infected website. The criminal would then have to
host a site for the keylogger to deliver the stolen credentials. The criminal would presumably use
software to read and analyze the credentials, and then perhaps use vulnerability scanning software to
see which websites identified by them have flawed software. The criminal may then use the user
name and password to execute flaw exploits against the website. Activities included in each step
are:
Reconnaissance: Criminal surveys the target to identify points of vulnerability, an attack planning phase.
Assembly: Criminal creates, customizes, or otherwise obtains malware to satisfy attack
requirements.
Delivery: Malware propagation occurs.
Compromise: Malware infection occurs.
Command: Malware capabilities are unleashed.

Execution: Malware delivers data to malware operator (exfiltration) or otherwise
accomplishes attack objective.

Although there are a wide variety of words and phrases that the media uses to refer to malware, they
all have their roots in the execution paths illustrated in Table 1 and Figure 1. The specialized
terminology tends to refer to the type of crime perpetrated using the software rather than the
technical description of the attack. For example:
Malvertising: The practice of paying for web advertisements and using them to cause
malware propagation
Ransomware: The use of malware to block access to computers or data until a payment is
made, also continues to be used for extortion purposes
Rogueware: Malware that is written to look and act like legitimate packages, in order to
trick victims into downloading and installing it
Scareware: Malware that is written to look and act like legitimate security anti-virus
packages, in order to trick victims into buying worthless software to fix
nonexistent virus or spyware problems, scareware may be a form of
rogueware
Spearphishing: Phishing attacks directed at wealthy or otherwise singularly attractive targets
with specific knowledge, capability or expertise
Spyware: The use of malware to observe any user activity, including keystrokes and
screenshots, and network connections, typically used to transfer passwords
and credit card numbers to the malware operator

For example, the advent of iFrame technology in web services has enabled a specific brand of
malware. The technology allows a URL to be placed in a web page hosted on server A that displays
content from server B. The user accessing server A does not see the call to server B, as server B’s
content appears displayed in the page rendered by server A. There are a variety of legitimate reasons
why a legitimate website may want to display content from multiple servers simultaneously. There
may be complex specialized algorithms required to display numerical data that is generated in realtime, and so, beyond the CPU capacity of a single web server. There may be business relationships
that require display of partner logos or advertisements from business partner servers. For whatever
reason, the legitimate iFrame feature exists.
The iFrame feature by itself does not enable malware. Criminals take advantage of the feature by
exploiting web server vulnerabilities and inserting their own servers in replacement, or in addition to
a legitimately placed server B (for a full explanation of this vulnerability). Figure 2 illustrates
how the server is modified to set up for a subsequent attack on a web server user. There also are
vulnerabilities in browsers with which users visit sites that have iFrames. The combination of server
and browser vulnerabilities enable malware criminals to use iFrames for malware propagation and
infection. The iFrame-enabled webserver, the code it links to on the malware host site, and the code
that is downloaded to the user when the user accesses the iFrame are different pieces of malware.
They are used in combination to infect the user. Only after the infection takes place for the last of
these pieces, the malware on the end user target, is it fully enabled with self defense and functional
capabilities required to harvest data.

Remediation of modern malware is becoming increasingly more difficult due to several factors.
There are significantly more varieties of malware being found in the wild that exploit zero-day
vulnerabilities. “Zero-day” modifies the word vulnerability to mean that the vulnerability is not
known to potential victims, and so victims have had no days to prepare for it. Malware has also now
been designed with polymorphic capabilities. Polymorphic malware changes certain characteristics
of itself upon each instance or infection. This change can be in the form of a non-functional code
change. This technique circumvents signature-based detection mechanisms because these typically
use a hash algorithm to produce a unique signature from a file containing malware, so any change to
the file will change its signature. Polymorphic malware can also change its own filename on each
infection, and this also makes detection more difficult by traditional means.

The root cause of malware is the black market for stolen information. Data thieves can sell their
spoils in a variety of forums.

In any dynamic marketplace, the prices claimed for a commodity will fluctuate with supply and
demand. In any technology marketplace, prices will also fluctuate with the utility of the commodity,
given changes in technology landscape. Automating malware delivery and data harvesting tasks reduces operating costs and allows malicious perpetrators to obscure their activities. Malware delivery and operations systems
have become increasingly modular, and these modules have themselves become a commodity.
When such malware software support systems are discovered to exist, the software is referred to as
crimeware. Continuing the Zeus malware example, a good example of crimeware
is the Zeus toolkit. Zeus malware was introduced in 2006, and its corresponding crimeware followed
in 2007. Zeus’ crimeware takes advantage of its modular design, so attackers can configure and
deploy new functionality very quickly. A user-friendly graphical interface allows an attacker to select
the capabilities to be incorporated in a “release” as well as to select a personal encryption key for
harvested data. Over 5,000 releases of the Zeus software have been created using Zeus crimeware. Although several Zeus users have been identified and charged with cybercrimes, the Zeus
crimeware authors remain at large.

Malware development and distribution is highly organized and controlled by criminal groups that
have formalized and implemented business models to automate cybercrime. Just as the software
industry has spawned a business model in reselling, installing, and maintaining legitimate code, the
malware industry has spawned distribution and support networks to assist criminals in successful malware usage. Developers of crimeware profit from the sale or lease of the malware to third parties
who then use it to perpetrate identity theft and account fraud. Individual groups of criminals
coordinate their efforts, and the product is Crimeware as a Service (CAAS).

The process depicted in Figure 4 leads with software vulnerabilities being sought by criminals in a
systematic way. The figure begins with “zero-day” vulnerabilities, because these are more valuable to
malware creators because potential victims are unsuspecting. These vulnerabilities are sold to
criminals who engineer malware to exploit the vulnerability, and aggregate multiple malware
vulnerability exploits into kits whose components can be systematically installed as in the iFrame
example. Because many vulnerabilities exist in unpatched systems long after they have
been announced, exploit kits may include combinations of zero-day and older attacks. The kits are
configured to send harvested data to private hosting services, and this configuration may be
customized for a given buyer. Crimeware market makers contact potential customers via email and
chat, agree on prices and sell not just software, but crimeware services. They engage malware
delivery services to operate the malware on behalf of buyers, who pay the market makers via
anonymous ecommerce payment systems.
Crimeware operation is blatantly illegal, yet individual risk of criminal prosecution is minimized by
the overall business model. Each malware profit center has a level of exposure corresponding only
to its role in the overall marketplace. For example, in academic circles, the study of vulnerabilities is
common.

Earnings for malware development are time sensitive but are very low risk. During the lifecycle of
malware, protections are developed to mitigate the risk. To remain competitive and profitable new
malware must be released frequently. Security analysts are seeing dramatic increases in the number
of malware specimens created and distributed. One report claims that a full third of all viruses that
exist were created in 2010. The profit incentive driving these activities creates a persistent risk
for financial institutions.
The supply chain in the malware industry encompasses more than just software. It is an elaborate
collection of organizations, people, technologies, processes, services, and products. Financial
services such as moneygrams, virtual credit cards, and online money transfer services allow
anonymity between buyers and sellers. However, not all of the players in this black market are
criminals. The marketing of malware, crimeware, and associated services and products can be found
on both black market forums and legitimate sales channels. Crimeware operators will use legitimate
online payment services to process purchases and then the payment details are used to facilitate
fraudulent transactions. They will also use legitimate Internet Service Providers (ISP) to host
databases of stolen data.
In addition to its use for criminal purposes, malware also enables other malicious actors that pose
risks for the financial services sector. The term Advanced Persistent Threat (APT) is now
increasingly used to describe a category of malicious activities facing a growing number of
government institutions and commercial organizations. As described in a recent Financial Services Information Sharing and Analysis Center (FS-ISAC) report, “APT refers to an advanced, clandestine
means to gain continuous, persistent intelligence on an individual, company or foreign nation state
government or military [15].” The report shows there has been a history of APT attacks since 1986.
Key risks posed by APT actors generally include efforts to access and exfiltrate data that contains
sensitive and/or classified information. The information may be related to technology and
operations, intellectual property, proprietary business processes, business strategy, and/or personal
data pertaining to executives. APT activities include network mapping and software modification to
gain and maintain remote access to a variety of systems within the target domain. Such sustained
access, knowledge of networks and business processes allows perpetrators to lay groundwork for
future disruptive activities. Increasingly, APT discussions also include the use of tools specifically
designed to achieve disruptive effects such as Stuxnet, which is malware designed to attack Iran’s
nuclear power plants. The possibility of attacks focused on data corruption in the future has
also been identified. Key characteristics of APT activities include, but are not limited to:

threat actors with clearly identified long-term objectives guiding their attacks


structured, sustained intrusive activities to deploy, support and maintain exfiltration
operations


ability to conduct intelligence on individuals, organizations and processes that will prove to
be valuable targets


use of sophisticated software tools and techniques to conduct activities

flexible and adaptable operations to avoid detection.

Public recognition of these activities has risen dramatically. Numerous reports exist related to ongoing activities against governments and defense industries worldwide, specific activities focused on
the US energy industry and the highly publicized attacks against Google, as part of Operation
Aurora. With regard to financial services, limited open source information exists regarding
specific activities but the financial services sector is often identified in discussions and doctrinal
writings about cyber warfare between nations.
The conduct of APT activities relies fundamentally on the use of malware to establish access, to
maintain footholds within organizations and to exfiltrate sensitive data and/or conduct disruption
of IT systems or networks. Directed efforts using spearphishing have been a principal approach of
many of the operations against governments and the defense industry. Often, the payloads of
spearphishing attacks include a range of malware targeted at the most common types of applications
for enterprise users, particularly those in Microsoft Office and Adobe products. Often this malware
uses well known code exploiting well known vulnerabilities, but APT activities also employ new and
custom code not detectable by enterprise intrusion detection and anti-virus systems. APT actors are
generally highly aware of the state of enterprise information security practices. They employ code
and techniques not only to avoid detection but also frequently use malware to disable anti-virus,
intrusion detection systems, and other security software on exploited computers, and even across
broader portions of the enterprise. More significantly, APT actors may have a portfolio of
capabilities at hand to ensure the ability to continue activities even when discovered. Malware

more unique to APT activities often includes redundant and diverse tools to conduct exfiltration of
user credentials and sensitive data.
Financial institutions must be cognizant of the growing risks posed by malware specifically designed to disrupt
operations, particularly the operation of industrial control systems (ICS). The emergence of the
Stuxnet worm in 2010 targeted at the Siemens ICS provides concrete evidence that cyberspace can
have devastating effects on physical resources such as data center environment and power systems,
electric grids, gas pipelines, water delivery systems, and manufacturing equipment [16]. While the
original purpose of this malware appears to be targeted at the Siemens ICS utilized in nuclear
programs in Iran, key features of the worm pose much larger concerns that should inform the
financial services sector. The possibility of another actor capturing the code and repurposing it for
other purposes such as disrupting power grids is a significant possibility. As a Department of
Homeland Security official testified before a Senate committee, “What makes Stuxnet unique is that
it uses a variety of previously seen individual cyber attack techniques, tactics, and procedures,
automates them, and hides its presence so that the operator and the system have no reason to
suspect that any malicious activity is occurring. The concern for the future of Stuxnet is that the
underlying code could be adapted to target a broader range of control systems in any number of
critical infrastructure sectors.” More generally, the financial services sector could be targeted by
disruptive ICS malware specifically designed to exploit vulnerabilities in ICS applications used in this
sector, specifically heating, ventilating, air-conditioning (HVAC) and power supply equipment used
to monitor and control data centers.
The FS-ISAC has conducted a more detailed analysis of APT threats, risks and mitigations available
to FS-ISAC members.

Malware is used by malicious parties, both inside and external to the organization, with different
motivations. Examples of such motivations include financial gain, competitive advantage or,
potentially, revenge for some perceived slight or adverse event. For example, according to the
United States Computer Emergency Readiness Team (US-CERT), malware, as logic bombs, has
been distributed by disaffected insiders to delete massive amounts of data. In one such case,
malware “was designed to disrupt business operations.” In another case, a disgruntled systems
administrator employed by a financial services firm caused more than $3 million in damage to the
company's computer network, and was convicted of securities fraud for his failed plan to drive
down the company's stock price upon activation of the logic bomb. Cyber espionage, or theft of
information to receive a competitive advantage, could be aimed at stealing information about a new
technology product, uncovering strategic plans about a potential acquisition, or confidential data
regarding litigation. A House Conference Report that accompanied the US Consolidated
Appropriations Act of 2010 accurately observed “Cyber-based attacks and intrusions upon U.S.
computer networks . . . result in substantial loss of critical intelligence by U.S. government,


academia, military, industry, financial and other domains.”
It is evident from past and ongoing cybercrime investigations that the financial industry hosts a good
deal of malware. The US Secret Service (USSS) is the primary investigation resource for the US
Department of Treasury. For the past two years, USSS shared their cybercrime case reports with the
Verizon Incident Response team so they could be included in a collaborative effort to establish
cybercrime metrics. The resulting report contains details on confirmed security breaches
within firms that are either Verizon clients or in the investigative jurisdiction of the Secret Service
(141 Verizon cases and 257 USSS cases in 2009, 94 Verizon cases and 661 USSS cases in 2010).
Financial services firms were the primary targets in 33% of 2009 and 22% of 2010 cases, making
them the most targeted sector in 2009, though in 2010 they were surpassed by hospitality and retail.
However, hospitality and retail breaches also have negative consequences for Financial institutions. Account balance
targets in Financial institutions represent the closest possible approximation to actual cash for the cybercriminal. Financial institutions
are not only targets, but they are also more likely than firms in other industries to detect and report
cybercrime. Regulatory controls imposed on transaction reporting and risk management in the
financial industry make it more probable that a breach will prompt forensic investigation than if the
same breach occurred in another industry.
The Verizon/USSS set of data breach cases are reported using structured data that Verizon has
suggested should be the basis for incident analysis metrics. Data on each case is decomposed into
four major categories, and each of these have subcategories [27]. An incident is considered to be
fully described if reliable data exists to fill in the framework.

The Verizon data breach classification suggests that malware paths are an important consideration in
the criminal decision on technology choice, and this decision reflects the criminal assessment of FI
vulnerability to a given attack vector. Figure 7 shows the relative percentages of infection vectors
identified in the Verizon report.

This type of attack is accomplished by a perpetrator with access to internal operating systems from
an external source. It may be accomplished by exploiting vulnerabilities that allow remote command
execution via exposed software (e.g. SQL injection into web URLs, see [28]). It may also be
accomplished via commands issued by malware via remote perpetrator command and control
interfaces.

Phishing techniques originally were used to impersonate a bank or other institution with which a
user may have an account, and encouraged the user to click a link in the email that would bring them
to a site that looked like their banking site, but was actually fake. That site would either directly
collect credentials, or download malware that would later collect them. As less easily detectable
techniques for installing malware have been developed, random phishing techniques for malware
propagation have become less common. Nevertheless, these techniques still exist and are
increasingly customized as part of an overall campaign of attack.

The web is a popular attack vector for the simple reason that its use is ubiquitous. Malware injection
processes that are generally classified as auto-infection occur without any overt action on the part of
the user, such as inclusion of malware that automatically exploits a browser vulnerability in the
iFrame example. The propagation and infection both occur without the user’s active
participation or knowledge. Malvertising, the practice of placing malware in fake (or real) online ads,
is also an increasing source of auto-injection attacks. Malware operators may place ads with
links to malicious sites in order to spread malware or the ads could also contain scripts which
execute code on the PC.
High default trust settings on browsers and users operating with administrative privileges increase
the effectiveness of this attack vector, which is enabled via a combination of vulnerable software and
infected websites. These websites may be owned and operated by criminals, yet not conspicuously
enough to be blocked by commercially available security services. They are often legitimate sites on
which criminals have installed malware propagation code.

Malware writers use creative methods to lure random users into executing malicious injection code.
Drive-bys can happen by simply visiting a compromised or malicious website, viewing an email
message and also by clicking on deceptive pop-up windows. Many of the latter incorporate a social
engineering aspect to persuade the user to follow a malicious link. (For example, a pop-up that
reads, “You are infected with a virus, click here to clean your system!”).
These attacks rely less on browser vulnerabilities, but do require administrative access to infect at a
level that will escape detection.
In any of the above attack vectors, malicious software may be planted within the internal network.
Although most Financial institutions block most inbound traffic, it is rare for a commercial institution to block
outbound web browsing. Malware with command and control capabilities will often connect back to
the malware operator’s site using common browsing protocols, and this allows malware on the
internal network to receive both software and commands from the outside. Bots will often be
equipped with multiple URLs so that if a malware operator site is taken down (whether due to
maintenance or by law enforcement), another will be contacted which will have the same ability to
issue commands to bots. Data collection networks are supported with a large number of proxy
servers configured to relay data to the malware operator and to update bots with new addresses for
data collection servers as the malware network evolves.

Network periphery security is the first line of defense in keeping out hackers, and yet it is common
for network firewalls to change and network engineer and operator mistakes, whether intentional or
unintentional, sometimes have the consequence of allowing unfiltered Internet traffic into private
networks. Even where firewall rules have not changed, changes to configurations of Internet-facing
equipment behind firewalls may have the effect of allowing unauthorized access. Malware operators
constantly attempt connections to addresses within the Internet address range owned by targets to
see if the opportunity for unfettered access may exist. Although the network propagations attacks
that took advantage of vulnerabilities in common network protocols (e.g. SQL Slammer) have not
been prevalent recently, the potential for such attacks still exists.

Although portable media and devices are currently used in a small percentage of attacks, this vector
category was a new addition to the 2011 Data Breach Report in recognition that the vector has
unique properties for attack enablement. Where Financial institutions approve a set of mobile devices for authorized
network and data access, that device becomes a target of attacker reconnaissance. There are typically
not mature security processes in place to identify and patch vulnerabilities in mobile devices, and
their operating systems are purposely designed to allow ease of communication at the expense of
access control. As more and more mobile devices are equipped with browsing capability, their utility
as a platform from which to launch malware attacks may be expected to grow to the level of
Web/Internet attacks. Figure 10 shows the results of a McAfee Labs study on the number of
separate malware instances identified by mobile platform. While the total number of mobile malware
instances does not approach that of desktop computer or servers, Figure 10, in comparison with
previous years, demonstrates that both the number and variety of mobile malware is increasing. This
indicates a growing interest in the mobile environment by malware creators and operators.

A paradigm example of desktop malware is spyware that evades detection while transmitting
keystrokes and other observations on the desktop environment to a remote observer; it is
considered even more insidious if it allows commands to be entered into the device from a malware
operator. Yet this type of spy capability software is distributed through legitimate software
distribution channels for mobile devices. The openness of the Bluetooth protocol by which
many of these devices communicate further blurs the line between legitimate and illegitimate
observation of mobile communication.

In order to embed malware into FI software, insider access is generally required. There are cases
where insiders behave corruptly on their own in acts of fraud or revenge; however, insiders may
be compromised by outsiders to behave corruptly via bribery or social engineering. Insiders may also
unintentionally create cyber risk and access to sensitive data for outsiders.
While cases often involve malicious insiders who developed the code or administer the system on
which it runs, some known cases of this type were committed by outsiders. One of these
involved an external agent that had access to the system for over six months. During this time, he
studied the input/output process and developed custom malware to provide ongoing access to
newly created internal data.

A significant 19% of cases cannot be ascribed to any of the attack vectors so far
mentioned, and while none of the categories recognize social media as the primary source of cyber
attacks, social media has been cited as a source of malware in very significant cases. Social media
is a generic term for Internet sites that allow users with similar interests to create web content in a
collaborative manner. Examples of these sites are Facebook, Orkut, Hi5, MySpace, LinkedIn. They
are also generically referred to as social networking sites, as the groups of people that collaborate on any
one site are called a social network. With the increasing popularity of social media and the large
communities of Internet users that it attracts, social media sites have become fertile hunting ground
for malware operators.
Social media applications include functions that open communication channels with friends and
acquaintances, and allow users to develop networks of people with like interests. It relies, for its
operation, on trust between users. Whether or not a user on a social networking site has ever met
the people with whom they communicate in person, there is an assumption that the people in a
social network are friends rather than foes. The Internet provides a cloak of anonymity for people
with malicious intent and allows them to use social media to masquerade as friends.
Friends in a social network frequently post links to a shared web page, and others in the group
follow those links to view the shared content. Hence, one successful method of malware delivery via
social media is to join a group of which the target is a member and post a link leading to a malicious
site on a web page shared by the group. As in Web/Internet User-Initiated attacks the link takes the reader to a malware operator’s website which automatically triggers a
malware propagation and infection. Social networking attacks also may be launched from a trusted
social networking site itself. As many of these sites allow collaborative application development and
sharing, any member of a group may deploy malicious code that would likely be executed by the
others.
Another option for using social media is to attack a primary target in two stages. In stage 1, the


malware operator targets friends of the primary target user, infects their computers, and captures the
friend’s login credentials for email and social media. With this information, the malware operator
will then log in to the friend’s accounts and post innocuous-looking links that lead to malware
infection. They may also impersonate the friend by sending direct emails or instant messages to the
primary target, encouraging them to select malicious links.


The first step in a cyber attack is reconnaissance, the step in which an
adversary surveys a target to identify points of vulnerability. It is an attack planning phase. However,
in targeted attacks, this phase may be expected to continue throughout the lifetime of the malware
install. Command and control facilities will typically be used to continue
reconnaissance within an internal network. Results will fuel further attack plans.
Malware authors mining an internal network for information have been creative. During 2010 an
increase in focused attacks has shown attackers to package open source, toolkits and well architected
botnets as part of their approach. In the past several years, malware professionals have been known
to develop custom exploit code intended for a specific target after learning about the environment
on their internal networks. Custom code increases overall malware effectiveness because it may
exploit legacy protocol weaknesses that are not usually found on the public Internet, and often
overlooked because internal networks are trusted. Custom code also allows malware operators to
incorporate features to avoid internal monitoring systems to evade detection. Internal malware
Internet communication is typically encrypted to evade content filters that may be installed in FI
perimeters.
No FI is an island, and neither is the financial services industry as a whole technically self-sufficient.
Successful malware attacks on Financial institutions and FI customers often are traced to vulnerability exploits that
originate from devices, components, and agents across the ecosystem in which the FI has deployed
service. The vulnerabilities may be due to human or automated responses to attack, and are often
outside of the FI’s direct influence. Therefore, a key element of FI anti-malware strategy must be to
acknowledge and face the problem of vulnerable ecommerce infrastructure. It is incumbent upon
the financial industry to support cross-industry engagements to reduce systemic risk of malware by
leveraging its collective influence on external entities. Financial institutions should examine the flow of sensitive data through
computing devices, and conduct multiple risk assessment scenarios. Each scenario should assume
that different subsets of the devices through which sensitive data flows are infected with malware.
Each scenario should be analyzed from multiple perspectives.
If and where any of the analysis of malware infection scenarios results in probable damage or loss,
Financial institutions should consider products, services, partnerships, or industry initiatives that may be leveraged to
improve the scenario’s risk profile. Of course, this exercise will be guided by the FI’s evaluation with
respect to both risk tolerance and cost/benefit trade-offs.
Options for risk mitigation will also vary, and these will be based on the attack vector as well as the
target. In order for any FI to successfully complete a scenario-based malware risk assessment, it
must employ personnel who are cognizant of malware threats from all kinds of technical devices,
including customized corporate devices, personal devices used for FI business communication or
transactions, and third-party devices such as partner-provided network connections or airport or
hotel kiosks used by traveling employees for remote access. As the threat environment continually
evolves, personnel must continually seek new sources of current information on the types of
malware being distributed and the common modus operandi deployed by cybercriminals. The FSISAC exists to serve this purpose, and has several processes with which to facilitate the
communication of threat, vulnerability, and countermeasure information among Financial institutions, software
vendors, and government intelligence sources.

Regardless of its source, for a malware attack to be effective, each of the multiple steps –
reconnaissance, assembly, delivery, compromise and execution – must be successful.
Reconnaissance may occur internally or from remote sources. Assembly may make use of java
toolkits, php scripts, or command line batches. Delivery and compromise may occur via WebInternet User-Initiated attacks or any other vector. Command stages may
include remote control of dormant bots or active and continuous keystroke logging. Exfiltration
may be continuous or malware may stash data locally until it is retrieved. There are also a variety of
other attack choice combinations, limited only by the imagination and programming skills of the
adversary.
Although end-user awareness and training had typically provided defense in depth in security
measures, malware attacks tend to follow the same pattern as a variety of legitimate software
installation processes that conflict with typical FI security software setting, and so easily escapes
even vigilant end-user detection. Users have been inundated with security instructions over the past
decade that have not been effective in reducing their vulnerability to data loss and identity theft,

while at the same time they are constantly exposed to unnecessary security pop-up warnings. So real
malware would appear to them to be yet another false positive. Unless an FI can develop accurate
guidance on how to tell the difference between false positives and malware, most security advice will
seems like a poor cost-benefit tradeoff to users, and so will be rationally rejected.
Moreover, malware operators will constantly vary attacks so that if any one is discovered, it will not
lead to the detection of a similar one. It is important that FI detection processes be as flexible and adaptable as the
capabilities of the adversary. The earlier in this attack progression an FI can detect that an attack is
underway, the more damage may be averted.

Financial institutions must be careful not to over-rely on traditional anti-virus software to detect malware infections.
Polymorphic malware has made those methods unreliable. To keep pace with these new malware
trends, many anti-virus and anti-malware software providers are incorporating heuristic capabilities
into their products. These monitor software process behavior and attempt to identify anomalies.
Heuristic malware detection can be effective, but often at the price of system performance.
While polymorphic malware may be capable of evading detection by traditional mechanisms, it can
often be detected through the effects of the actions it takes. Organizations that monitor for
unauthorized file level changes and for unauthorized or unusual communications patterns both
within a network and through the network’s perimeter may have a view into activity associated with

malware. Botnet and APT command and control activity can often be detected at the network level
with specialized appliances being offered by a number of security vendors. This activity can also be
gleaned from the analysis of server, proxy and firewall logs.
Financial institutions typically monitor multiple aspects of their operation, and these monitoring processes may be
engaged to assist in identifying and minimizing the impact of malware attacks. For example, Financial institutions
have monitoring processes that detect red flags in customer transactions. These range from
anomalies in web server logs to unusual customer transaction patterns. Financial institutions also have monitoring
processes designed to detect and respond to events that impact technology operations. Malware
operators will attempt to stay below the radar of these monitoring processes, so Financial institutions may need to
adjust them in order to bring malware to the forefront of situation awareness. Red flag detection
processes in different departments or business units may be combined with IT incident detection.
Where malware incidents are detected, appropriate responses should not only mitigate the financial
damage, but may involve redesigning internal processes to ensure the same attack vector will not be
successful in the future.

determine the scope of an incident, resolve it if possible, and if not, hand it off to a more skilled
administrative contact in the problem domain. This next level of support will attempt to resolve
problems, but also recognize when the problem solution is beyond their capability and so requires
escalation to more specialized expertise such as that found in systems engineers or application
developers. Wherever an FI has such an alert and incident response process in place, whether based
on automated monitoring or end-user trouble reports, there should be increased recognition at all
levels of escalation that the root cause of the incident may be malware.
Additional processes designed to respond to malware that Financial institutions should consider implementing at an
enterprise level, if they are not already incorporated into existing incident detection response
process, include:
Identification: to ensure that incidents that have been identified as caused by malware are
catalogued as such so that appropriate follow-up may be thorough and
provide comprehensive insight into the overall state of the malware situation
in proper context
Eradication: to ensure that systems infected with malware are removed from service and
reconstituted in such a way that does not allow malware persistence post
reconstitution; reconstitution differs from recovery in that it implies rootcause forensic analysis and system configurations developed to ensure that the
entity is no longer vulnerable to the same type of attack
Resilience: to ensure that malware incidents do not have a lasting effect on business
operations, damaging impact is minimized, and that operations processes are
modified to incorporate prevention and detection techniques that would
prevent the same type of attack in the future
The proper selection of controls to be included in each process will of course be based on
circumstances specific to the FI business process. However, the scope of controls may extend to the
customer, vendor, or business partner environment. For example, malware at a customer site may
result in transfer of customer account balances to a malware operator. In this case, the FI may
identify and catalogue the event via a Red Flag monitoring process, recommend that the customer
initiate an eradication process, and provide services such as positive pay or two-factor authentication
as resilience measures.

The report cautioned that, although such attacks may appear to begin with a
narrow scope, they should be assumed to be capable of lasting repercussions due to the inclusion of
multimodal attack techniques over time. By several estimates, a large percentage of both internal and
external users experienced an average of more than 100 web malware encounters per month,
increasing the probability that any given user will be infected. Although specific numbers in
various surveys that chronicle increasing costs of data breaches may be debated, it is obvious that both the variety of incidents and the quantity of data lost is constantly rising, and that each incident
is accompanied by monetary loss. Financial institutions should do their own risk analysis. According to the US
Secret Service, given current evidence that there are large criminal communities directly targeting the
US financial sector, FI exposure to malware is extremely high and should be treated with the
probability of 1 in FI risk management calculations.
Malware presents a range of evolving risks: reputational, regulatory, financial, and legal. Reputational
risk is increased because of the high visibility created by reporting requirements and the volume of
information at risk. Regulatory risk is derived from the types of information assets targeted by
malware operators, which include personally identifiable information, account information, and
deposits, as well as the criticality of the service and the provider to the monetary system. Security
requirements for these information assets are included in the Gramm-Leach-Bliley Act (GLBA),
Sarbanes-Oxley Act (SOX), Payment Card Industry (PCI) Data Security Standard, and the Health
Insurance Portability and Accountability Act (HIPAA). Financial risk may be estimated using
potential losses associated with successful malware attacks. Legal risk may be associated with civil
challenges on due care and due diligence issues.
Proceeding on the assumption that malware presents risk to Financial institutions, there are a number of standard
controls that may mitigate the risk. The first and most important is software change control.
Software change control refers to a process whereby software is developed, compiled into packages
for installation, labeled with version numbers, deployed by authorized personnel, and tracked on
production systems. To be effective against malware, software change control processes must
continue to track software through its deployment and operation. Changes to software must be
automatically detected. Upon detection of a change, the change must be analyzed by someone with
sufficient knowledge and reference materials to tell the difference between an authorized change and
an unauthorized change. The reference materials should include tests such as cryptographic
checksums that can be used to verify that code deployed in production is the same as the package
that was delivered from a development environment or vendor. These detection processes must
occur immediately after changes are detected. Changes must be detected on all operating system
platforms and monitored for integrity, as malware operators are likely to attempt to disable or
corrupt the software used for change monitoring.
Change monitoring should not be limited to software, but should extend to security configuration
and role assignments such as start-up variables, firewall rules, and privileged accounts. Privileged
account monitoring must be established in conjunction with a policy of authorized account usage so
that authorized use may be distinguished from unauthorized use. For example, where users or
software running in an administrator context is typical in a firm, this scenario used to install malware
would not be detected as an intrusion. Even desktop administrators should be furnished with
separate accounts reserved for privileged operations. Ideally, administrative access would be
segmented so that systems would be subject to malware compromise via only a small percentage of
total system users.

Another standard control that is a critical component of any malware mitigation strategy is control
over the network periphery. This control requires that an FI establish a clear policy that allows
administrators to determine authorized from unauthorized connections, and oversight that ensures
compliance with these policies. Firewall rules and security configurations over all network
equipment should also be subject to change control as described above for software. Financial institutions with
network peripheries that are too large to manually review firewall rules in near real-time should have
automated means to determine policy compliance for both inbound and outbound network
connections. Lists of malicious sites are published and announced to Financial institutions by the FS-ISAC.
Connections to or from the FI network to any published malicious site should be restricted via
automated means. Both inbound and outbound network traffic should be examined for known
malware patterns and signatures using intrusion and/or prevention detection systems. Any
discretionary Internet traffic generated by FI users that may be a conduit for malicious content, such
as email and web browsing, should be routed to choke points where proxy servers may be employed
to inspect content for malware signatures as well as sensitive data. Proxy servers are frequently
capable of decrypting encrypted web traffic, and these servers should block encrypted traffic if it
cannot be decrypted for inspection (of course, exceptions may be made for authorized business
applications).
A third critical component of any malware mitigation strategy is vulnerability management.
Operating system and application security standards should be established that, if followed, will
ensure compliance with FI objectives for access to system programs, facilities, and data. These
standards should be enforced with automated compliance-checking software, and that software
should be monitored for integrity. All operating system and software security patches should be
applied to any system for which they are available. Where vendors no longer support software patch
processes, or do not commit to fixing security vulnerabilities in a given commercial product, Financial institutions
should consider alternative software vendors or versions for which security patches are available.
Financial institutions should also consider what may constitute evidence of malware intrusion in their technology
environment, and identify patterns of activity that it may be possible to log and automatically detect
in a manner that would trigger an incident response. Where it is not feasible to automate detection,
manual log review procedures may be necessary to identify evidence of intrusion. Candidates for log
monitoring include, but are not limited to, failed outbound email server connections attempts,
network scanning, and excessive domain name queries [53]. Due care should be exercised to ensure
that the logs are collected as expected, and that they are archived with integrity.
Metrics on software change control, network periphery control, vulnerability management, and log
management, as well as digital identity and incident response metrics, should be devised and
employed as part of a comprehensive security management strategy. These metrics should be
generated and reviewed as part of continuous operations monitoring processes and used in the
course of daily security management.

Though Financial institutions are a high value target for malware-based exploitation, the financial sector does not
operate in cyberspace in isolation. Other institutions are not only targets for malware, but more
importantly, they are exploited as the means by which attacks are perpetrated against Financial institutions. Any device
capable of running operator-installed software is a potential attack vector, and much of the software
run by Financial institutions is not under FI control. This distinction has been characterized as the managed versus
unmanaged device issue. Yet even in cases where Financial institutions manage electronic devices, software updates may
be delegated to vendors and are usually accomplished via automatic downloads.
A business or technology partner that provides software or electronic processing that is used to
support customer relationships is generally referred to as a “third party” to distinguish it from the
two-party relationship an FI has with its customer. The decisions third parties make about their own
anti-malware practices have a direct impact on the efficacy of FI anti-malware strategies, sometimes
with grave consequences. It is therefore imperative that the financial services sector set clear
requirements for third party stakeholders.
Moreover, the Internet ecosystem that Financial institutions inhabit is populated by service providers that are not
currently considered third parties in the traditional sense of the term. These are technology
operators that facilitate Internet communications between Financial institutions and customers, but are not
contractually or otherwise bound to specifically support FI services. Figure 14 illustrates that Financial institutions
operate in an Internet environment that is facilitated by Internet Service Providers (ISPs), Internet
eXchange Points (IXPs), and administrative institutions such as Internet Corporation for Assigned
Names and Numbers (ICANN), among other technology operators.

Customers know only the FI domain names, not the
IP addresses, and therefore must rely on the domain name system to direct them to legitimate FI
network resources. However, perpetrators of cybercrime exist in the same ecosystem, and may be
expected to attack any vulnerable component if that attack vector could provide information that
may ultimately lead to a successful attack against an FI. For example, if a provider of domain name
services to a customer or FI business partner is corrupted, then those customers and business
partners who type an FI domain name into their browser may be directed instead to a malicious site.
In such cases, criminal operations are beyond the FI scope of operations, and correspondingly
beyond its ability to quickly detect or respond.
A recent exercise tested the ability of financial institutions, card processors, businesses, and retailers
to respond to major cyber attacks against payment systems [54]. Attack vectors used against the
financial industry in the exercise included many of those described in Section 4. Over 600 Financial institutions, card
processors, retailers, and business customers participated in the exercise, and all reported that they
would have been severely negatively impacted from the attack. For example, most participating
financial institutions (58%) don’t have a contingency plan focused on Distributed Denial of Service
(DDoS) and a majority of those that do rely on Internet Service Providers to provide mitigation. Yet
a recent Arbor network survey of ISPs reveals that a full 50% of their DDoS mitigation strategies
are known to further degrade service, essentially completing the attack as a first step toward defense
[55].
The decisions that domain name service providers and other cyber facilitators make with respect to
anti-malware practices have a direct impact on the efficacy of FI strategies. It is therefore important
that each FI set clear requirements for security practices at service providers as part of any cyberrelated service level agreement. These practices should not simply refer to financial industry
standards, but should specify that service providers must create and maintain continuous security
situational awareness and defense capabilities that correspond to the risk that malware may present
to the integrity of the contracted-for services. Where service providers do not share traditional thirdparty relationships with Financial institutions, such requirements must nevertheless be articulated in order to provide
a basis for defining due diligence on the part of service providers, and potential corresponding
claims of negligence.

The second column identifies technology controls that the provider in the
corresponding row may reasonably be expected to perform in order to minimize potential damage
due to malware. The third column classifies the control in the second column according to the
information security industry triad of prevent, detect, recover. The word “prevent” in the third column
indicates that the control activity identified in the second column may prevent a “malware event”
before it happens. The word “detect” in the third column indicates that the control activity
identified in the second column contributed to FI capability to discover when a “malware event”
occurred. The word “recover” in the third column indicates that the control activity identified in

Certificate Authorities (CA) play a key role in the security of
online banking applications in that proper application of the
technology allows a customer to identify imitation banking
sites. However, recent failure in the security of CA
administrative functions secure has resulted in issuance of
“valid” SSL and EV-SSL certifications to criminal elements.
CAs need to evolve their technology and service offerings
to prevent these malicious activities.

Domain Name Service (DNS) Registrars should play a key
role in malware prevention by improving due diligence so
that cybercriminals find it harder to register new domains to
perpetrate phishing attacks and/or malware drive-by sites.

It is absolutely critical that DNS Registrars maintain
accurate data on domain name owners so Financial institutions can perform
effective investigation into instances of abuse. Unavailable
or inaccurate registration data increases the cost of online
fraud investigation and remediation activity.

Various industry standards have been developed to prevent
email spoofing and spamming at the server level. Email
hosts should observe Internet Engineering Task Force
(IETF) standards and BITS recommendations for enabling
email authentication and validation processing on both
inbound and outbound mail streams.

ICANN issues generic Top Level Domains (gTLDs) for
specific purposes but generally does not enforce the manner
in which they are used (e.g. .com, .edu). A gTLD issued for
financial services should be restricted to FI registrations. In
this environment, technologies could automatically provide
higher levels of security for FI online services.

Internet Service Providers (ISPs) should ensure that their
networks do not allow traffic that spoofs IP addresses.

ISPs should offer proxy services that block known malware
and criminal sites by default.

Both land-line and mobile ISPs are in a unique position
within the ecosystem to detect patterns of malicious activity
such as botnet traffic and malware infection signatures.

ISPs that detect malware should either warn or educate the
system owner, and/or quarantine the affected system as
appropriate to safeguard other systems.

Mail User Agents (MUAs) like Outlook, Apple Mail,
Thunderbird, etc. can help prevent malware infections by
leveraging security indicators and business rules that users
can leverage to identify when a message in their inbox (or
spam folder) is suspect.

Operating Systems (OS) vendors that include features for
internet connectivity and operator-installed software should
provide improved platform functionality to prevent
unauthorized software installation. They should take
advantage of advances in trusted computing technology to
accomplish these goals.

OS vendors should provide customers with the ability to
identify and catalogue all software on their system, and
disable the ability of malware to evade standard software
monitoring utilities.

OS vendors should maintain rapid incident response
capabilities that allow customers to report security incidents
and provide accelerated distribution services for security
patches.

OS vendors should consider offering customers malware
remediation services, wherein the vendor can assist in
cleaning up a subscriber’s device of the malware without
otherwise impacting its productive operation.

Web browsers should have safe modes wherein no software
may be installed on a local machine no matter what the user
behavior. Vendors can help prevent malware infections by
collecting information about malware-infected websites and
enabling safeguards in the browser that warn or block a user
from visiting known infected sites.

Web browsers should provide logs and statistics of software
installation and operation that originates via browser
functionality such as downloads and plug-ins.

Vendors that encourage users and web developers to
incorporate their software into web-enabled environments
should include security features that allow users to limit the
access of the plug-in to specific operations that may be
secured at the operating system level. These security
configurations should be configured by default upon
installation.

Web browsers plug-ins should provide logs and statistics of
files accessed and operations performed by the plug-in, and
these should be archived and available for inspection.

Web Server Hosts Web server hosting providers should follow strict software
security and change control procedures to prevent
vulnerabilities that allow unauthorized malware to be
planted on their sites.

Web hosting providers should provide whistle-blower
reporting facilities to receive and respond to notifications
that a website that they host has been discovered to include
malware.

Web hosting providers should create, maintain, and follow
standard procedures to effectively quarantine malware on a
client’s website. Web hosting providers should proactively
test their sites to detect the presence of unauthorized
malware.

Regulators should consider the above recommendations for
malware risk reduction for entities within their scope in
establishing regulatory requirements for ecommerce and
Internet safety.

End users should assume responsibility to update and
maintain their systems to the extent possible to enhance
security. Additionally, these users should take responsibility
for protecting information necessary to implement security
measures such as PINs.

Malware is both insidious and pervasive. The financial services industry is a prime target, making it
imperative for financial institutions to prepare to face malware attacks and prevent financial loss,
damage to reputation, reduction in customer assets, data breaches, regulatory oversight, and/or lack
of management control over technology assets. Financial institutions should recognize that malware operators rely on
a strong and stable financial industry in order to profit from crime. They are unlikely to target critical
transaction processing systems for fear that their own fraudulent transactions will not be processed.
Unless there is a hostile intent to cause damage, as in a nation-state declaration of war, malware
operators are likely to maneuver between the seams of authorized business processes, and inject just
enough variation required to execute their criminal mission. Moreover, although crimeware and
state-sponsored cyber attacks and campaigns are the most visible form of attack, Financial institutions should
recognize the increasing threat from both external and internal sources, and take practical measures
to detect and defend against potential internal malware interference with business process.
Financial institutions should evaluate their vulnerability to the malware described in this report and implement

appropriate safeguards to minimize any potential for damaging impact. This should not only include
the implementation of layered preventative measures, but also measures to detect the presence of
malware and a plan to respond to malware once it is detected. The plan should be exercised
periodically as is done for business continuity and disaster recovery planning purposes. Response
plans should be integrated into the financial institutions’ overall crisis management process so that
highly impactful attacks are responded to with the appropriate level of senior management
involvement and oversight. Integrating the plan within the overall crisis management process will
ensure that escalation points are defined, governance processes are in place, and there are
representatives from the appropriate functional units that might be called upon to assist in
management of the response and communications to stakeholders.

Bot: derived from the word “robot,” and used in a variety of Internet contexts, in the
context of this paper, it refers to a program that runs in the background on a
personal computer of an unsuspecting user, having been installed by malware.

Botnet: a collection of bots that receive instructions from the same “master” program.

Data Host: company that maintains servers on the Internet that process data for customers
using a standard technology such as web or email servers.

Exfiltration: method by which malware exports data from an infected host, typically refers to
an unauthorized process of acquiring data from a computer system through
network channels or unauthorized portable media.

Footprint: with reference to a software component is used to indicate the physical
characteristics of a file such as its size, the file names as well as the operating
system’s resource utilization. These characteristics help to uniquely identify the
various software components encountered during the investigative process.

Jabber: a communications protocol used for instant messaging.

Kernel: operating system component that serves as a bridge between software applications
and system services provided by hardware, and typically designed to facilitate a
trusted channel between the OS user and system-level functionality.

Malware: malicious software, any and all software that is deployed with malicious intent
Operating System: software that directly manages and controls interaction with hardware devices that
combine to compose a computer, provides common services to applications, and
makes resources available to users.
Phishing: email-born malware propagation systems.
Rootkit: enables privileged access to a system and the ability to hide that access by
subverting the provided authentication, authorization, and audit functions.
Socks: a protocol that allows multiple network connections to route network traffic
through a single network-enable device.
Zero-Day: modifier for the word threat or attack, meaning that the vulnerability that is used
by the threat agent is not known to potential victims.

With the initial success of uncovering and subsequent protection against CW3, CTA continued to
monitor CryptoWall’s activity to learn and understand the repercussions and reactions from the malware
authors. Unsurprisingly, CryptoWall authors were not deterred with the publication of CW3 and sharing of
associated IOCs but were determined to release a fourth variant of CryptoWall (CW4) to overcome known
malicious characteristics of its predecessor.
Based on the analysis of CW4, CTA researchers found the refreshed malware had similar traits to its
predecessor but differed sufficiently to attempt evasion and kept an otherwise similar infrastructure of
operation and distribution. Thus the focus of this updated report is to study the prevalence and global
impact of CW4 as compared to CW3.
Ransomware is a type of malware that encrypts a victim’s files and subsequently demands
payment in return for the key that can decrypt said files. When ransomware is first installed on a
victim’s machine, it will typically target sensitive files such as important financial data, business
records, databases, personal files, and more. Personal files, such as photos and home movies, may
hold sentimental value to the victim.
Once these files are identified, the malware will encrypt them using a key known only by the
attackers. In order to acquire this key to decrypt these files, the victim must pay a ransom to
the attackers, often in the form of electronic currency such as bitcoin. In the event a victim does
not have backups of this data, and chooses not to pay the ransom, these files are unlikely to be
recovered. Ransomware has been known to cause irreparable damage to both individual users and
large corporations alike.
There is another variant of ransomware that blocks the usage of the device with the same goal of
extracting payment from the victim. This behavior includes spawning multiple messages across
the screen disrupting user application usage or inhibiting the normal boot process of the operating
system with displaying a ransom message instead of a user login screen.

During the period from November 2015 to June 2016, CW4 had reached a total of 7.1 million attempted
infections spread across the globe with the largest impact found in North America. The same
result resonated in CW3. The main contribution factor is the willingness of victims to pay the
required ransom to avoid the loss of business continuity.
India is the second most-impacted country, the study found. India’s fast-growing IT industry
typically supports multinational companies and operates mainly through the Internet. These factors have
made them a significant target for CryptoWall as India rose from the bottom of the Top 10 in CW3 to the
second in CW4.

During CW4’s eight-month-long operation, CTA saw a total of 7,194,840 attempted infections with a peak
of 228,496 in one day. This pales in comparison with CW3’s 406,887 total attempted infections.
Although CW4 was much more aggressive in its attempt to spread the malware (i.e., 18 times more than
CW3), the number of confirmed victims was just 36,118. CW3, on the other hand, impacted hundreds of
thousands of victims.
One could surmise that the promotion of awareness of CryptoWall and proactive protection provided to
the public community served to dampen the success of CW4, leaving CW4 with a success rate of 0-5%.
This is evidenced by a ratio of a much lower number of victims in relation to the highest peak of
attempted infections during the same active period. This impact led to two major consequences. First, a
low success rate meant CW4 became less relevant as the usage (i.e., the number of attempted infections)
dropped significantly after late January.


Since cyberattackers are opportunistic in nature, they quickly turn to other ransomware, rapidly
transitioning to Locky and Cerber in the three-month window from February 19 to May 19, 2016.
A low success rate also meant a much lesser payout for CW4. Based on the average ransom price
of $500 or 1 bitcoin, CTA estimates $18 million in damages associated with CW4 compared to $325 million
with CW3.

The price paid by victims ranges from a few hundred dollars to over thousands of dollars (USD)
based on the perceived value of the ransomed artifact(s). Payment responsiveness to a ransom
affects the price ultimately paid (e.g., late payments could lead to as much as doubling of the asking
price of the ransom).

CTA researchers analyzed CW4 campaigns from May 19 to June 14, 2016, and discovered
“crypt7” was the most active campaign in CW4 as was in CW3. This indicates the same individual or group
that was most active (i.e., 8,000+ sessions) and enjoyed past financial success with CW3 increased its
investment by almost seven times on the fourth variant, reaching up to 55,170 sessions. Unfortunately, it
did not pan out financially, as the total CW4 profit was estimated at $18 million when compared to CW3’s
profit of $325M.
CryptoWall Campaign Crypt ID Convention
Campaign naming convention uses a string “crypt” followed by successions of numbers. This
campaign identifier references the person or group that is attributed when successful infections are
made. This attribution tracks the success of malware distribution and consequently the share of
profit between cybercriminal organizations.

Both CW4 and CW3 campaigns follow the same malware distribution model (i.e., email phishing
campaigns and exploit kits) (Table 2). Email phishing campaigns were the biggest contributor for
CryptoWall distribution in both CW3 and CW4 campaigns due in part to user susceptibility to email as a
vector of attack.

The CryptoWall authors(s) mostly likely analyzed the flaws and characteristics of the malware published
in the CryptoWall version 3 Threat Report. They showed persistence with the creation of the fourth variant
of CryptoWall and characteristically held true to the tenacity of advanced cybercriminals. Fortunately, CW4
was materially less damaging at an estimated $18 million compared to CW3’s $325 million, even though
CW4 was more aggressive at 7.1 million attempted infections compared to CW3’s 406,887 attempted
infections. CW4’s low success rate (i.e., 0.5%) eventually led to the decline of this malware activity after
January 2016.
However, the waning of CW4 in the following months also saw a rise of Locky and Cerber ransomware.
The flexibility tied to financial motivation of cybercriminals cannot be understated as they continue to prey
on organizations concerned with business continuity such as those found in North America and many
developed countries.
Email phishing continues to play an important role in distributing CryptoWall, thus malware protection for
email should be a serious consideration for the reader.
This updated report on CryptoWall marks the Cyber Threat Alliance’s second milestone since the release of
the CryptoWall v3 Threat Report. The Cyber Threat Alliance and its members are dedicated to identifying,
researching, and exposing incredibly dangerous and impactful threats around the world in order to better
protect our customers and the open source community.

There is no common understanding of what a cyber security incident is, with a wide variety of
interpretations. With no agreed definition– and many organizations adopting different views in practice
– it is very difficult for organizations to plan effectively and understand the type of cyber security incident
response capability they require or the level of support they need.

The original government definition of cyber security incidents as being state-sponsored attacks on critical
national infrastructure or defence capabilities is still valid. However, industry – fuelled by the media – has
adopted the term wholesale and the term cyber security incident is often used to describe traditional
information (or IT) security incidents. This perception is important, but has not been fully explored – and
the term cyber is both engaging and here to stay.

The main difference between different types of cyber security incident appears to lie in the source of
the incident (eg a minor criminal compared to a major organised crime syndicate), rather than the type
of incident (eg hacking, malware or social engineering). At one end of the spectrum come basic cyber
security incidents, such as minor crime, localised disruption and theft. At the other end we can see major
organised crime, widespread disruption, critical damage to national infrastructure and even warfare.
Furthermore, the nature of attacks is changing from public displays of capability to targeted attacks
designed to be covert.
organizations vary considerably in terms of the level of maturity in their cyber security incident response

capability, but also in the way in which they need to respond. Whilst good practice exists – and is being
improved – the lack of both a common understanding and a detailed set of response guidance is limiting
organizational capabilities and approaches, as well as restricting important knowledge sharing activities.

Few organizations really understand their ‘state of readiness’ to respond to a cyber security incident,
particularly a serious cyber security attack, and are typically not well prepared in terms of: People (eg assigning an incident response team or individual; providing sufficient technical skills;
enabling decisions to be taken quickly; and gaining access to critical third parties)
Process (knowing what to do, how to do it and when to do it), eg identify cyber security incident;
investigate situation; take appropriate action (eg contain incident and eradicate cause); and recover
critical systems, data and connectivity
Technology (knowing their data and network topology; determining where their Internet touch
points are; and creating / storing appropriate event logs)
Information (eg recording sufficient details about when, where and how the incident occurred;
defining their business priorities; and understanding interdependencies between business
processes, supporting systems and external suppliers, such as providers of cloud solutions or
managed security services).

In practice it is often very difficult for organizations to identify the type of cyber security incident they are
facing until they have carried out an investigation, particularly as very different types of cyber security
incident can show similar initial symptoms. Even when organizations have comprehensive detection
software and logging it can be difficult to determine the nature of an attack in a timely manner. Despite the current level of threat from cyber security incidents, those responsible for preparing for, responding to and following up cyber security incidents in many organizations still face significant challenges in: Persuading senior management to appreciate the extent of the problem; restricting budget
and resources; Knowing who to contact to provide expert help (and why); Involving experts at a sufficiently early stage in proceedings; Providing them with sufficient information to be able to investigate effectively.

Most organizations need professional help in responding to a cyber security incident in a fast, effective manner. However, it is very difficult for them to identify trusted organizations that have access to
competent, qualified experts who can respond appropriately whilst protecting sensitive corporate and
attack information.
Employing the services of properly qualified third party experts (such as those CREST members who
provide cyber incident response), can significantly help organizations to handle cyber security incidents in
a more effective and appropriate manner – particularly serious cyber security attacks. Research revealed
that the main benefits of using this type of external supplier are in: Providing resourcing and response expertise, by gaining access to more experienced, dedicated
technical staff who understand how to carry out sophisticated cyber security incident investigations
quickly and effectively; Conducting technical investigations, by providing deep technical knowledge about the cyber security
incident, including: the different types of attacker (and how they operate); advanced persistent
threats; methods of compromising systems; and sophisticated analysis of malware; Performing cyber security analysis, for example by monitoring emerging cyber threats; applying
modern analytic capabilities to aggregate relevant data from many different systems; and providing
situational awareness, particularly in the area of cyber intelligence.

The four key steps in the process for choosing a suitable supplier of cyber security
incident response services (‘The Selection Process’) are described in detail in the
complementary CREST Cyber Security Incident Response – Supplier Selection Guide
Throughout the Guide you will find a set of tips, warnings and quotes provided by a diverse set of contributors,
including expert suppliers (such as many CREST members), consumer organizations, government bodies and
academia. These bring real-world, practical experience to the Guide, allowing you to get a better feel for the types of
action that are most likely to apply to your organization.

It may also be of interest to business managers, risk managers, procurement specialists and auditors.

Cyber is the latest buzzword that has really taken the media by storm. There are examples everywhere about the
possible horrors of cyber security attacks. Many organizations are extremely concerned about potential and actual
cyber security attacks, both on their own organizations and in ones similar to them.
Cyber security incidents have become not only more numerous and diverse but also more damaging and disruptive,
with new types of cyber security attacks emerging frequently.


organizations are seldom adequately prepared for a serious cyber security incident. They often suffer from a lack
of: budget; resources; technology; or recognition of the type and magnitude of the problem. In addition, they do
not have the software, testing, process, technology or people to handle sophisticated cyber security threats, such as
Advanced Persistent Threats (APTs).
An effective method of responding to cyber security incidents is therefore necessary for rapidly detecting incidents;
minimising loss and destruction; mitigating the weaknesses that were exploited; restoring IT services; and reducing
the risk from future incidents.

Current cyber security incident response guidelines can be very useful, but do not typically provide: solid, consistent definition of a cyber security incident - or any real distinction between cyber security
incidents and traditional information (or IT) security incidents; In-depth guidance about dealing with cyber security incidents, particularly for commercial consumer
organizations outside government or Finance sectors; Advice on who organizations can ask for help – backed up by selection criteria.
Consequently, many organizations do not have access to appropriate external sources and levels of guidance to help
them prepare for most types of cyber security incident, let alone a serious cyber security attack.

The term cyber (which actually means robotic) can be interpreted in many ways. For example, one dictionary definition
defines the term Cyber as ‘relating to computers and the Internet’, which again can mean different things to different
people. Furthermore, project research identified that cyber is often associated with the concept of cyberspace.

Cyberspace is an interactive domain made up of digital networks that is used to store, modify
and communicate information. It includes the internet, but also the other information systems that
support our businesses, infrastructure and services.

Cyberspace is constantly evolving and presenting new opportunities. The desire of businesses to quickly adopt new
technologies (using the Internet and adopting cloud services to open new channels) provides enormous opportunity,
but also brings unforeseen risks and unintended consequences that can have a negative impact.

The interconnectedness of the Internet brings huge benefits to the World but also an unrivalled
opportunity for harm.
Many computing devices (eg PCs, laptops, tablets and smart phones) are connected to the Internet on an almost
continuous basis. Technical exploits target not only vulnerabilities in infrastructure, but also in many web-based
applications. It may be that cyber security is the security of cyberspace and that a cyber security incident is one that
impacts on cyberspace or uses cyberspace as part of an attack vector.

The term Cyber Security is poorly defined – and understood.
It often appears to be replacing the term information (or IT) security, rather than
being supplementary to it. For example, the PwC / BIS cyber security breaches survey
was previously called the information security breaches survey, but the questions
appear to be virtually the same.
The UK Government tendency to focus on Cyber Security and Information Assurance
(CSIA) seems to work, but is not well understood by commerce – or commonly used
outside the UK.

There are many types of information (or IT) security incident that could be classified as a cyber security incident, ranging
from serious cyber security attacks on critical national infrastructure and major organised cybercrime, through hacktivism
and basic malware attacks, to internal misuse of systems and software malfunction.
However, project research has revealed that there is no one common definition of a cyber security incident. There is no
authoritative taxonomy to help organizations decide what is (or isn’t) a cyber security incident, breach, or attack.
Often cyber security incidents are associated with malicious attacks or Advanced Persistent Threats (APTs), but there
appears to be no clear agreement. Many different organizations have different understandings of what the term means,
consequently adopting inconsistent or inappropriate cyber security incident response approaches.
The original government definition of cyber security incidents as being state-sponsored attacks on critical national
infrastructure or defence capabilities is still valid. However, industry – fuelled by the media – has adopted the term
wholesale and the term cyber security incident is often used to describe traditional information (or IT) security incidents.
This perception is important, but has not been fully explored – and the term cyber is both engaging and here to stay.
The two most common (and somewhat polarised) sets of understanding – as shown in Figure 2 below - are either that
cyber security incidents are no different from traditional information (or IT) security incidents – or that they are solely cyber
security attacks.

Many respondents to the project questionnaire felt that there is a need to differentiate between a cyber security attack,
which requires a more modern approach (both technically and holistically) - and other types of information security incident
that can still be addressed by traditional incident handling approaches (often forensics or law enforcement led).

The main difference between different types of cyber security incident appears to lie in the source of the incident (eg a
minor criminal compared to a major organised crime syndicate), rather than the type of incident (eg hacking, malware
or social engineering). Therefore, it may be useful to define cyber security incidents based on the type of attacker, their
capability and intent.
At one end of the spectrum come basic cyber security incidents, such as minor crime, localised disruption and theft.
At the other end we can see major organised crime, widespread disruption, critical damage to national infrastructure and
even warfare.
Some of the most common ways in which different types of cyber security incident can be compared are outlined in
the table below – but they can vary considerably for any given incident, with many different groups attacking many
different targets.

Most of these do not apply to traditional cyber security incidents, where organizations would typically have to support
themselves (possibly with some help from the police). It is not the Government’s role to protect every organization against
cyber security attacks. They will provide assistance in responding to major cyber security attacks – or to help protect
national defence and critical national infrastructure. They also provide guidance on how to respond to other types of cyber
security incident, it is not their role to get actively involved in the actual response.
All types of attack - be they basic or advanced - will utilise similar attack vectors (eg hacking, malware, social engineering)
to carry out attacks, but with very different levels of sophistication, scale and resourcing. Furthermore, the nature of attacks
is changing from public displays of capability to targeted attacks designed to be covert. Depending on the nature of the
cyber security incident, the types of attacker shown to fall into one category (eg insider, hacktivist) may actually fall into the
other category.

Cyber security attacks are closely related to the agent (or actor) responsible for the attack, typically malicious third parties,
but can include insiders. There are also a range of more specific (or related) threats, such as those from crafted malware,
blended threats and phishing attacks.

Project research revealed that most basic attacks (such as those crafted by small-time
criminals, random hackers and most Hactivists) could be dealt with by many suppliers.

When it comes to identifying and responding to suspected information, IT or cyber security incidents, most organizations
treat them in the same way until some sort of investigation has taken place. Consequently, typical comments made during
project research suggested that: Cyber should be threaded through all incident response, not identified separately; A cyber security incident feels more like an attack than a business continuity issue, but at what point does an incident
move from any other type of attack to a cyber security attack?
This guide will help organizations respond to all types of cyber security incidents, including traditional information (or IT)
security incidents. More focus will, however, be placed on preparing for, responding to and following up cyber security
attacks, highlighting key points in this area.

Cyber criminals innovate just as business does and the potential rewards for them grow as business use of cyberspace
grows. They have access to powerful, evolving capabilities, which they use to identify attack and exploit carefully
chosen targets. They also have well-developed marketplaces for buying and selling tools and expertise to execute
sophisticated attacks.
When looking at a cyber security attack in more detail there are often a number of phases that attackers will undertake,
which can sometimes take place over a long period of time. An example of the basic components of such a phased
approach is outlined in Figure 3 below, together with some of the common countermeasures for each phase.

When dealing with a sophisticated cyber security attack, it is important to address all stages carried out by an attacker, be
they cybercriminals, extremists or state-sponsored agents. However, many organizations do little or nothing before phase
two of an attack, often because they do not have the awareness, resources or technical skills to tackle issues during the
reconnaissance stage.

The term advanced persistent threat (APT) usually refers to a group, such as organised crime syndicates, nation
states, state-sponsored groups of individuals or extremist movements, who have both the capability and the intent to
persistently and effectively target a specific entity. The term is commonly used to refer to both cyber security threats
and incidents, in particular that of Internet-enabled espionage, using a variety of intelligence gathering techniques to
access sensitive information.

Advanced Persistent Threat (APT) was not originally intended to be the generic term
that it is today. It was developed to refer to specific, known, state-sponsored groups
that conducted attacks against specific targets – often aimed at obtaining information
to enable either political or commercial advantage.
APT has not tended to refer to organised crime, but these days we are seeing more
organised crime attacks that utilise APT style techniques and tools.
The number of APTs is increasing rapidly, many of which are being used for corporate espionage or state-sponsored
attack. These attacks follow the broad anatomy of any attack outlined on the previous page, but specifically
comprise: Intelligence gathering (eg conduct detailed research into a target); Initial exploitation (eg carry out initial attack and establish foothold); Command and control (eg achieve persistent access that can survive a re-boot of the system and move
to new systems); Privilege escalation (eg gain system administrator rights on target systems); Data exfiltration (eg gather and remove (or copy) target data).
This APT life cycle can also be used to describe attacks by other sophisticated attackers, but the essence of an APT
attack is its targeted nature.

Many targeted APTs (and some non-targeted ones) initiate communication from
within the network, often to circumvent firewalls (which will block suspicious
inbound connections but will not routinely block responses to something that
started inside). They use standard ports and protocols to hide within obvious/
allowed traffic, thus defeating most Intrusion Detection Systems (IDS) and Intrusion
Prevention Systems (IPS).

The main challenges in cyber security incident response
In the commercial world (and often in governments), even large organizations can have significant difficulty in responding
to cyber security incidents, particularly sophisticated cyber security attacks.

Top management in organizations often do not believe that they are at risk from a
cyber security incident and are unaware of (or unconvinced by) the level of business
impact that could result. Even if they provide support during an attack, they can
then withdraw this soon afterwards, refusing to acknowledge that they could be
badly hit again.

For small organizations, one of the biggest problems can be in implementing effective cyber security controls, often due
to a lack of awareness, experience, or simply because they are expensive. For larger organizations, many of them will have
IT staff who can deal with most cyber security incidents. However, the increasing use of cloud computing – not always
supported by appropriate controls or service level agreements (SLAs) – can hamper their cyber security response capability.

Top management in organizations often do not believe that they are at risk from a
cyber security incident and are unaware of (or unconvinced by) the level of business
impact that could result. Even if they provide support during an attack, they can
then withdraw this soon afterwards, refusing to acknowledge that they could be
badly hit again.

So how do we respond?
There are many difficulties facing organizations when determining how to prepare for, respond to and follow up a security
incident, be it a simple virus or a sophisticated cyber security attack.

There is specialised support available for government departments. They assist public sector organizations in the response to computer security incidents
and provide advice to reduce the threat exposure. They gather data from all available sources to monitor the general threat
level. For these reasons the early reporting of incidents and attempted attacks is highly recommended.
However, there is limited publicly available support in the private sector, where it is more common for commercial suppliers
of cyber security incident response experts to be employed.

Many organizations do not treat cyber security incident response any differently to other forms of information or IT
security incident response, which may be reasonable for traditional types of cyber security incidents.

A traditional response to an attack on a Domain Controller (DC) could be to take
images of certain components – and then recover and re-build the DC from a backup. This will not help if the attacker already has sysadmin privileges for the DC as
they can just start again.

organizations of all types are struggling to deal with cyber security incidents effectively, with a growing number of cyber
security incidents now taking place on a regular basis – and causing significant business impact.
Many larger organizations can respond to traditional cyber security incidents themselves, sometimes very successfully – but
smaller organizations would typically need expert help. However, when it comes to dealing with a sophisticated cyber
security attack virtually all organizations should consider employing the services of one or more specialist third party cyber
security incident response providers for at least some activities (eg investigating advanced types of cyber security attack or
analysing evidence of unusual occurrences). For example, upon discovery of a cyber security incident, these specialists can
evaluate the situation and undertake the most appropriate actions to enable fast recovery from the incident, and to help
prevent reoccurrence.


There are many benefits in procuring cyber security incident response services from a trusted, certified external company
who employ professional, ethical and highly technically competent individuals.
Building a cyber security incident response capability
Dealing with cyber security incidents – particularly sophisticated cyber security attacks – can be a very difficult task, even for
the most advanced organizations. You should therefore develop an appropriate cyber security incident response capability,
which will enable you to adopt a systematic, structured approach to cyber security incident response, including the
selection and management of external suppliers.


Most organizations need professional help in responding to a cyber security incident
in a fast, effective manner, be it for all of their cyber security response capability - or
just specialised areas like technical or forensic investigations; situational awareness;
and advanced data analytics.

When dealing with a cyber security incident, one of the most important actions is
to be properly prepared. This will help you to recover your systems more quickly,
minimise the impact of the attack, instil confidence in your customers and even
save you money in the long term. This first phase is crucial, but can easily be
overlooked because of a lack of awareness, support or resources.

To be effectively prepared, you should be able to determine the criticality of your key assets; analyse threats to them; and
implement a set of complimentary controls to provide an appropriate level of protection. Considering the implications of
people, process, technology and information; you can then update your cyber security response capability and review your
state of readiness in cyber security response.

Project research revealed that the five main challenges faced by organizations when making the necessary risk assessment
and awareness arrangements to help them prepare for a cyber security incident are: Defining their critical information assets; Determining which cyber security threats are most likely to affect these critical information assets; Applying the relevant management or technical controls to reduce the likelihood and impact of cyber security
incidents affecting their critical information assets; Raising awareness about the need for an effective cyber security response capability; Determining the likely (or actual) level of business impact associated with a possible cyber security incident.

Research revealed that many organizations often did not know the criticality of their own assets and failed to carry out
business impact assessments, making it difficult to determine how to protect these assets before, during and after a cyber
security incident. You should therefore carry out a criticality assessment to identify your critical information assets (eg
important business applications, key systems and confidential data), for example in terms of their strategic or monetary value.
The potential harm that could be caused if your organization was hit by a cyber security incident should then be
determined. This is typically achieved by carrying out a business impact assessment – focusing on confidentiality, integrity
and availability – determining the level of business impact if: Sensitive information was disclosed to unauthorised parties (confidentiality); Important information was compromised (eg key data is inaccurate or wrongly processed); Critical systems or infrastructure were no longer available.
When determining business impact, it is often useful to consider scenarios and identify any serious implications in the event
of a cyber security incident compromising your critical assets.
Once you have identified your critical assets you should determine where they are located in your organization (and
beyond), and record important details about their level of criticality (eg critical, significant, minor or negligible). Finally, you
should assign responsibility for protecting these assets to capable, named individuals.

The next step in being prepared for a cyber security incident is to understand the level of threat to your organization from
different types of cyber security incidents, which is often achieved by carrying out a cyber security threat analysis.
To do this, you should first have produced a definition of what a cyber security incident means to your organization
and created a set of examples of the types of threats associated with these incidents, such as malware, hacking and
social engineering.
In order to contextualise the cyber security threat analysis, you will firstly need to gain a solid understanding of the: Nature of your business, business strategy, business processes and risk appetite; Key dependencies your organization has; for example on people, technology, suppliers, partners and the environment
in which you operate; Assets that are likely to be targeted, such as infrastructure, money, intellectual property or people – and the computer
systems that support them; Potential compromise to the confidentiality of sensitive information; the integrity of important business information
and applications; or the availability of critical infrastructure.

Delegates at the project workshop believed that realistic scenarios and rehearsals were effective ways of carrying out
threat analysis. A good testing method would typically include initiating a fictional (but realistic) attack internally and
assessing how well you can respond to it.
You should therefore engage in periodic scenario-based training, working through a series of attack scenarios
fine-tuned to the threats and vulnerabilities your organization faces. These scenarios also help ensure that relevant
individuals understand their role and help prepare them to handle incidents.

There are a number of traditional methods of carrying out threat analysis, and some
newly emerging ways of conducting more advanced cyber security threat analysis.
In particular, threat intelligence can play a key role in improving the effectiveness of
cyber security threat analysis.
Some specialised vendors – including a number of CREST members – are investigating
cyber security threat intelligence, but more research needs to be done in this area.

organizations often do not have a formal cyber security incident response team or even a named
individual who is responsible for dealing with such an incident. More important can be that there is
often a lack of technical expertise and nobody available who can take business decisions quickly.

Many organizations do not have adequate processes or methodologies (if they have any at all) to
help them deal with cyber security incidents in a fast, effective and consistent manner. They struggle
to know what to do, how to do it, who to contact – and can even compromise investigations by
their actions.

Many organizations have not configured their systems or networks to help them identify or respond
to cyber security incidents, with inadequate monitoring processes in place. In particular, systems
may not have been configured to record appropriate events, identify possible attacks or provide
adequate assistance to investigators.

organizations seldom have information readily available that will help the cyber security incident
response team (including third party experts) to respond quickly and effectively, such as details
about business management; IT infrastructure; key suppliers; sensitive data; and event logging.

Project research has shown that organizations often do not have a formal cyber
security incident response team – or even a point of contact for handling the
incident. Nor do they have the budget, resources, technical expertise or support
required to respond to cyber security incidents effectively.

Furthermore, cyber security attackers often exploit the people factor (eg by using
spear phishing or other social engineering techniques) through the use of common
hacking tool kits freely available in the public domain. Consequently, every person
in your organization will need to be aware of the risk from cyber security attacks – and be shown how to help reduce
the likelihood and frequency of these attacks.

Your incident response team can consist purely of your own staff, be completely
outsourced to a third party or – more typically in today’s world – involve a
combination of both.
However, many organizations have difficulty in determining whether to establish a specialised cyber security incident
response capability; or integrate cyber security incidents into existing incident management processes.
To deal with cyber security incidents effectively, many organizations will need to be able to integrate their response
mechanism far more widely across the organization, not just through the IT department (eg the IT help desk). For
example, many third parties (eg suppliers, partners and customers) now have a significant effect on organizations.
Furthermore, IT services are often used directly by business units, such as through cloud computing and the use of
social networking. Cyber security attacks can use all of these avenues, so an integrated response approach
is recommended.
For serious cyber security attacks, both top management and a specialised crisis management team (or equivalent)
would also need to be involved, often as part of an escalation process.
The people held responsible for dealing with cyber security incidents are typically in
IT and information security, but may not have sufficient resources or support, even
operating in a blame culture where they fear for their jobs.
Project research identified that many organizations do not have adequate policies,
processes or methodologies (if they have any at all) to help them respond to cyber
security incidents effectively. They struggle to know what to do, how to do it, who
to contact – and can even compromise investigations by their actions.

To help tackle cyber security incidents in an effective and consistent manner, you
should develop an appropriate strategic approach, backed up by a formal cyber
security incident response process, which should include: Identifying cyber security incidents; Investigating the situation (including triage); Taking appropriate action (eg contain incident and eradicate cause); Recovering systems, data and connectivity.

By taking the wrong initial action when a cyber security attack occurs (eg taking
systems off the network or cleaning up systems) you could create a detrimental
affect like alerting an attacker or destroying vital evidence).

Project research identified that nearly all organizations are likely to use a standard security incident management
process, but with cyber security attacks often being dealt with by a major incident response team (or similar).
Whatever approach is adopted, a clear methodology and plan should be established to help you respond to cyber
security incidents in a fast, effective, consistent manner.
Whilst every situation is unique, there are commonalities that allow for a standardised plan that you can proactively
implement and adapt as needed. The plan should be sufficiently comprehensive and agile to cover, and adapt to,
many different scenarios, often meaning that it will need to be written at a higher level.
However, the use of standard incident response plans can be a difficult topic for suppliers of cyber security incident
response expertise to deal with as the response technique is seldom a linear set of steps and more a set of decisions.

Expert suppliers of cyber security incident response services can help you develop an
appropriate process – or implement their own tailored version.
You should appoint a suitable supplier(s) in advance, who is ready to help at short
notice, as required (for example by keeping third parties on a retainer for times of
need). Should you suffer a cyber security incident, you will then be able to undertake
full-fledged breach investigation and eradication quickly and effectively.

Project research revealed that the biggest IT infrastructure challenge faced by
organizations when making the arrangements to help them prepare for a cyber
security incident is in failing to log the right events or turn on the appropriate
logging features.

Many organizations have vastly insufficient logging, archiving, correlation and
simulation capabilities. For example, when handling a cyber security incident,
historical data can be very important as attacks have often been taking place over
an extended period of time – but logs (if they record the right things at all) are
often incomplete or do not adequately cover past events.

You should combine key information from as many of the different logs as possible into one central repository,
such as a Security Information and Event Management (SIEM) system. For example, evidence of an incident may be
captured in several logs that each contains different types of data.

SIEM solutions are a combination of SIM (security information management) and
SEM (security event manager) systems. SIEM technology provides real-time analysis of
security alerts generated by network hardware and applications. SIEM solutions come
as software, appliances or managed services, and are also used to log security data
and generate reports for compliance purposes.

You should try to avoid providing internet access locally, rather than through a central
corporate gateway, otherwise you are likely to have no real logging capability and
very limited knowledge of your Internet points of presence (sometimes referred to as
exfiltration (or infiltration) points).

It is essential to make sure that your organization has the information readily
available that will help the cyber security incident response team (including third
party experts) to respond quickly and effectively. Depending on context, the kind of
information that expert suppliers typically want to know about falls into four
main categories:

The amount of information required by an organization will differ based on a number of factors, such as its size,
market sector, internal capabilities and nature of the particular cyber security incident being investigated.
organizations can overlook the need to gain fast access to facilities at their outsourced service providers (ie access
to premises or equipment). They often have difficulty in getting their third party suppliers (eg cloud service suppliers,
infrastructure outsourcers and managed service providers) to provide important information (eg event logs) pertaining
to their cyber security incident, sometimes having to wait for several days for something to be actioned.
To operate effectively and efficiently during a cyber security incident investigation, organizations should establish
relationships with important third parties in advance of a breach. These third parties may include business
relationships, joint ventures, individuals with a link into the network, contractors and anyone else who would be
impacted if your organization had to operate in a degraded capacity.
Once these parties are identified, their contact information should be retained and kept easily accessible by
the appropriate individuals, including technical security specialists, business representatives and the Crisis
Management Team.

Advanced cyber security attack or not, many organizations struggle to get the basics right, like establishing a patch
management policy – which could stop a large range of malware and make it more difficult for advanced cyber
security attackers.
Project research has revealed that there are a number of basic controls that you can implement to help reduce the
likelihood of a cyber security incident occurring in the first place, such as access control, firewalls, malware protection
and backups. Even if these basic technical controls do not actually prevent cyber security attacks, they can frustrate or
slow-down a determined attacker – providing further time for detection before the attack gets to a critical point.

The guidance provided is about getting the basics right. Where companies adopt these steps, it has made a tangible
difference to their vulnerability to cyber security attack. The document also includes a useful two page section
covering incident management.

Many cyber security attacks can now circumvent many traditional security controls,
such as malware protection and firewalls, with many cyber security attacks passing
through the defences of most signature-based products. organizations should
therefore consider using specialised APT prevention tools available in the market.
There are a number of specialised controls that seemed to be particularly helpful in reducing the likelihood of some types of cyber security attacks, such as: Multi factor authentication - something you know (eg a User ID and password) and something you have (eg an
access, bank or smart card); Digital certificates used to “sign” code from a vendor so that the code can be trusted; Whitelisting (defining all acceptable ports, addresses or similar – and preventing all other access) or blacklisting (preventing access from specific sites, or addresses); Technical monitoring tools, such as intrusion detection or prevention systems (IDS and IPS), data loss preventions (DLP) systems and searchable incident event repository (SIEM).

Even specialised controls are now being defeated. For example, some attackers have
been able to break into an application whitelisting vendor and have its code-signing
infrastructure sign the malicious code so that they are effectively on the whitelist.

It is important that your organization maintains an appropriate cyber security incident response capability. This should
consist of appropriately skilled people guided by well-designed processes that enable the effective use of relevant
technologies. Having the right capability can help you to conduct a thorough investigation and successfully eradicate
adversaries who are deeply embedded in your environment.
However, many organizations do not know their state of readiness to be able to respond to a cyber security incident
in a fast, effective manner.

Different types of organization will require different levels of maturity in cyber security incident response. For example, a
small company operating in the retail business will not have the same requirement – or ability – to respond to cyber security
incidents in the same way as a major corporate organization in the finance sector – or a government department.
Consequently, you should review the level of maturity your organization has in cyber security incident response and
compare it to your actual requirements for such a capability. You can compare the maturity of yours with similar
organizations to help determine if this level of maturity is appropriate for your organization.
The maturity of your cyber security incident response capability can play a significant role in determining the level of thirdparty involvement during a breach investigation and eradication event. organizations with mature cyber security incident
response capabilities may conduct most of their operations in-house, while those who are less mature may depend entirely
on third parties.

There are a number of common steps that cyber security incident response
experts typically follow to help them handle an incident effectively, which should
be part of a wider approach, with an emphasis on investigation. Consequently,
to provide you with a broader understanding of a typical live situation, the four
following steps have been developed.

organizations often treat a cyber security incident as if it is a single one-off event.
In reality, for most sophisticated incidents, they have been going on for some time
(including reconnaissance) and / or cover more than one part of the organization.

For many organizations, the most challenging part of the incident response process is accurately detecting and
assessing possible cyber security incidents - determining whether an incident has occurred and, if so, the type, extent,
and magnitude of the problem.

You will need to detect cyber security incidents, analyse them at a high level and confirm what type of incident has
actually occurred, if any. Some incidents have overt signs that can be easily detected, whereas others are almost
impossible to detect.
Many cyber security incidents are about stealing critical / confidential data – often state-sponsored or organised by
cybercrime gangs – that are looking to obtain intellectual property or other sensitive information. As a result, they
are mostly non-destructive (although some are very destructive), unobtrusive and difficult to detect (often because
attackers have covered their tracks).
Cyber security incidents may also take place over a long timeframe and / or in different parts of the organization.
Advanced targeted attacks can go undetected for many months or years, and even when discovered are often
assumed to be nothing more than a common malware infection. Equally, many variants of credential stealing Trojans
can remain undetected for many months at a time.
There are many different ways in which a cyber security incident can be identified (with varying levels of detail and
accuracy), which include: Alerts generated by technical monitoring systems, such as Data Loss Prevention (DLP), intrusion detection
systems (IDS), antivirus software, and log analysers; Suspicious events reported, for example, to the IT help desk by users; to account managers by third parties
(often customers); or directly to the security team by industry bodies, your vendor partners or the government; Anomalies detected by audits, investigations or reviews.


Cyber security incidents can be detected in any part of the organization – or through third parties. You should
therefore ensure your cyber security incident response process is sufficiently broad and emphasise the importance of
incident detection and analysis throughout the organization.
Users should be informed that they should: Report all suspected cyber security breaches to a central point (eg information failures; loss of services;
detection of malicious code; denial of service attacks; errors from incomplete or inaccurate business data); Note all important details (eg type of breach, messages on screen, details of unusual occurrences); Restrain from attempting to take remedial actions themselves.

In an organization, thousands of possible signs of incidents may occur each day, recorded mainly by logging and
computer security software.
Indicators (there are many) can include: A network intrusion detection sensor alerts when
a buffer overflow attempt occurs against a
database server; Antivirus software alerts when it detects that a host is
infected with malware; A system administrator sees a filename with unusual
characters; A host records an auditing configuration change
in its log; An application logs multiple failed login attempts
from an unfamiliar remote system; An email administrator sees a large number of
bounced emails with suspicious content; A network administrator notices an unusual deviation
from typical network traffic flows.

Security software (eg IDS, IPS, DLP, SIEM,
antivirus and spam software, file integrity
checking software, monitoring services
(often provided by a third party)
Logs (eg operating system logs, service and
application logs, network device logs and
network flows)
Publicly available information (eg
information on new exploits, information
exchange groups, third party organizations,
governments)
People form within your organization
Third parties (eg customers, suppliers,
IT providers, ISPs, partners; government
bodies).

Some of the main challenges facing organizations are often to do with monitoring the relevant events on their
systems and networks for signs of a cyber security attack. organizations often collect a lot of data, but do not have
the resources, technical skills or awareness to analyse data effectively.
In particular, IDS is not always given sufficient prominence, often seen as a ‘fit and forget’ solution.

Once a cyber security incident has been identified, the next stage is to define what the objectives are for the response
activities – and to investigate the situation in an appropriate manner.

During an investigation into a cyber security incident, it can be very useful to have access to cyber threat intelligence
- research into the attackers to determine their capabilities, motives and likely actions. This can be provided by the
government, CERTS, collaborative groups or expert third parties.
When a security team conducts and applies cyber threat intelligence, the team will more clearly understand the
tactics, techniques and procedures of the attackers and can defeat some attacks by disrupting or degrading their
efforts. Threat intelligence can also help you detect an incident during the reconnaissance phase, before you have
actually been attacked.

Cyber security attacks are often more critical than many traditional security incidents, but should still be subject to a
consistent classification process.

These incidents will usually cause the degradation of vital service(s)
for a large number of users, involve a serious breach of network
security, affect mission-critical equipment or services or damage
public confidence in the organization.


Less serious events are likely to impact a smaller group of users,
disrupt non-essential services and breaches of network security
policy.


Many minor types of incident can be capably handled by internal
IT support and security. All events should be reported back to the
information security team who will track occurrences of similar
events. This will improve understanding of the IT security challenges
and may raise awareness of new attacks.

It is not necessary to report on incidents with little or no impact or
those affecting only a few users, such as isolated spam or antivirus alerts; minor computer hardware failure; and loss of network
connectivity to a peripheral device, such as a printer.

The first people dealing with the incident are sometimes refrerred to as first responders, ideally as part of a team.
These first responders should be able to determine whether any specialist resources – including third parties will be required.
Many organizations do not have the right tools, systems or knowledge to conduct a suitable investigation. You
need to identify quickly when the scope and severity is beyond in-house skills, before decisions are made that
may adversely affect an investigation. It is critical for arrangements to have been made in advance so that expert
investigators are available at short notice and have enough prior information to be able to hit the ground running.

Whoever actually carries out all or part of the investigation, it is still your
responsibility, so you will need to monitor each step carefully – and record what
has happened.

As well as expert cyber security incident response experts, other third parties that you may wish to get involved can
include technology forensics specialists, technology analysts (for example, database experts), Information analysts (for
example, accountants), legal experts and on-site police support.
Some organizations set up a “war room” during serious cyber security attacks. This is the crisis management team’s
primary meeting and collaboration space, where all relevant parties (incident investigators, IT staff representatives,
stakeholders and other leaders) assemble to manage the incident from one central point.

In the early stages of investigating a cyber security incident, the precise nature of the incident may be unknown and
initial analysis will be required.


All types of event logs should be considered, including: Firewall/router logs (including proxy servers); Technical security monitoring logs and alerts (eg from intrusion detection (IDS) or
data loss prevention (DLP) software; Traditional Server and workstation logs; Business application audit logs; Web server logs; DNS and DHCP logs covering all devices; Email history and archives; Internet usage logs; Network data; Building access logs.
During an investigation these logs will provide valuable information and are often
requested by third parties.
When carrying out an investigation, each possible trigger event should be thoroughly investigated.

One of the first key actions to be taken after the initial investigation (and often as part of that investigation) is to
contain the damage being done by the cyber security incident, for example by stopping it from spreading to other
networks and devices both within your organization and beyond.
Containment typically comprises a number of concurrent actions aimed at reducing the immediate impact of the
cyber security incident, primarily by removing the attacker’s access to your systems. The objective of containment is
not always to return (directly) to business as usual, but to make best efforts to return to functionality as normal, while
continuing to analyse the incident and plan longer term remediation.


You should consider creating separate containment strategies for different types of major cyber security attack, with
criteria documented clearly to facilitate decision-making.
After an incident has been contained, eradication is often required to eliminate key components of the incident (eg
removing the attack from the network, deleting malware and disabling breached user accounts), as well as identifying
and mitigating vulnerabilities that were exploited.

Effective eradication plans must be executed with speed and precision because attackers often try to re-establish
a base and then entrench themselves again into the network once they sense they have been discovered and
eradication is underway.

Research indicated that organizations have significant difficulty in meeting forensic requirements for cyber security
incident response, such as in preserving evidence and maintaining a chain of custody.

It is essential that you maintain a chain of evidence for both paper-based and electronic information.
All forensic works should only be performed on copies of the evidential material (eg using imaging technology)
and the integrity of all evidential material must be protected. Furthermore, for many cyber security attacks, a more
detailed forensic investigation will be required. organizations should therefore consider employing third party
forensic experts.

The final step in responding to a cyber security incident is to restore systems to normal operation, confirm that the
systems are functioning normally, and remediate vulnerabilities to prevent similar incidents occurring.

It is important to validate that systems are operating normally again, which can often be achieved by carrying out an
independent penetration test of the affected systems, complemented by a security controls assessment.
Advanced cyber security attackers will often try to get back into the network through all of the methods at their
disposal. They will also come back knowing that they are being investigated and that their existing tactics, techniques
and procedures have been discovered. Therefore, it is important to ensure that all elements of the attack have been
eradicated and that the attackers cannot carry out further attacks.

Many members are able to help ensure that a cyber security incident has been
eradicated effectively – and help prevent attackers from returning.
To help detect further attacks, cyber security threat intelligence (including network situational awareness) should be
gathered and retained and the network monitored for any further attempted attacks. Monitoring may need to take
place over an extended time to detect any further attacks (or attempted attacks).
Once systems have been recovered and controls have been tested, stakeholders should then be provided with a
brief summary of what took place. The team should report that eradication was completed successfully and note
any exceptions and other significant findings. Briefings to stakeholders about the results should be well planned
and conducted soon after the event. An initial, high-level communication can be issued within a day or so of the
event, followed by a deeper explanation of the activities that took place, as described in

There are many important activities that should be undertaken following a cyber
security incident. In practice, some of these (often important) follow-up actions
may not be carried out due to insufficient resources, higher priorities, lack of
awareness or the pressing need to get the organization back on track, business
as usual.

Research indicated that the biggest challenges organizations face when following up a cyber security incident are: Conducting sufficient investigation (eg using deep dive forensic capabilities) to identify (and prosecute, if
appropriate) the perpetrator(s); Performing problem cause analysis; Carrying out root cause identification; Quantifying the business impact of the incident; Supporting criminal investigations; Performing trend analysis.

Reporting was also seen as a critical part of following up a cyber security incident, with few organizations admitting having
significant challenges in this area.

There is typically a need for you to investigate a cyber security incident more thoroughly after the event than when
responding in the ‘heat of the battle’. This will help you to find out what actually happened, improve controls, share data
with partners and prevent the incident from reoccurring.

You should carry out sufficient investigation to identify the perpetrators(s) of the cyber security incident, which may involve
specialist support, such as from forensic investigators.

Report the incident to relevant stakeholders.

Once a cyber security incident has been successfully handled, formal reporting will often be required to both internal and
external stakeholders.

Government Departments have a responsibility to report computer incidents (including cyber security incidents) under the
terms laid out in the Security Policy Framework (SPF). The categorisation is built primarily
around whether the Department has been specifically targeted or not.
In many cases there can also be benefits in voluntary reporting to other important stakeholders.
Important information about the cyber security incident should be discussed during a post incident review.

An essential part of following up a cyber security incident is to document, communicate and build on lessons learned. This
should be viewed as an on-going process through which you can collaborate and learn from previous mistakes, incidents
and experiences.
Communication to all stakeholders should be clear, concise and focused on problem resolution and control improvement. It
should clearly identify any gaps that remain and propose efforts to mitigate them.
An action plan should be created that explains how the organization will leverage lessons learned from the incident to
become more resilient in the face of future cyber security attacks. The action plan should include projects or initiatives,
technical and nontechnical, that will help reduce an attacker’s chance of success and respond to an attacker’s activities
more rapidly and effectively. Analysis of the cyber security incident should consider whether technical capability gaps
contributed to the attacker’s success or whether people or process gaps were the main culprit.
Each action should be assigned to a named individual and given a suitable priority and completion date. The status of all
action should then be monitored to ensure that they are being completed in a timely and effective manner.

You should use any lessons learnt to share both key issues and good practice across all
areas of the business, not just within IT and cyber security teams.

You should maintain records about the status of all security incidents (including cyber security incidents), along with other
pertinent information. You should review relevant cyber security incident data regularly to help: Evaluate patterns and trends of cyber security incidents; Identify common factors that have influenced cyber security incidents; Determine the effectiveness of controls (eg which controls are better at preventing, detecting and delaying cyber
security incidents or minimising their business impact); Understand the costs and impacts associated with cyber security incidents.

Generally there is nothing specifically different about archiving cyber security
incidents in comparison to archiving any other data, although: You may wish to search your archived data more often; Incident-related data is usually sensitive and you should apply appropriate security
mechanisms to protect it.

There are many reasons why an organization may wish to employ external cyber security incident response providers, such
as to help carry out activities outlined in previous sections.
Most organizations need professional help in responding to a cyber security incident in a fast, effective manner. However, it
is very difficult for them to identify trusted organizations that have access to competent, qualified experts who can respond
appropriately whilst protecting sensitive corporate and attack information.
Employing the services of properly qualified third party experts, can significantly help organizations to handle cyber security incidents in a more effective and appropriate
manner – particularly serious cyber security attacks.

The National Cyber Security Strategy sets a strategic objective of making the UK more resilient to cyber attacks. Such
attacks can vary in terms of persistence, sophistication and impact.

Select an appropriate supplier who can meet your requirements
If your organization decides to appoint an external provider of cyber security incident response services, it is important that
you choose a supplier who can most effectively meet your requirements – but at a reasonable cost.

It has been recognized that organizations suffering cyber security attacks often do not know where to go for help, or what
the quality of the service will be from the suppliers who provide expert response services. What organizations really need
is the ability to access demonstrably skilled, knowledgeable and competent individuals working for organizations that have
been independently assessed against best practice and who have the policies, processes and procedures in place to enact
recovery and protect confidential information.

Project research identified ten key findings about cyber security incident response in general, which could be important
to your organization.
Cyber security incidents, particularly serious cyber security attacks, such as advanced persistent threats (APTs),
are now headline news.

There is no common understanding (or taxonomy) of what constitutes a cyber security incident, with no
definitive set of threats.

The original government definition of cyber security incidents as being state-sponsored attacks on critical
national infrastructure or defence capabilities is still valid. However, industry – fuelled by the media – has
adopted the term wholesale and the term cyber security incident is often used to describe traditional
information (or IT) security incidents.
The main difference between different types of cyber security incident appears to lie in the source of the
incident (eg a minor criminal compared to a major organised crime syndicate), rather than the type of incident
(eg hacking, malware or social engineering).

Few organizations – of any type - are well prepared for a cyber security incident in terms of people, process
and technology and in the information needed to respond effectively.

In practice it is often very difficult for organizations to identify the type of cyber security incident they are
facing until they have carried out an investigation.

organizations vary considerably in terms of the level of maturity in their cyber security incident response capability, but also in the way in which they need to respond.

Despite the current level of threat from cyber security incidents, those responsible for preparing for,
responding to and following up cyber security incidents in many organizations still face significant challenges,
for example in terms of budget, resources, technical skills, support and influence.

Most organizations need professional help in responding to a cyber security incident in a fast, effective
manner. However, it is very difficult for them to identify trusted organizations that have access to competent,
qualified experts who can respond appropriately whilst protecting sensitive corporate and attack information.


Employing the services of properly qualified third party experts (such as CREST members), can significantly
help organizations to handle cyber security incidents in a more effective and appropriate manner – particularly
serious cyber security attacks.

Access points into an enterprise have dramatically expanded. Cloud services, social media, mobile devices and bring your
own device (BYOD) policies have moved the corporate perimeter through firewalls, devices and applications down to the
most sensitive data that should be protected.
Creative, talented and aggressive attackers continue to drive the threat world into new areas.
The threat landscape will continue to evolve, with new and innovative attack methods being able to adapt to their chosen
target environment(s). Furthermore, future attacks will be ever more likely, using automated tools, to compromise hundreds
of thousands of computers around the globe.
Even in today’s world it will not be possible to prevent all cyber security incidents. As attackers adapt and change,
organizations will need to adapt and change as well. You should therefore prepare for an attack executed by an advanced,
sophisticated, organised, well-funded and persistent adversary.

Nations of the world are giving high priority to implementing cyber security strategies that will both
improve their resilience to cyber security incidents and (where possible) reduce the impact of cyber security attacks.

Public
and private sector analysts will be joined by members of intelligence agencies, law enforcement and
government IT as they exchange information and techniques and monitor cyber security attacks in
real time.
The Cyber Security Information Sharing Partnership
(CISP) also includes a secure web portal and programmes aimed at building cross-sector trust to underpin
information sharing. The web portal is based on a social networking structure, giving members of the
CISP the freedom to choose who they wish to share information with in real time.

organizations are seldom adequately prepared for a serious cyber security incident. They often suffer from a lack of
budget, resources, technology or recognition of the type and magnitude of the problem. In addition, they do not have the
software, testing, process, technology or people to handle sophisticated cyber security threats, such as Advanced Persistent
Threats (APTs).
However, you can respond to cyber security incidents in a faster, more effective manner.

The theft of private, financial, or other sensitive data and cyber attacks that damage
computer systems are capable of causing lasting harm to anyone engaged in personal or commercial online transactions. Such risks
are increasingly faced by businesses, consumers, and all other users of the Internet.
A private sector entity that is a victim of a cyber incident can receive assistance from government agencies, which are prepared to
investigate the incident, mitigate its consequences, and help prevent future incidents. For example, federal law enforcement agencies
have highly trained investigators who specialize in responding to cyber incidents for the express purpose of disrupting threat actors
who caused the incident and preventing harm to other potential victims. In addition to law enforcement, other federal responders
provide technical assistance to protect assets, mitigate vulnerabilities, and offer on-scene response personnel to aid in incident
recovery. When supporting affected entities, the various agencies of the Federal Government work in tandem to leverage their
collective response expertise, apply their knowledge of cyber threats, preserve key evidence, and use their combined authorities and
capabilities both to minimize asset vulnerability and bring malicious actors to justice. This fact sheet explains when, what, and how to
report to the Federal Government in the event of a cyber incident.
When to Report to the Federal Government
A cyber incident is an event that could jeopardize the confidentiality, integrity, or availability of digital information or information
systems. Cyber incidents resulting in significant damage are of particular concern to the Federal Government.
A cyber incident may be reported at various stages, even when complete information may not be available. Helpful information could
include who you are, who experienced the incident, what sort of incident occurred, how and when the incident was initially detected,
what response actions have already been taken, and who has been notified.
How to Report Cyber Incidents to the Federal Government
Private sector entities experiencing cyber incidents are encouraged to report a cyber incident to the local field offices of federal law
enforcement agencies, their sector specific agency, and any of the federal agencies listed in the table on page two. The federal agency
receiving the initial report will coordinate with other relevant federal stakeholders in responding to the incident. If the affected entity
is obligated by law or contract to report a cyber incident, the entity should comply with that obligation in addition to voluntarily
reporting the incident to an appropriate federal point of contact.
Types of Federal Incident Response
Upon receiving a report of a cyber incident, the Federal Government will promptly focus its efforts on two activities: Threat Response
and Asset Response. Threat response includes attributing, pursuing, and disrupting malicious cyber actors and malicious cyber
activity. It includes conducting criminal investigations and other actions to counter the malicious cyber activity. Asset response
includes protecting assets and mitigating vulnerabilities in the face of malicious cyber activity. It includes reducing the impact to
systems and/or data; strengthening, recovering and restoring services; identifying other entities at risk; and assessing potential risk to
the broader community.
Irrespective of the type of incident or its corresponding response, Federal agencies work together to help affected entities understand
the incident, link related incidents, and share information to rapidly resolve the situation in a manner that protects privacy and civil
liberties.

Report cybercrime, including computer intrusions or attacks,
fraud, intellectual property theft, identity theft, theft of trade
secrets, criminal hacking, terrorist activity, espionage,
sabotage, or other foreign intelligence activity to FBI Field
Office Cyber Task Forces.

Report suspected or confirmed cyber incidents, including when
the affected entity may be interested in government assistance
in removing the adversary, restoring operations, and
recommending ways to further improve security.


In 2015, global cybersecurity spending is
expected to reach an all-time high of $76.9
billion.1 However, the majority of IT executives anticipate receiving only half of the
funding necessary to execute their preferred
organizational security strategy.2 This disconnect is common when it comes to capital
planning, but the risk of underfunding cybersecurity defenses has never been greater.
On average, US businesses fall victim to
1.7 successful cyber attacks per week3
and incur annual costs of $12.7 million to
remediate the impacts of these events. The
frequency, complexity, and costs associated
with attacks is also increasing, with financial damages up nearly 10 percent in the
last year alone. Despite these trends, many
organizations are reluctant to increase
cybersecurity spending because they are
unable to accurately quantify the financial
value of prospective investments. However,
with the right methodology and tools, these
organizations can make more calculated
cyber investment decisions and channel
available funds to address the highest
priority security needs.

Historically, the financial benefits of cyber
technology implementation have not
been calculated with the same financial
discipline used to evaluate other material
investments. This was mainly due to a lack
of readily available data and systematic
methodology to support the efficacy of
cyber investments. This gap has prevented
managers from being able to formulate
generally accepted financial metrics—such
as return on investment (ROI), net present
value (NPV), and breakeven period—to
communicate the value of cybersecurity
projects and defend spending decisions.
The path to resolving these challenges
begins with the appointment of a Chief
Information Security Officer (CISO).
The CISO provides the vision and functional
knowledge to shape an organization’s cybersecurity strategy, establish a cyber budget,
and develop controls to protect sensitive
information. However, the CISO must also
secure funds to carry out the organization’s
cyber mission. Without this funding, the
benefits of the organization’s cyber functions cannot be realized. It is for this reason
that CISOs need a system of tools that can
formulate a defensible measure of financial
performance, provide sufficient transparency to justify capital allocations toward
cyber investment, and operationalize capital
planning activities for cyber functions.
Fortunately, the industry has increased
efforts to collect, analyze, and publish cyber
incident data. Improved access to this information has enabled businesses to advance
their understanding of cybersecurity and
awareness of the threats and consequences related to cyber breaches. Armed
with this new information and the CISO’s
leadership, businesses can now develop a
standard methodology for quantifying the
benefits of cybersecurity investments.

An effective cybersecurity investment
planning process must be strategic,
logical, defensible, and repeatable.
A Cyber ROI capability helps CISOs
meet all four criteria by providing a
standard framework for implementing a
cyber investment management system.

An effective cybersecurity investment
planning process must be strategic,
logical, defensible, and repeatable. A
Cyber ROI capability helps CISOs meet all
four criteria by providing a standard framework for implementing a cyber investment
management system. When
properly applied, the Cyber ROI methodology will quantify the value of cyber
investments, produce a robust analytical
framework that resonates with an organization’s most senior leaders, increase
information transparency to regulatory
authorities, and operationalize the cyber
capital planning process. There are five
key steps to establishing Cyber ROI.


Understanding the enterprise’s existing
security framework and how it currently
identifies, protects, and responds to cyber
threats is an absolutely critical first step
in the Cyber ROI process. This establishes
the baseline to which all subsequent
cyber investment decisions are compared.
However, a security framework alone is not
sufficient to drive cybersecurity success.
Instead, organizations should develop a
cyber value chain that accurately reflects
both the operating model and core functions
of the business. To ensure the approach is
holistic, value chain analysis should take
both an internal and external perspective,
and include activities such as determining
key control groups, identifying security
gaps, and collecting relevant benchmark
data. Trusted industry frameworks, such
as the National Institute of Standards
and Technology (NIST) and the Federal
Information Security Management Act
(Financial institutionsMA), can be used to guide this process,
but the final analysis should be customized
to account for organizational specifics like
industry, size, region, product mix, and the
desired state of cyber operations.


Once the value chain is clearly established, and security gaps and requirements are understood, the enterprise can
begin to address the issue of cyber investment impact. The organization should start by developing a list of possible cyber
projects that address security needs. Each
potential project should then be analyzed
against the control groups defined within
the value chain. Doing so will help assess
the incremental functional impact of each
potential project and prioritize initiatives
for investment analysis.

With project parameters defined, the organization can proceed to place a value on
each investment opportunity. Many aspects
of cyber investment financial value are the
same as those for any traditional investment (e.g., costs, benefits, exposure, and
risk). The differentiating factor, however,
is that cyber investment value is based
on three key cost avoidance components:
(1) Cost to Fix, (2) Opportunity Cost, and
(3) Equity Loss.
Cost to Fix – The real costs required to
remediate a successful cyber attack. Nearly
one in five organizations is expected to
experience a cyber breach involving a
minimum of 10,000 records within the next
2 years. Thwarting these attacks and their
impacts results in a substantial cost avoidance to affected organizations.
Opportunity Cost – The potential benefits
lost due to a successful cyber attack. In
many cases, organizations are significantly
hampered or completely shut down by
cyber incidents, rendering them unable
to operate at normal capacity. Depending

When properly applied, the Cyber ROI
methodology will quantify the value of
cyber investments, produce a robust
analytical framework that resonates with an
organization’s most senior leaders, increase
information transparency to regulatory
authorities, and operationalize the cyber
capital planning process.
on the duration of the event, organizations may experience substantial adverse
economic impacts in the form of lost
revenue, brand awareness, market position, and even customers. Proper cyber
mitigation ensures that the majority of
these costs can be avoided.
Equity Loss – The direct and indirect capital
damages incurred due to a successful cyber
attack. Most effective attacks result in
the loss of data and/or records. In some
instances, this information—such as intellectual capital, trade secrets, or patents—is
extremely valuable. Compromise of this
information not only inflicts direct harm on
the organization, but may also trigger subsequent losses such as declines in market
capitalization due to investor response,
lawsuits, or leadership turnover. The right
mix of cyber investment can protect an organization’s most sensitive information, thus
limiting potential equity losses.

Loss associated with cyber attack
completes the picture of anticipated
benefits through cost avoidance from cyber
investments. This knowledge then enables CISOs to present other
executives with clear, justifiable, and traditional measures of financial investment.
In addition, knowing the financial value of
individual investments, as well as their
alignment to the organization’s cyber value
chain, enables CISOs to conduct portfoliolevel analyses that help identify the optimal
set of cyber projects to suit business and
security needs.

Producing value metrics is without question
the most challenging piece of the Cyber ROI
process, but delivering actionable information to executives is equally important. In
doing so, CISOs help increase awareness of
cybersecurity issues throughout the enterprise and establish credibility in defending
cyber initiative recommendations to senior
leaders. Armed with a set of clear financial
metrics, a robust analytical methodology,
and hard data to support reasoning, CISOs
can clearly articulate the value of cyber
projects to decision makers and make a
compelling case for increasing investment
in cyber initiatives.

While Cyber ROI can be deployed as
a one-time solution, it should truly be
implemented as an ongoing process and
integrated with strategic and capital planning activities. Doing so ensures that the
organization advances its cybersecurity
capabilities and remains a step ahead of
threat actors attempting to violate privacy
rights and compromise critical information.
To achieve this level of adoption and inclusion, it is not only important to formulate the process, but institutionalize it.
Guidelines should be established to assign
data ownership and document governance
and reporting activities. This responsibility
should lie with the CISO, but also incorporate relevant stakeholder groups (e.g.,
data stewards, IT administrators, divisional
managers) across the enterprise to foster
collaboration and learning. Organizations
that follow these recommendations will
benefit not only from the analysis and
output, but also a set of defined procedures that help frame and inform cyber
investment decisions over time.

Cyber ROI provides Chief Information
Officers (CIO) and CISOs with a methodology and tool to meet pressing cybersecurity needs. Each step in the Cyber ROI
process is specifically designed to address
a critical component of cyber investment
evaluation, starting with value chain analysis and culminating in the implementation
of a defined cyber-focused capital planning
framework. Most importantly, Cyber ROI is
proven in its ability to deliver results. We
have deployed Cyber ROI across the spectrum of cyber maturity, often post-breach
or after a major cyber incident, with great success. Cyber ROI has helped a CISO of
a major infrastructure company achieve
a 70 percent increase in budget over
5 years, a healthcare provider justify a
single year investment of $53 million to
improve the organization’s continuous
monitoring capability, and a CISO at a
major utility identify targeted investments
to reduce end user device risk exposure
by 54 percent. With Cyber ROI’s ability to
confidently justify, prioritize, and operationalize cyber investment strategies, senior
executives can position their organizations
to proactively thwart cyber attacks and
protect stakeholders’ valued interests.

Each step in the Cyber ROI process is
specifically designed to address a critical
component of cyber investment evaluation,
starting with value chain analysis and
culminating in the implementation of a
defined cyber-focused capital planning
framework.

Cybersecurity is one of the most important aspects of managing the modern enterprise,
with duties and responsibilities extending through every level of the workforce. The
central nervous system of any organization—its data, systems and infrastructure—
depend not just on secure technology but also on the quality and skills of the people
entrusted with managing and operating it. As a human resource (HR) professional, you
play a critical role in ensuring that your organization is prepared to manage escalating
operational, financial, reputational, and legal risks related to cyber attacks through
effective workforce planning and management.
Cybersecurity is no longer just an information technology (IT) issue; it is an enterprisewide issue with implications for everyone. Cyber risk is now one of the top concerns for
board directors and executives as they manage the full spectrum of enterprise risks. HR
has always had an important role in managing risks—from natural disasters to layoffs,
lawsuits, and workplace violence—and cyber risk is no different: you have a role to play.
A human capital crisis
The stakes are high. For individual companies, the global average annualized cost of
cyber incidents stands at $7.7 million, although in many regions, such as the United
States (at $15.4 million), the average costs are higher, and many individual companies
must bear costs much greater still, while an estimated 80% of the value of S&P 500
companies is based on intellectual propertyiv—which is often vulnerable to cyber attack.
More broadly, the critical infrastructure we all depend upon for daily life—from
electricity and water to emergency response services—are increasingly dependent upon
interconnected IT, and thus vulnerable. Indeed, cyber attack has risen to become a top
national security concern, according to the senior American intelligence official. At the
same time, there is a critical shortage of people with the requisite knowledge, skills and
abilities to address emerging threats.
The shortage of cybersecurity talent is unique in its intensity, scope and potential
impact. The speed at which the world became connected through the Internet—from
individuals to organizations—far outpaced the ability to produce professionals who can
adequately secure the data, systems and critical infrastructure that is now dependent on
it. Traditional sources of professional talent, such as four-year universities, are lagging
far behind.

This is a problem of global proportions. There is already a shortage of over one million
professionals worldwide, and the gap is expected to widen to more than 1.5 million by
2019. In the United States alone, there are nearly 250,000 postings annually for
cybersecurity-related jobs, many of which go unfilled. This shortage is exacerbated by
a lack of clarity and consistency in competency models, job descriptions, and
certification, training and education standards for cybersecurity professionals. Finding
the right talent will be a challenge for years to come.
And the problem is not limited to
cybersecurity-specific roles, or even the
broader category of IT roles. An
organization’s greatest vulnerability
remains its own workforce, so even if all
needed cybersecurity roles were filled, the
enterprise would still be open to
exploitation. In other words, effectively
managing the entire workforce with
cybersecurity in mind is essential.
You don’t need to become a technical expert, but
you can contribute in a meaningful way. This paper provides an introductory set of
guidelines and considerations to help you get started.

It’s important to begin with an understanding of what an enterprise looks like when its
workforce is optimized for cybersecurity. In a cyber-secure enterprise: The people responsible for tools and technology are given proper (but limited)
authorities to access and protect data and systems; The organization has the right governance structure to balance its business
objectives with security needs as part of overall enterprise risk management; The organization has clear and broad consensus on the cybersecurity risks that
are most important to the business and industry in which it operates; Security is a broadly-understood priority, with leaders building a cybersecurity
culture where the right behaviors are encouraged; The right people—with the right competencies and capabilities—are available to
support the enterprise; These people are deployed to the right places within the organizational structure; The entire workforce is doing the right things to be resistant to exploitation of
human vulnerabilities, which typically represent the biggest risk to any
organization;

Actively participate in the cybersecurity
planning process from the outset by
attending meetings, observing, and
offering input on workforce matters—
this will help you understand what
you’re planning for and how cyber risk
is being addressed.

Ensure appropriate leaders from IT,
operations, legal and finance are
involved in the workforce planning that you’re guiding from the outset—this will
help you capture the input of the functional areas which impact security.

Seek the support of senior leadership to drive
active participation in this effort—this mitigates
the risk of “stovepipe” views of departmental
needs which undermines integrated planning.

Establish a cross-functional committee or work
group with representatives from key departments
and functional areas to integrate their
perspectives and address their needs.

Are you starting
at the right place?

All technical roles involved in cybersecurity, regardless of organizational
placement—this includes the IT department, as well as technical roles
embedded within business units (such as
developers or engineers)

Partners and suppliers—this includes roles which oversee the sharing of
data and systems (such as procurement or project management
applications) with partners and suppliers

Build partnerships with cybersecurity
professionals (on staff or from external
providers) to understand specific staffing needs—
including roles, responsibilities and requisite
knowledge, skills, and abilities—related to
cybersecurity.
Learn the basic needs for certifications and other
advanced training by working with your
cybersecurity leaders.

Is your plan based
on independently-validated best
practice?


Work with your colleagues to assess the competencies and capabilities of the
existing workforce against the needs identified above. Perform a gap analysis to identify the roles and responsibilities which are not
appropriately filled already. Use existing frameworks and toolkits, such as the National Cybersecurity
Workforce Framework developed by the National Initiative for Cybersecurity
Education (NICE) at NIST, DHS'
Cybersecurity Workforce Development
Toolkit, and the Council on CyberSecurity
Cybersecurity Workforce Handbook.
Recognize that demand forecasting must
be based on an overall enterprise-wide
cybersecurity strategy, not merely the sum of
staffing requests from various departments,
because cybersecurity is not merely an intradepartmental function, but an interdepartmental, enterprise-wide function!

Draft a workforce planning mapxi specific to your enterprise, placing
cybersecurity roles where they need to be within the organizational structure and
delineating appropriate (and limited) authorities for each role

Identify positions most critical to enterprise security and which require the most
sophisticated technical knowledge and experiencexii—this list can be used to
prioritize budget and talent acquisition efforts
Do you clearly
understand who is accountable
for what?

For most small and medium-sized businesses, cybersecurity functions will
need to be performed by third parties, since in many cases much of the IT
infrastructure is outsourced.

Explore creative ways to source talent internally—many of your current
employees may have abilities and affinities fit for cybersecurity, and can become
productive contributors if provided the right
training and development.

Engage the entire workforce to secure the
enterprise by working with management to drive awareness and increase cyber
hygiene as part of a strengthened security culture.

While you don’t need to know everything about cybersecurity, you do need to be a
“smart user” of IT and cybersecurity. To be an effective member of the management
team, all leaders need to have a foundational understanding of the major functional
areas of the business, such as finance, operations, business development, and (of
course) HR. The same is now true for cybersecurity, and it applies to you as an HR
professional. It also means doing your part to secure data and systems—especially those
pertaining to sensitive personal information and employee records. This can be accomplished through a variety of controls, from restrictive account access (through
multi-factor authentication, role-based permissions and least privileged user) to data
encryption and regular log reviews.
To implement these controls, and
become more knowledgeable in general,
you can reference foundational
frameworks such as the Cyber Security
Framework (CSF) developed by NIST,
and the CIS Critical Security Controls,
which are a set of independently
validated best practices for securing
enterprises. There are also resources
specific to workforce management,
including the National Initiative for
Cybersecurity Careers and Studies
(NICCS) portal and the aforementioned
Cybersecurity Workforce Handbook.
This is an opportunity for you to contribute in a real way to the organization and its
leadership. If it hasn’t happened already, it’s a matter of time before cybersecurity
becomes a hot topic at your office. Cyber attacks plague enterprises of all sectors,
industries and sizes—a quick glance at the news is evidence enough. Take a proactive
approach to help your enterprise operate securely and effectively in the digital world—
now and in the future.

Cyber attacks on industrial control systems (ICS) differ in impact based on a number
of factors, including the adversary’s intent, their sophistication and capabilities, and their
familiarization with ICS and automated processes. Cyber attackers target systems not in
single incidents and breaches but, instead, through a campaign of efforts that enables access
and provides sufficient information to devise an effect. A campaign represents the entirety
of the operation against the defender organization and its systems. Understanding where an
adversary is in his or her campaign can enable defenders to make better-informed security
and risk management decisions. Additionally, this knowledge of the adversary’s operations
can help defenders appreciate the attacker’s possible intent, level of sophistication, capabilities
and familiarization with the ICS, which together work to unveil the potential impact of the
attack on an organization. The authors believe ICS networks are more defensible than
enterprise information technology (IT) systems. By understanding the inherent advantages of
well-architected ICS networks and by understanding adversary attack campaigns against ICS,
security personnel can see how defense is doable. The authors introduce the concept of the
ICS Cyber Kill Chain to help defenders understand the adversary’s cyber attack campaign.
In 2011, Lockheed Martin analysts Eric M. Hutchins, Michael J. Cloppert and Rohan M. Amin
created the Cyber Kill Chain to help the decision-making process for better detecting and
responding to adversary intrusions.1 This model was adapted from the concept of military
kill chains and has been a highly successful and widely popular model for defenders in IT and
enterprise networks. This model is not directly applicable to the nature of ICS-custom cyber
attacks, but it serves as a great foundation and concept on which to build.
ICS-custom cyber attacks capable of significant process or equipment impact require
adversaries to become intimately aware of the process being automated and the engineering
decisions and design of the ICS and safety system. Gaining such knowledge enables an attacker
to learn the systems well enough to cause predictable effects on systems in a way that
circumvents or impacts safety mechanisms and achieves a true cyber-physical attack rather
than an attack characterized as espionage, ICS disruption or intellectual property theft. To
accomplish such an attack requires adversaries to initiate a two-stage attack against an ICS.
The multiple stages, or exaggerated kill chain, provide additional opportunities for defenders
to increase the adversary’s cost of an attack and to position themselves to detect and disrupt
attackers before they reach their goal.
The first stage of an ICS cyber attack is best categorized as the type of activity that would
traditionally be classified as espionage or an intelligence operation. It is very similar in nature to
attacks covered in Lockheed Martin’s Cyber Kill Chain and often has the purpose of gaining
access to information about the ICS, learning the system and providing mechanisms to defeat
internal perimeter protections or gain access to production environments.

Planning is the first phase and includes performing reconnaissance. Reconnaissance
is an activity to gain information about something through observation or other detection
methods. Cyber attack planning and reconnaissance often includes conducting research about
the target, usually with open source information-gathering tools such as Google and Shodan,
as well as through searches of publicly available data such as public announcements and social
media profiles.
The objective of the Planning phase is to reveal weaknesses and identify information that
support attackers in their efforts to target, deliver and exploit elements of a system. The types
of information that may be useful to an attacker can include human, network, host, account
and protocol information, as well as information about policies, processes and procedures.
Planning and reconnaissance for ICS can also include activities such as researching ICS
technical vulnerabilities and features or gaining an understanding of how the process and
operating model may be susceptible to exploitation. Passive reconnaissance techniques (often
referred to as footprinting) can take advantage of the tremendous amount of information
available on the Internet to develop information about the target without being observed.
Reconnaissance can often include actively mapping a target’s publicly or privately accessible
attack surfaces, patterning activity and determining versions of operating system software
through routine queries.
Attackers can also attempt to hide within the noise of expected Internet traffic and activity.
Publicly available information about organizations helps shape the target options available to
adversaries, and the one thing defenders do not get to choose is whether their organizations
are worth targeting.

Preparation is the second phase and can include weaponization or targeting.
Weaponization includes modifying an otherwise harmless file, such as a document, for the
purpose of enabling the adversary’s next step. Many times weaponization is manifested as
files, such as PDFs, that have an exploit contained within them. The weaponized document,
however, may just take advantage of available features in a malicious way, for example, as
macros in Word documents.
Targeting can also take place in the second phase and occurs when the adversary or its agent
(such as a script or tool) identify potential victim(s) for exploitation. Targeting, in modern
military parlance, is the process of analyzing and prioritizing targets and matching appropriate
lethal and nonlethal actions to those targets to create specific desired effects. Cyber attackers
decide what attack tool or method they will use against the target based on the trade-offs
between effort required over some period of time, likelihood of technical success and risk
of detection. For example, after reconnaissance an adversary may determine that a virtual
private network (VPN) into the environment is the right part of the defender’s network to
target because it may be the best approach to meet their objectives with the least amount of
resource expenditure needed.
Weaponization and targeting can both take place, but both are not required. In the VPN
example, the adversary may identify credentials to log in to the network directly and bypass
the need for weaponization. Likewise, adversaries can weaponize capabilities to be delivered
to a number of targets without specifically targeting any specific one and select a desired
target only after they gain initial access.

To gain initial access requires the third phase, known as the Cyber Intrusion. An
intrusion is any attempt by the adversary, successful or not, to gain access to the defender’s
network or system. This includes the Delivery step, in which the adversary uses a method to
interact with the defender’s network. For example, a phishing email would be the delivery
mechanism for the adversary’s weaponized PDF, or the VPN would deliver the adversary
directly to the network. The next step, the Exploit step, is the means the adversary uses to
perform malicious actions. The means may be an exploit for a vulnerability when a PDF or
other file opens, or it could be an exploitation of existing accesses to the network, such as
using the credentials for a VPN. When the exploitation is successful, the adversary will install
a capability such as a remote access Trojan. The adversary may also, or instead, modify existing
capabilities. For example, in newer Windows environments the PowerShell tool provides
enough functionality for an adversary that they do not need to rely on malware to perform
their intrusion. Defenders should focus is on finding and understanding the threat and should
not always assume that the threat is malware-based.

With a successful cyber intrusion the adversary moves to the next phase, Management and
Enablement. Here the actor will establish command and control (C2), using methods such
as a connection to the previously installed capability or abusing trusted communications
such as the VPN. Capable and persistent actors often establish multiple C2 paths to ensure
connectivity is not interrupted if one is detected or removed. It is important to note that
C2 methods do not always require a direct connection that supports a high frequency of
bidirectional communication. Some access to protected networks, for example, may rely
on one-way communication paths and require more time to move information out and
deliver commands or code in. Attackers often establish C2 by hiding in normal outbound
and inbound traffic, hijacking existing communications. In some cases, attackers establish C2
by implanting equipment to establish their own communication bridge. With managed and
enabled access to the environment, the adversary can now begin to achieve his or her goal.

The Sustainment, Entrenchment, Development, and Execution phase documents a number of
end goals that an adversary might have. In this phase, the adversary acts. The complete list
of every attacker’s actions would be cumbersome; however, common activities include the
discovery of new systems or data, lateral movement around the network, installation and
execution of additional capabilities, launching of those capabilities, capturing transmitted
communications such as user credentials, collection of desired data, exfiltration of that data out
of the environment and anti-forensic techniques such as cleaning traces of the attack activity
or defending his or her foothold when encountering defenders such as incident responders.
This can be a critical phase for the planning and execution of the ICS Cyber
Kill Chain. A significant amount of information about the ICS and the industrial process,
engineering and operations exists in Internet-facing networks such as corporate or enterprise
networks. It is vital that defenders assess what information and tools exist in less-protected
networks that could aid attackers in an attempt to compromise the ICS. It is also important
to note that an attacker may perform Stage 1 against a supplier or partner network to gain
necessary information, such as ICS project files delivery paths or an integrator’s or vendor’s
remote access link to the ICS. Stage 1 may be completed when the attacker has successfully
compromised the security of an ICS and is able to move on to Stage 2.
Stage 1 most directly maps to what would constitute a breach in traditional IT networks.
It is important to highlight that this stage can be bypassed if defenders have Internetfacing ICS components or information about the ICS and process from a successfully
compromised third-party. Recent Black Energy campaigns attempt to exploit susceptible
Internet-facing devices.
A significant portion of malware and network intrusions in the community occur during
Stage 1 because this is where nation-state-level intelligence and espionage operations are
most likely to take place. In addition, it is where criminals are most likely to get information
that can be monetized.
In many cases, there is significantly more value, depending on the attacker’s current goals, in
performing espionage than in perpetrating an actual attack that would include the destruction
or manipulation of systems. Enjoying sustained access provides the opportunity for attackers
to initiate follow-on actions later if they align with national security or military goals and/or
criminal objectives. Therefore, it is important to identify and remediate adversary intelligence
efforts—even if there is no immediate danger or business impact.


What makes performing an ICS cyber attack so different from a traditional IT cyber attack is
that ICS components are shaped by the underlying engineering and process and are designed
in unique ways and configurations that require the attacker to have extensive knowledge to
impact them in a meaningful and designed way. Additionally, in a properly architected ICS,
there are many layers of systems and detection sensors that an adversary has to traverse in
Stage 1 to gain access to the ICS components. Unfortunately, directly connecting an ICS to the
Internet significantly undermines the inherent advantages that a properly architected ICS has
with regard to security.
To continue to take advantage of these inherently defensible architectures, defenders must
be careful in the design choices they make and how they integrate systems. For example,
integrating safety systems into the same network as operations significantly reduces the effort
an adversary has to expend to fully compromise the system.3 It also gives the defenders less
opportunity to identify and remediate the attack. This loss of opportunity to defend coupled
with a simultaneous increase in value to the attack accounts for a significant decrease in ICS
security. With a properly architected ICS, even environments that do not traditionally have
security designed into them, which can be a significant problem, are not easy to impact in a
meaningful and predictable way. This problem is visualized in Stage 2 of an ICS attack.

It is in Stage 2 that the attacker must use the knowledge gained in Stage 1 to specifically
develop and test a capability that can meaningfully attack the ICS. Unfortunately, due
to sensitive equipment it is possible that Stage 1 adversary operations could lead to an
unintended attack. This is a significant risk for a nation-state cyber operation because such an
attack may be perceived as intentional and have unforeseen consequences. For example, an
attempt to actively discover hosts on an ICS network may disrupt necessary communications
or cause communication cards to fail. Simple interactions with ICS applications and
infrastructure elements may result in unintentional outcomes. This activity would still be
contained within Stage 1 and be an unintended effect in the Act step. Intentional attacks take
place in Stage 2 and are described in Figure 2.

Stage 2 begins with the Attack Development and Tuning phase, in which the aggressor develops
a new capability tailored to affect a specific ICS implementation and for the desired impact.
This development will most likely take place through exfiltrated data. Only brazen attackers
that have a very low opinion of the ability of the system owner and operator ability to
observe their actions will experiment and develop their attack through live in-production
testing. Therefore, under normal conditions, the adversary’s development and tuning is
especially difficult to detect. There may also be significant lag between Stage 1 and Stage 2
operations due to the need for prolonged development and testing time.

Once an adversary has developed a capability, the next phase is the Validation phase. Here,
the attacker must Test his or her capability on similar or identically configured systems if the
capability is to have any meaningful and reliable impact. Even simple attacks, such as increased
network scanning for the denial of service to systems, need a level of testing to confirm that
the scanning can deny service to the systems. However, for more significant impacts, significant
testing may occur in which the adversary may acquire physical ICS equipment and software
components. While it is difficult for most defenders to have insight into the ICS vendor
community, various government organizations can utilize their sources and methods to identify
unusual acquisitions of such equipment that may indicate a Stage 2 attack for an already
established Stage 1 operation.

Ultimately, the last phase is the ICS Attack, in which the adversary will deliver the capability,
install it or modify existing system functionality, and then execute the attack. The attack may
have many facets (preparatory or concurrent attacks) that fall into the attack categories of
enabling, initiating or supporting to achieve their ultimate effect. These may be necessary to
trigger conditions needed to manipulate a specific element of the process, initiate changes in
process set points and variables or support the attack over time by such tactics as spoofing
state information to fool plant operators into thinking everything is normal.

The complexity of launching an attack is determined by the security of the system, the process
being monitored and controlled, the safety design and controls, and the intended impact. For
example, a simple denial of service that disrupts the ICS is significantly easier to achieve than
manipulating the process in a designed way or being able to attack the system and have the
option of re-attacking as illustrated in Figure 3. The attacker ultimately needs to manipulate the
process to do significant harm, including reliable or predictable physical destruction, damage
of equipment under control or process elements, or modification, including manipulation of
formulas, recipes and mixtures.

Although there are various ways to attack an ICS environment, the most common methods to
achieve functional impact fall into three categories: loss, denial and manipulation. They include a
loss of view, denial of view, manipulation of view, denial of control, loss of control, manipulation
of control, activation of safety, denial of safety, manipulation of safety and manipulation of
sensors and instruments.

There is an inherent contrast in impacts between IT and operations technology (OT) that
operate an ICS. As an example, denial of service to an IT system may be extremely significant
to a business process, whereas in ICS the manipulation of sensors or the process is more
disturbing because it could lead to the failure of safety systems designed to protect human life
or induce the process to injure personnel.

The ICS community, as a whole, does not fully understand the extent of the possibilities
available to an attacker. The scenarios of power grid failure and dam overflows are commonly
discussed, but other impacts, such as the release of deadly chemicals, degrading manufacturing
goods slowly over time or financial loss due to unusable product resulting from modified
mixtures, are other concerning scenarios.4 It is, therefore, essential that IT and OT security
personnel, as well as national policy makers, fully engage the engineering community to
uncover the scenarios that could be harmful at various facilities to help them understand the
potential achievable goals of an adversary. The industry must approach the problem of ICS
attacks as they do equipment prognostics. It is not a matter of if it will fail, but when it will fail,
and the community must complete the necessary assessment, engineering and instrumentation
tasks to plan for and deal with the potential for attacks on the best terms.
Another effective way to understand ICS attacks, as well as visualize the ICS Cyber Kill Chain,
is to review case studies of ICS targeted intrusions and attacks.

Analyzing previous intrusions into ICS networks provides validation and insight into the
ICS Cyber Kill Chain as a workable model for defenders. The ICS community historically
lacks visibility into their networks and suffers from having sparse forensic evidence and data
following compromises. For this reason, it is not feasible to properly identify and extract every
piece of evidence from these case studies. However,
understanding them at a high level is sufficient.
It is important to understand the layout and structure
of a typical ICS network. We use the Purdue Reference
Model to illustrate the architecture of
an ICS network.
In the following case studies, the Purdue Model will
illustrate the architectural level at which the ICS was
impacted, and the ICS Cyber Kill Chain will demonstrate
the phases the adversary completed in their campaign.

The Havex malware, used in a campaign against ICS
to gather sensitive data and network architecture information from thousands of sites around the world,
was a remote access Trojan that was originally used for general-purpose espionage and
evolved into a criminal tool set. It was also adapted to target ICS by including new code
and modules specific to ICS environments. From publicly available information, it has been
determined that the campaign took place over the course of at least three years.

These multiple methods of compromise highlight that adversaries remain flexible and are
not bound by a single technique for delivery and intrusion when conducting a campaign.
The observed techniques indicate the attackers were successful in their planning phase of
identifying weaknesses to exploit, such as the general trusting nature of engineers and inherent
trust and reliance on the ICS supply chain. Additionally, it offers three intrusions to map against
the ICS Cyber Kill Chain.
In the first intrusion, the spearphishing email, the adversary would have first performed
reconnaissance to determine good targets and tailor the phishing emails. Next, the actors
performed weaponization by combining a file with an exploit and attaching it to the
spearphishing email. Specific targeting took place to choose which people would receive
the email. The email itself was the delivery mechanism, and when the user opened the file
attached to the email, it exploited the system to install the Havex malware. Then, the Havex
malware attempted to communicate with one of hundreds of C2 servers. Havex then scanned
the environment to discover ICS components, collect the information and exfiltrate it to the
C2 server for the adversary to gather. The phishing email-based intrusion mostly impacted the
external network. This method was less likely to provide specific information about the ICS,
except in cases where organizations kept engineering files on the business network.
The second intrusion, the infected websites, followed the first intrusion closely but used other
methods to carry out Stage 1. Note, the intrusion against the ICS vendor websites had its
own kill chain, and the adversary’s efforts were to enable an intrusion against ICS networks.
The kill chain against the ICS networks would have needed reconnaissance to identify what
ICS networks were desired and what ICS vendors they used. From there, the vendor websites
were the subject of the weaponization, with the intent of targeting the ICS networks that used
those vendors. The delivery mechanism in this scenario was the Internet connection using the
HTTP protocol to access the web page.

The websites were weaponized using exploits from a common penetration testing framework
known as Metasploit. The re-used exploits against known vulnerabilities acted as the exploit to
allow the adversary to then install Havex into the environment, where it established its C2 and
completed the same actions observed in the first intrusion. This intrusion had a higher chance
of gaining access into the ICS because of the engineers and operators that were visiting the
vendor websites. This intrusion mostly impacted the DMZ of ICS networks, but it was able to
gain access deeper into the ICS for those organizations that did not utilize the Purdue Model
or a defense-in-depth–styled architecture.
The third intrusion was the most creative. It placed a trojanized version of ICS software
installers on vendor websites.9 Reconnaissance would have to take place in much the same
way as it did in the second intrusion. In this case, though, it was the installer that was the
subject of the weaponization, with the intent of targeting ICS networks employing those types
of ICS software. The delivery mechanism, the exploit, install, C2 and related actions occurred
just as they did in the other intrusions. The difference in this scenario, though, was that even
well-architected networks that only allowed Internet access from the business network or
DMZ were subject to Havex being present in lower zones of the Purdue Model. This delivery
technique may have evolved from initial attempts to defeat planned security controls, such
as perimeter protections, by relying on engineers to physically transport files from Internetfacing computers into the production ICS network. The Exploit, Install, C2 and Act steps in this
case took place internal to the ICS networks. The majority of reported infections took place
in the supervisory level, where engineers and operators would have been accessing systems
such as engineering workstations and human machine interfaces (HMIs). The adversary
undoubtedly gained great data from this third intrusion. Because of that, it was the most
observed intrusion method.

To date, the security research community has not observed evidence of follow-on actions by
the Havex actors. The authors believe Havex can be characterized as a generally successful
Stage 1 ICS attack. To date, there has been no documented evidence of Stage 2 activity. A
representation of the ICS Cyber Kill Chain for Havex mapped to the Purdue Model for the
Observable
  
The Stuxnet malware, which has been reported to have physically destroyed centrifuges
at the Natanz facility in Iran, serves as a great case study of an attack that took place over
Stage 1 and Stage 2 of the ICS Cyber Kill Chain. Stuxnet was mostly observed in 2010;
however, it was a campaign that may have taken place over a number of years, with earliest
estimates around 2006 and 2007. Over that period of time, the actor went to a significant
amount of effort to create a highly targeted attack that was able to physically destroy
specific centrifuges. This intelligence-gathering period is best understood through Stage 1 of
the ICS Cyber Kill Chain.
The actors behind the Stuxnet campaign may have performed reconnaissance to identify
potential paths to the Natanz facility. However, experts have speculated that there may also
have been a physical component to the reconnaissance to gain such intimate data about the
facility. It is always important to consider the impact of the physical security component, as
well as geopolitical tensions. For example, the Iranian uranium enrichment program at Natanz
was of significant concern for various countries in the world, and the purpose and location of
the Natanz facility was publicly leaked by a dissident group in 2006. That, along with other
non-cyber data, likely significantly contributed to the actor’s planning and reconnaissance efforts.
Additionally, the Natanz facility reportedly had an air-gapped network that did not allow for
using traditional methods to compromise the ICS over network connections. Instead, it could
have been an insider threat that, knowingly or unknowingly, compromised the network through
the use of an infected engineering laptop or removable media device, such as a USB.
The weaponization would have been the malware’s code combined with the exploits that
were placed onto the laptop or removable media. The removable media or laptop then
acted as the delivery mechanism to exploit the Natanz network. Then, Stuxnet installed
itself on various versions of Windows systems and repeated the exploit and install phases,
compromising a number of systems until it could activate Internet access and reach out to the
attacker’s C2 servers.

This version of Stuxnet, at the time, may not have had any ICS-specific payload. The gathering
of information about the environment and exfiltration of that to the C2 servers or by other
means were the material actions that took place in the Stage 1 Act phase. Investigators believe
a large amount of data was collected over many years, so that the adversary knew the ICS and
its network as well as, if not better, than the engineers and operators on site.
During the second stage of the ICS Cyber Kill Chain, the attacker developed and tested an
update to the Stuxnet malware that would impact the ICS. Once the capability was ready,
the attacker delivered it to Natanz. There are a number of methods that could have been the
delivery mechanism, but the engineering workstation or USB method is the most common
theory. Additionally, the attack had provisions to continually deliver and install itself throughout
the environment until it found its appropriate targets: One updated sample of Stuxnet would
move throughout the network, find older versions of the malware that did not have the newly
developed ICS attack modules, and have those older versions update to the newer version of
Stuxnet. Once Stuxnet was on the correct targets, a WinCC SIMATIC server connected to
specific Siemens controllers with other specific conditions, it then performed the Execute ICS
Attack phase. The impact of this attack was the modification and manipulation of the process
and systems to force the centrifuges being controlled into physically destroying themselves.

In the Stuxnet campaign there is a completed two-stage ICS attack that led to the highly
tailored and impactful manipulation of the process to cause physical destruction. The campaign
ultimately reached all layers of the Purdue Model and represents a worst-case scenario of a
completed ICS Cyber Kill Chain against a victim.
The ICS Cyber Kill Chain is a model that builds upon the traditional understanding of a
cyber kill chain and tailors it to adversary attacks on ICS. The model provides defenders
an opportunity to better understand the phases of an adversary’s campaign into an ICS to
identify opportunities for detection, remediation and defense. These opportunities for success
also highlight that ICS networks are more defensible than traditional IT networks and stress
the importance of maintaining this defensible architecture through actions such as limiting the
integration of safety systems with operations networks and removing ICS components from
direct Internet access.
There are a growing number of models for ICS defenders to apply against the concepts
revealed in the ICS Cyber Kill Chain. For example, the Sliding Scale of Cyber Security added
a nuanced discussion to resource investments and defender actions that can be implemented
to protect the safety, security and reliability of operations. Models mapped to this sliding
scale, such as the Active Cyber Defense Cycle and Defense in Depth concepts, are all vital
for defense. With these emerging models and with the appreciation of the adversary’s ICS
Cyber Kill Chain, ICS security personnel can learn a great deal and leverage their knowledge
to advance the security of the ICS community.

As cyberspace has grown and become more pervasive, military art has
changed. No one today can exert or maintain national power without
acute sensitivity to the digital networks that underpin the world’s
communications, prosperity, and security. Although the US Department
of Defense and Intelligence Community had an early advantage in
cyber capabilities, today much of the technical expertise necessary here
resides outside government and often outside our nation. The United
States is working hard to maintain its edge over potential adversaries
in cyberspace—but we must acknowledge our nation is facing peer
competitors in this domain.
Our mission in cyberspace is to provide mission assurance for the
operation and defense of the Department of Defense information
environment, deter or defeat strategic threats to US interests and
infrastructure, and support the achievement of Joint Force Commander
objectives. Our challenge is to protect the things we value—freedom,
liberty, prosperity, intellectual property, and personal information—
without hindering the free flow of information that fosters growth and
intellectual dynamism.
All nations have vulnerabilities that can be exploited in and through
cyberspace, but we can lessen ours dramatically by harnessing the power
of our nation’s cyber enterprise. We as a Department are still in the
early stages of this journey. The necessary cyber workforce, defensible
architecture, situational awareness, operational concepts, authorities, and
capabilities are not fully in place. Knowledge of our mission set across
the Department and the government is not yet where it needs to be. Our
traditional command and control and organizational constructs do not
enable the speed and agility required to keep pace with change in the
cyber domain. We must adapt, and soon!
Our challenge at US Cyber Command is to apply our experience and
expertise in an adaptive manner, making initiative, innovation, and
excellence our standards as we execute our responsibilities to secure
our nation’s freedom of action in cyberspace and help mitigate risks to
national security. Our task is to make this domain understood by other
warfighters and integrated into broader military and governmental operations while providing decisionmakers and operational commanders
with a wider range of options while resources are constrained and threats
are growing.
This guidance provides US Cyber Command with strategic direction to
ensure unity of effort as we perform our duties in the service of the nation.

The nation’s cybersecurity
requires a collaborative approach with a range of interagency and industry
partners contributing authorities, capabilities, and insights to protect US
infrastructure and information, detect attacks, and deter adversaries in cyberspace. By working together we improve our collective knowledge about
what is happening across the cyber domain and protect our networks.
US Cyber Command leverages the nation’s cryptologic heritage to defend the
nation’s vital interests in cyberspace, prevent strategic surprise, and maintain
technological advantage. We team with the National Security Agency (NSA)
to leverage its expertise in intelligence, analysis, and information assurance,
thereby saving America the resources, security, and opportunities involved in
duplicating capabilities for tactical, operational, and strategic decision makers.
US Cyber Command works with focus and energy
to build capacity and capability, and to integrate
Oriented toward
cyberspace operations into joint force objectives.

The US Government has made significant strides
in defining cyber doctrine, organizing cyber capabilities, and building cyber
capacity. We at US Cyber Command will increase our momentum in a
domain where adversary capabilities continue to evolve as fast as ours. US
Cyber Command remains at the core of the Department of Defense’s cyber
enterprise and we will help the nation extend this enterprise beyond the
boundaries of DoD’s expertise and authorities. This is crucial to our nation’s
ability to deter or defeat enemies in cyberspace so that they do not imperil
our safety, prosperity and way of life.

Our imperatives are mutually supporting, with success in one supporting
success in others. We must identify obstacles to achieving our goals, develop
plans to overcome those obstacles, and establish meaningful metrics to gauge
our progress.

Our greatest imperative is to execute our assigned missions in defense of the
nation. States, groups, and individuals are using and developing sophisticated
capabilities to conduct cyber coercion, cyber attacks, and cyber exploitation
against the United States and our allies. The targets of their efforts extend
well beyond government and into privately owned businesses.
US Cyber Command, teaming with federal, foreign, and industry partners,
will help to mitigate, halt, and attribute acts of disruption and destruction
and campaigns of cyber espionage; dissuade adversaries from malicious
behavior; and strengthen the resilience of DoD systems to withstand attacks.
In appropriate circumstances, and on order from the National Command
Authority, we must be able to conduct offensive cyber operations. US Cyber
Command will provide decisionmakers and operational commanders with
capabilities and options that can be integrated with other elements of US
national power to shape our operating environment in peace, crisis, and war.

To execute the missions assigned to us, we must turn strategy and plans
into operational outcomes. This requires commitment to an operational
mindset whereby our networks and cyber capabilities are not administered
but rather led by commanders who understand they are always in real or
imminent contact with adversaries. The many components of our information
environment must be designed and led so they can operate and interact
dynamically, constantly, and simultaneously, and continue to function and
fight in the face of damage and casualties.

Operations in and from cyberspace are crucial to the future of the joint force.
Joint task force commanders, regardless of mission, need to understand their
networks and how those networks affect the execution of their missions. Joint
Force 2020 envisions operations in cyberspace becoming a precursor to and
an integral part of conflict in the land, maritime, air, and space domains.
We must integrate cyber into a broader range of military operations and
offer options for commanders and policymakers to use cyber tools from
Phase 0 (peacetime daily ongoing) operations, through Phase 3 (conflict and
crisis), to Phase 5 (recovery) operations. We must train to these scenarios,
exercise them, and integrate cyber operations into other warfare areas in
a manner that joint and coalition warfighters understand. Cyber concepts
must take their place among the fundamentals of military doctrine that
guide continuous employment of mutually supporting capabilities to achieve
advantage against adversaries. We will employ traditional terminology,
operational concepts, and tactics, techniques and procedures (TTPs) where
possible, emphasizing cyber’s similarity to other mission sets. We will
improve integration and synchronization of the planning, execution, and
assessment of cyberspace operations with joint war-fighting processes. Even as we support other Commands, we will shift our mindset from enablers
to operators, from supporting to supported, and from administrators to
warfighters as we integrate cyber into new ways of defending, fighting,
and partnering.

We must generate the capability that gives commanders and policymakers
options to execute full-spectrum operations, even as senior policymakers
continue to refine the requisite authorities. We must build the capacity of the
Cyber Mission Force and enable the partnerships so vital to its success. Our
capabilities should be brought to a level where they are as trained and ready
as any carrier strike group, squadron, marine air-ground task force, brigade
combat team, or regimental combat team. Our cyber teams will be tangible
and operationally ready to execute their assigned missions. To do this they
require platforms, tools, training, and infrastructure, just like maneuver
elements in all other domains.
We will continue to strengthen the nexus between NSA and US Cyber
Command, and between DoD’s IT infrastructure, cyberspace intelligence,
and cyberspace operations. We will enhance our capability by ensuring a
depth of knowledge and unique capabilities across our workforce, making
them ready, when granted the necessary authorities, to execute the widest
possible range of missions. We will act to preserve and extend America’s
cyber advantage so that the joint force can operate globally with speed,
flexibility, and persistence.

The freedom and ability to operate in cyberspace is a goal in itself and also
is a critical enabler for operations on land, in the air, at sea, and in space. We
must assure the availability of this enabling capability to facilitate military
advantage in all domains, and we must be able to integrate and synchronize
cyberspace operations with other tools of national power to defend DoD
networks, support other Combatant Commanders, and, when called upon,
defend the homeland. The following are key enablers for accomplishing
our imperatives.

DoD will continue to build trust in the ability of the US Armed Forces
to conduct operations in accordance with law and in a manner that meets
broader foreign and defense policy objectives. Trust needs to be earned. We
must demonstrate our value through specific contributions and continuing
dialogue, giving our interagency partners confidence that we will act with
restraint in employing our forces. We must build transparent, repeatable, and
accountable processes for planning, reviewing, and obtaining approvals for
proposed cyberspace operations. We must clarify roles and responsibilities
between US Cyber Command and other US government and private sector
entities during all phases of a conflict. We must establish and demonstrate
effective command and control over cyberspace operations.

DoD leaders and commanders naturally seek control over the means they
require to accomplish their missions. We must define supported/supporting
relationships with Combatant Commanders and ensure we effectively
function as both a supported and supporting Command. We must help the
Defense Information Systems Agency (DISA) transition from an acquisition
and engineering organization to an operational partner that can maneuver
at the tactical level to operate and defend DoD networks. Our command
and control structure must enable the Services to man, train, equip, and
configure forces to be presented to Combatant Commanders; enable
US Cyber Command to globally synchronize all cyberspace operations;
and enable Regional Commanders to employ the force (including
DoDIN capabilities), as we maintain the availability and security of our
technological advantages.
Collaboration with partners inside and outside government will determine
our success in defending the nation in cyberspace. Mission accomplishment
depends on unity of effort. We can learn from allies and industry as well as
from non-traditional partners in academia and the information technology
(IT) security community. To do so, we need to understand our partners and
how they operate, and help our partners understand us. Our relationship
with NSA is key. We must optimize this relationship for the missions of both
organizations with the objective of creating two independent but symbiotic organizations with a well-defined and close partnership. Finally, we must build
information-sharing mechanisms to ensure regular contact with those whom
we fight and operate alongside, both inside and outside DoD.

The nation needs a motivated, fully-trained, and well-led cyber workforce
that understands evolving technologies and adversary TTPs. The workforce—
military (both active and reserve), civilian, and contractor—is the Command’s
greatest resource. Innovation in recruitment, excellence in training and
education, and career-path flexibility are critical enablers. Competition for
talent continues to be fierce as opportunities in the private sector proliferate,
but we can compete by offering people an opportunity to serve, a global
mission that matters, and responsibility at a young age. Retention may be
more difficult among civilians, but we should take this opportunity to create
a workforce whose outward focus and extended definition of teamwork makes
us even more effective in executing our missions.

US Cyber Command requires unique products and services to support
specialized and time-sensitive mission needs. Acquisition agility is critical in
the cyber domain because technology and threats evolve rapidly and a 2-5
year budget cycle may prevent us from properly equipping our cyber warriors.
US Cyber Command must advocate for the agility necessary to respond with
products and services to meet emerging mission needs. We must accelerate
the development and acquisition of new tools, and leverage Departmentlevel volume buying to achieve greater bargaining power with lower costs and
greater interoperability, particularly with regard to IT acquisition. We must
partner with industry and make effective use of the unique talent inherent
in our reserve force to redefine relationships and create unity of effort across
public and private sectors.

Cyberspace is a dynamic domain which changes every time someone
connects a networked device. The only certain feature of this environment
is uncertainty, which makes agility a necessity. Innovation, leadership,
and education will continue to be crucial to this agility. Cyberspace is a
human construct, so the broad principles of strategy and conflict still apply.
Warfighting skills remain critical—they just have to be faster and partnered.

WE MUST BE READY.

At a time when most of DoD is facing budget cuts, the National Security
Strategy and its implementation by the Department of Defense call for
increased investments in cyberspace capacity because of a belief that the
cyber mission set merits new investment. The Department of Defense and
the nation are counting on us to be there. We must be ready.

The cloud has spawned many types
of services – Software as a Service
(SaaS), Platform as a Service (PaaS),
Infrastructure as a Service (IaaS), and others.
Now add a new one. According to the 2011
Data Breach Investigations Report (DBIR),
recently published by Verizon, it’s Malware
as a Service (MaaS).
Verizon’s 2011 DBIR sends a clear message
– small companies, watch out. The Internet
has become a source of free or low-cost
malware that is easily customizable to meet
every hacker’s needs. Malware as a Service
significantly reduces the skill set needed by a
cybercriminal to launch automated attacks.
The result is a shift in the demographics
of information-asset attacks from large
corporations to smaller companies.
The DBIR suggests two reasons for this change in
attack demographics – the success of law enforcement and
the emergence of Malware as a Service. Law-enforcement
successes are motivating cybercriminals to look for softer,
less risky targets. Malware as a Service is making the effort
of attacking smaller companies more attractive.
In this article, we look at the DBIR’s path to these
conclusions.
The number of 2010 breach incidents analyzed in this year’s DBIR
has been significantly expanded by breach data provided
by the United States Secret Service. The USSS was formed
in 1865 to combat counterfeiting of the newly issued
U.S. paper currency. It is now charged with safeguarding
payment and financial systems in the United States.
The DBIR analyzes breaches in terms of the number of
breach incidents and the number of compromised records
(a “record” is a card number, a file, or some other data
unit). In 2010, the combined forensic databases of Verizon
and the USSS held the details of 761 data-breach incidents.

After four years of increases, compromised records
hit an all-time high of 361 million in 2008. In 2009,
this number had dropped to 144 million compromised records. Clearly, something was changing. Was corporate
security getting that much better?
Then in 2010, less than 4 million records were
compromised. 361 million to 144 million to 4 million!
Something significant was happening. Was it simply that
fewer incidents were being investigated? No – there were
70% more incidents investigated in 2010 than in the
previous years.
The clue to this drastic reduction was that the number
of compromised records per incident from 2004 through
2009 averaged 2,000,000, but this statistic dropped
drastically in 2010 to less than 7,000 records per incident.
Even more telling is that in years prior to 2010, 13% of all
breaches exceeded 1,000,000 records (mega-breaches). In
2010, 93% of all breaches compromised less than 10,000
records; and there were no mega-breaches.
Is this a trend or an anomaly? Unfortunately, as it turns
out, this is an anomaly. Mega-breaches have returned in
force in 2011, with Sony, Epsilon, RSA, Citigroup, and
Steam being prime examples. Sony itself contributed more
than 100 million records to the hackers. But this anomaly
had a fortunate outcome. It exposed a much more subtle
but equally serious threat – the commoditizing of malware
to make it available to even inexperienced hackers. As
the Verizon report indicates, companies not only must
continue to protect themselves against mega-breaches
but now must be able to detect and maintain a growing
number of mini-breaches.
Let us first review the Verizon DBIR methodology in
order to trace the DBIR findings to the new security threat
– Malware as a Service.

An Internal Active Agent is a manager, employee, or
contractor who maliciously participates in a breach.

92% of all incidents investigated in 2010 and 99% of
all compromised data were attributed to External Agents.
Over half of them were organized crime groups.
However, Internal Agents accounted for the majority
of stolen company confidential information. In fact, most
attacks by Internal Agents were active and malicious.

Malware includes the installation of backdoors through
which an attacker can gain access to a system; key loggers,
frame grabbers, and RAM scrappers; and interference
with security controls. Malware may be injected via emails
or compromised web sites.
Hacking is any effort to access or harm information
assets by thwarting security mechanisms. Hackers may
gain entry to a system via a backdoor or via the use of
stolen, guessable, or default login credentials.
Attaching skimmers to ATMs and pay-at-the-pump
card readers and using hard drives to copy confidential
data are examples of physical actions.
Skimming credit-card information and embezzlement are
misuses of trusted assets. For instance, wait staff are entrusted
with possession of credit cards while preparing bills.
Social actions may be collusion with bank tellers to
steal account information or with wait staff to skim creditcard information. Deception is a common social action
– for instance, sending a phishing email or pretexting
to be a repair man in order to replace POS devices with
compromised devices.
Errors include omissions, misconfigurations,
programming errors, etc. A frequent example of an error
is the failure to change default credentials.
Hacking and malware are by far the most common
actions, each having been used in half of all incidents
in 2010.

Multiple assets were often attacked in an incident.
Servers were compromised in over half of all incidents
by being infected with malware. POS servers, database
servers, and web servers were compromised most often.
User devices also were involved in over half of all
incidents. User devices include POS devices, ATMs,
pay-at-the-pump terminals, laptops, and desktops.
These devices are popular because they handle
financial transactions, and many are open to the public.
Interestingly, smart phones and tablets did not play a role
in any attacks – yet.
Offline data assets comprise confidential documents
and removable media from which information is stolen.
People may be deceived, bribed, or intimidated into
participating in a breach action.

The following example (on page 18) of a breach incident
is taken from the 2011 DBIR. It diagrams a chain of events
that led to a successful breach. It shows four primary threat
events (E1 through E4) and one conditional event (CE1) –
an event not perpetrated by an attacker. The diagram gives
a brief description of each event and the specific Four A
attributes associated with each event.
If any of these events had been thwarted, the breach
would not have occurred. One advantage of the VERIS
diagram is that it is educational in terms of what could
have been done to foil the breach. For instance, security
awareness and email filtering could have prevented E1
from occurring. An antivirus product on the executive’s
laptop might have stopped E2. Stopping progression
between E2 and E3 might have been accomplished if
egress filtering or netflow analysis had detected the
backdoor. Training and well-documented change-control
procedures could have prevented the administrator’s
misconfiguration, described in the conditional event,
and would have precluded the compromise of intellectual
property in E4.

A striking observation from the 2011 DBIR is the
absence in 2010 of mega-breaches of millions of records
and the rapid increase in mini-breaches. As noted earlier,
in the 2004 to 2009 timeframe, 13% of all breaches
compromised more than one million records, with the
average breach compromising two million records. In
2010, 93% of all breaches compromised less than 10,000
records and averaged less than 7,000 records per breach
incident. There were no mega-breaches.
Verizon suggests two reasons for the change in
breach strategy – the success of law enforcement and the
emergence of Malware as a Service.
Law Enforcement Successes – The Stick
The USSS has experienced significant success in fighting
cybercrime. Over the last several years, USSS agents have
arrested and prosecuted more than 1,200 suspects for
cybercrime involving a total of over $500 million in actual
fraud loss. In addition, because of these arrests, the USSS
estimates that it has prevented an additional $7 billion in
potential losses.
The perpetrators of many of the largest data breaches
are now behind bars. After any major investigation
that leads to the incarceration of big-time hackers,
cybercriminals evaluate what happened; and their tactics
evolve from the lessons learned. The main lesson learned
from the law enforcement successes is perhaps that bigger
isn’t better. Mini-breaches attract less attention and
therefore are less risky.
As a consequence, cybercriminals are now going after
easier targets that provide a smaller but steady stream
of compromised data and its resulting revenue. Though
financial-service companies remain a major target for
some cyber groups, these companies are no longer the
predominant focus of cybercriminals. Rather, companies
such as hotels, restaurants, and retail stores are becoming
the predominant targets.

Going hand-in-hand with this modus operandi, the
people behind breaches are no longer wholesalers stealing
millions of credit-card numbers and selling them on the
black market. Rather, they are using the fruits of their
breaching activities for their own purposes. Cybercriminals
are moving to compromised POS devices, fraudulent ATM
withdrawals, account takeovers, and ACH transaction fraud.
The Internet Comes to the Rescue of the
Cybercriminals – The Carrot
Launching an effective attack on a well-protected
company takes a lot of skill, time, and resources. Why go
after small companies when the return can be so small?
Underground developers all over the world are making
this attractive. A raft of prepackaged malware has recently
become available on the Internet. Some is for sale; some is,
in effect, open-source and free.
Many of these malware packages are customizable. They
provide a basic backdoor or a basic keylogger, for instance,
which can be modified to meet the hacker’s specific needs
through a GUI interface. Just a few clicks and the hacker
has his chosen malware, customized to launch the specific
attack he has in mind. Furthermore, if he is a serious and
frequent hacker, he can modify his malware periodically to
avoid detection by antivirus software.
Furthermore, there is a growing use of antiforensic
capabilities being included in the customizable malware.
Techniques such as data-wiping (deleting evidence of an
attack) and data-hiding (hiding or obfuscating data via
encryption or steganography) are making commodity malware
harder to detect and harder to track back to the perpetrator.
Verizon found that the majority of highly automated
attacks against small organizations in 2010 used customized
malware. The cost of launching such attacks is low, and the
skill requirements are becoming minimal. The technology
is growing ever more accessible to an increasing number of
criminals. Verizon’s data shows that the requirement for a
high skill level for successful attacks dropped from 90% of
all incidents in prior years to just 8% in 2010.
This is Malware as a Service. It is changing the face of
automated attacks against secured information assets.

Several observations from the 2010 data support the
fact that cybercriminals have moved from large, difficult
targets to smaller, softer, less risky ones. Restaurants,
hotels, and retail establishments accounted for 65% of all
incidents. Over half of the successful attacks were against
organizations with less than 100 employees.
Even more telling is the distribution of compromised
records. In past years, over 90% of such records were
stolen from financial firms. In 2010, this number
dropped to 35% - an almost two-thirds reduction. 56% of
compromised records came from retail stores, hotels, and
restaurants.
Several other statistics in the 2011 DBIR also point
to the supposition that it is the smaller firms that
are experiencing attacks. Large firms have typically
implemented sophisticated security measures for detecting
and containing data breaches, Not so, evidently, for small
firms, as is shown by how they discovered the breaches,
how long it took them to discover the breaches, and how
unsophisticated they were in their security defenses.

Interestingly, over half of all attacks took days or more
to reach the point of compromise. This time period is the
window of opportunity for a company to detect an intrusion
and to stop it. Clearly, even several days was not enough for
many small companies to notice an attack in progress.
Once a company’s data had been compromised, threequarters of the companies took weeks to months to discover
the breach. This is perhaps explained by the fact that many
of these companies were notified as a result of a CPP
investigation, which can take this long. Less than 5% of
companies discovered the compromise within hours of its
occurrence. These were probably the larger companies with
sophisticated security protections in place.
After an intrusion was discovered, it took two-thirds of
the companies weeks to months to contain the intrusion. This
has to be particularly frustrating to a company when it knows
that its data is being stolen but is unable to stop the attack.
These timelines point out that smaller companies not
only are not using good security protection methods, but
that they also have neither an incident plan nor an incident
staff in place. How can a store or a hotel afford this? No
wonder they are the soft targets after which cybercriminals
are now going.

The smaller firms were not proactive in discovering
breaches. In fact, they often did not discover breaches at
all. In 86% of the incidents, it was some third party that
discovered the breach and reported it to the company.
Third parties included customers, partners, and law
enforcement agencies. Many of the breaches were
discovered by banks and credit-card companies using
CPP (Common Point of Purchase) investigations, which
identify suspected fraud based on the purchase histories of
stolen cards and account numbers.
5% of all breaches were reported by employees who
saw odd things happening.
Only 6% of breaches were discovered by a process
specifically designed for breach detection, such as logging
to a SIEM (Security Information and Event Management)
system. Tragically, Verizon’s forensic analyses discovered
that 69% of all breaches were detectable from data
contained in a company’s system logs. Clearly, most of the
breached companies had no capability to make effective
use of their log information on a timely basis.
Verizon notes that how victim companies discover a
breach is an indicator of how well they know and monitor
their own environments. An effective way to contain a
breach is to consolidate log activity in a SIEM and to use a
log-analysis tool to rapidly detect suspected intrusions. If
all breached companies had adopted this technology, the
number of breaches could have been reduced by a factor
of two-thirds!

Many of the victim companies were required to be PCI
DSS (Payment Card Industry Data Security Standard)
compliant. How did the small companies that were the
targets of these attacks fare in this regard?
Not well. 89% of the victim companies had not
validated their compliance within the last twelve months,
as required by the standard. Most companies encrypted
data at rest and in transit because these are capabilities
supplied by their equipment vendors. However, they were
noncompliant in the following areas.

Sadly, monitoring access to network resources and
cardholder data, coupled with a strong firewall, is the best
defense against backdoor attacks, which accounted for a
good many of the successful breaches.

Cybercriminals are moving
away from the difficult, high-risk targets such as financial service organizations and are instead focusing their efforts
on softer targets that require less skill and that are of less
interest to law enforcement.
This move is being propelled forward by the prolific
availability on the Internet of sophisticated malware of all
types. These products not only are easily customizable,
but they also are increasingly using powerful antiforensic
techniques to prevent their detection and to hinder the
determination of the source of the attacks. With this
malware, data theft is no longer the province of the skilled
few. It is becoming available to the nefarious masses.
The absence of mega-breaches seems to have been an
anomaly in 2010. 2011 already has experienced its share
of targeted mega-breaches, including Sony, Espilon, RSA,
Steam, and Citigroup. Companies must continue their
efforts to protect against these mega-breaches. Nevertheless,
Malware as a Service is moving the focus of good security
practices into the little companies that right now are
ill-prepared to provide good security controls. Verizon
estimates that 96% of all breaches were avoidable without
difficult or expensive corrective action. 69% were detectable
from the system logs. It seems that a little education will go
a long way to combat the ill effects of Malware as a Service.

Will your organization, or one you work with, suffer a security breach this
year? Nobody’s immune, no target is too small, or too large. The methods
used by hackers to gain access to data are numerous, wide-reaching and
ever-growing. This isn’t a threat you can afford to ignore.
All kinds of organizations — from government agencies to iconic consumer
brands, internet startups to trusted financial institutions — have reported major
data breaches in the last year. Many of the stories that hit the headlines are
from the US, but this year’s Data Breach Investigations Report (DBIR) profiles
data breaches from 27 countries. This is not a localized problem, or one that you
can afford to ignore. Forty-six US states already have public disclosure laws,
and governments around the world, including the European Union, are actively
discussing introducing mandatory reporting requirements of their own.

Disclosure laws mean that you can’t keep quiet about a
breach while you deal with the fallout. As well as trying to
avoid being hacked in the first place, organizations need
to be able to spot compromises quickly and minimize the
amount of data lost.
Keeping on top of the threat landscape is a constant challenge. The best way
to effectively prepare yourself is with hard data and expert analysis. The DBIR
analyzes data from 19 organizations — covering more than 47,000 reported
security incidents and 621 confirmed data breaches from the past year. It gives
unparalleled insight into the attackers and their methods, enabling you to better
protect yourself.

Its scale is unparalleled.

69% of breaches were
spotted by an external party
— 9% were spotted by
customers.

Social tactics — using
email, phone calls and
social networks to gain
information on individuals
— are often ignored, but
contributed to 29% of
attacks.

76% of network intrusions
exploited weak or stolen
credentials. Strict policies
are required to reduce this
easily preventable risk.

SURELY SPIES AREN’T INTERESTED IN
COMPANIES LIKE MINE, ARE THEY?
The assumption is
that these attacks only target government, military and high-profile
organizations, but our data shows that this increasingly isn’t true. Don’t
underestimate the likelihood that your organization will be a target.

WHO ARE THE ATTACKERS?


75% of attacks are
opportunistic — not
targeted at a specific
individual or company
— and the vast majority
of those are financially
motivated.


Three key groups of actors commit cyber attacks. Each has different motivations
and tactics, but the net effect of their actions is disruption, financial loss and
damage to reputations. By understanding their characteristics you can be better
prepared and reduce your risk.

Activists still use very
basic methods, but
recent years have seen
some notable and widely
publicized successes.
They are opportunistic,
but have numbers on
their side. Their aim is to
maximize disruption and
embarrassment to their
victims.

Motivated by financial
gain, criminals are
more sophisticated and
calculated in how they
select targets. They
often use more complex
hacking techniques than
activists. Once they’ve
gained access, they take
any data that might have
financial value.

Often state-sponsored,
this group uses the most
sophisticated tools
to commit the most
targeted attacks. They
know what they want
— be that intellectual
property, financial data
or insider information —
and are relentless about
getting it.

Where are they from?
19% of all attacks analyzed
in this year’s report were
perpetrated by state affiliated actors — in other
words, a form of espionage.

The majority of financially motivated incidents we looked at originated in the US
or Eastern Europe — particularly Romania, Bulgaria and the Russian Federation.
Espionage cases were predominately attributable to East Asia. But the attacks
that we studied happened to companies all around the world. Geographic borders
are no protection against cyber attacks.

WHAT ARE THEIR MOTIVES?
In most industries, you’re still much more likely to suffer an attack motivated by
financial gain or revenge than espionage. Even in the industries most likely to be
targeted, the likelihood of an espionage attack is still relatively low.

Nearly three-quarters of espionage attacks were targeted at the
manufacturing, professional services and transportation industries.
But it’s not just direct espionage attacks that you need to worry about. What
happens if your partners or suppliers get hacked? The knock-on effect within your
supply chain could be just as damaging as a direct attack. Worse still, you could be
a route to an attack on one of your customers. Who would want to explain that?

WHO ELSE IS INVOLVED?
Contrary to popular belief, 86% of attacks do not involve employees or other
insiders at all (Figure 2). Of the 14% of attacks that do, it’s often lax internal
practices that make gaining access easier than you would expect.

Over half of the insiders
committing sabotage were
former employees taking
advantage of old accounts
or backdoors that weren’t
disabled.

Over 70% of IP theft cases
committed by internal
people took place within 30
days of them announcing
their resignation.

These figures add up to over
100% because sometimes
more than one asset is
involved in a breach.

Many leading newspapers and journals, online and print, are full of stories
about the dangers of the latest technologies. So it’s little wonder that
many CxOs rate cloud computing as their biggest security concern. But in
the six years we’ve been publishing the DBIR, our data has been dominated
by well-known techniques, used against the same sort of assets, again and
again. This year is no exception.

WHAT ARE the biggest threatS?
Very few of the breaches that we see each year surprise us. It’s rare that we see
something completely new, it’s usually just variations on familiar themes. Wellestablished threats shouldn’t be ignored — many are increasingly prevalent and
present an ongoing danger.
It’s still traditional assets (laptops, desktops and servers) that are most at risk —
not the new web applications that you might be spending your time worrying about.
Unapproved hardware (such as handheld card skimmers and personal storage
devices) accounts for 41% of the cases of misuse in the report.

While the sophistication of attacks is growing, most
breaches could still be easily prevented.
And while perpetrators are upping the ante — trying new techniques and
leveraging far greater resources — less than 1% of the breaches in this year’s
study used tactics rated as ‘high’ on the VERIS difficulty scale for initial
compromise. In fact, 78% of the techniques we saw were in the ‘low’ or ‘very low’
categories (Figure 3). The barriers to entry for becoming a hacker are pretty low.

It’s not just elaborate actions that have serious implications. While most breaches
are deliberate, many involve an unintentional element. Taking information home,
copying data onto a USB drive, attaching the wrong file to an email or sending it to
the wrong person, or leaving a laptop in a cab can all lead to a data breach.

It wasn’t IT-savvy developers and administrators that were responsible for
most data breaches, but customer service staff (like cashiers and call center
employees) and end users. Administrators came third, but in 60% of the
cases, their involvement was accidental.

From our analysis it’s clear that techniques targeted at users — like malware,
phishing and misuse of credentials — are major vulnerabilities. In particular,
phishing techniques have become much more sophisticated, often targeting
specific individuals (spear phishing) and using tactics that are harder for IT to
control. For example, now that people are suspicious of email, phishers are using
phone calls and social networking.
Finally, it’s tempting to think that data is most at risk when it’s being transmitted
from one location to another. For the cases where the Verizon Investigative
Response team was asked to investigate, we also tracked the state that the
data was in when it was compromised. Not one breach in this sample happened
to data that was ‘in transit’. In fact, two-thirds of breaches involved data ‘at
rest’ (in databases and on file servers), and the rest was being processed when
compromised. Does the balance of your security efforts reflect that?

If you want to see how widely available hacking tools have
become, do a web search for ‘password cracker’. And in
today’s hyperconnected world it’s highly likely that more
sophisticated tools and techniques — like those used in
espionage attacks — will quickly spread too.


95% of all state-affiliated
espionage attacks relied
on phishing in some way
— even the most targeted
and malicious attacks often
rely on relatively simple
techniques.

‘Unapproved’ hardware
accounts for 41% of the
cases of misuse in this
year’s study.

Not a single case where
the Verizon Investigative
Response team was called
in — and so we could track
the state of the data —
involved data in transit.

IT’S OBVIOUS WHEN YOU’VE BEEN HACKED.

It’s easy to spot when systems
have been breached, isn’t it?

You probably have a detailed security policy and have spent a lot of money
on security audits, hardware and specialist advice. So it’s tempting to think
that alarm bells must go off when a data breach happens. Sadly, they don’t.

HOW LONG DOES IT TAKE to spot A BREACH?
No matter how strong your defenses are, it would be foolish not to be prepared in
case an attacker gets through. Organizations should be able to spot breaches and
shut them down quickly, but our figures show that most can’t.

66% of the breaches in our
2013 report took months or
even years to discover (62%
months, 4% years).


What’s alarming is how long breaches took to spot, and how long they took to fix.
And while sensitive data remains exposed, losses grow and reputations suffer
further damage.

Breaches were identified by a wide variety of parties — a worrying 9% of breaches
were found by customers (Figure 6). Over half of the breaches identified internally
were spotted by end users — not the IT team as you might have expected.
Focusing on improving processes and giving staff better awareness training could
reap huge rewards, cutting the time taken to spot breaches and even preventing
many from happening in the first place.

Many organizations devote a disproportionate amount of time and money to
detection methods that fall below the 1% mark.

Attacks are inevitable. Companies should devote more
time and effort to detection and remediation; preventing
attacks becoming breaches, and breaches becoming
financial and reputational disasters.

WEAKEST LINK OR GREATEST ASSET?
As we’ve discussed, many attacks target people. The ‘carbon layer’ can be
a weak point, but your staff can also be your greatest asset. If you train
them how to spot breaches and avoid social engineering approaches, they
can be your first line of defense. The wider IT team should also be given
awareness training — for example, to consider that complaints about system
performance from users might be early warning signs of a breach.

69% of breaches were
spotted by an external
party.

9% of breaches were
spotted by customers.

This summary gives just a taste of the depth of information in the full
Verizon 2013 Data Breach Investigations Report. The information it offers
will help you to understand the specific threats to your company — based
on what you do, where you operate and how big you are — and be better
prepared to tackle them effectively.

Don’t buy into the
idea that there’s
a one-size-fits all solution to
protecting your
company’s assets
and reputation.

There’s no silver bullet to preventing breaches, not even in the DBIR. Spotting and
preventing data security incidents is an unending task, and one that should not
be the sole responsibility of the IT department or the chief information security
officer. Ensuring data security should be a company-wide effort all the way up to
the boardroom. The information in the DBIR will help guide your efforts.
We’ll leave you with eight key recommendations:
Eliminate unnecessary data; keep tabs on what’s left.
Perform regular checks to ensure that essential controls are met.
Collect, analyze and share incident data to create a rich information source
that can drive security program effectiveness.
Collect, analyze and share tactical threat intelligence, especially indicators
of compromise (IOCs), that can greatly assist defense and detection.
Without de-emphasizing prevention, focus on better and faster detection
through a blend of people, processes, and technology.
Regularly measure things like “number of compromised systems” and “mean
time to detection”, and use these numbers to drive better practices.
Evaluate the threat landscape to prioritize a treatment strategy. Don’t buy
into a “one-size-fits-all” approach to security.
Don’t underestimate the tenacity of your adversaries, especially espionagedriven attackers, or the power of the intelligence and tools at your disposal.

Every day more than 157 million attempts were
made (via emails, browser searches, etc.) to
entice our customers into connecting to risky
URLs.
Every day more than 353 million infected files
were exposed to our customers’ networks.
Every day an additional 71 million potentially
unwanted programs attempted installation or
launch.
Every day 55 million attempts were made by
our customers to connect to risky IP addresses,
or those addresses attempted to connect to
customers’ networks.


Intel Security interviewed almost
500 security professionals to
understand their views and
expectations about cyber threat
intelligence sharing. We learned
that awareness is very high and
that 97% of those who share
cyber threat intelligence see value
in it.

Security industry expectations are very high that cyber threat intelligence
sharing will significantly improve system and network security. But do security
practitioners actually see value in sharing cyber threat intelligence? Are they
willing to share it themselves and, if so, what are they willing to share? In 2015,
Intel Security interviewed almost 500 security professionals in a wide variety of
industries and regions, asking these questions and more. Among other things,
we learned that awareness is very high and that 97% of those who share cyber
threat intelligence see value in it. In this Key Topic, we discuss the promise of
cyber threat intelligence sharing and findings from our customer research.

The Adwind remote administration tool (RAT) is a Java-based backdoor
Trojan that targets various platforms supporting Java files. Adwind is typically
propagated through spam campaigns that employ malware-laden email
attachments, compromised web pages, and drive-by downloads. Because spam
campaigns are now short lived, with frequently changing subjects and carefully
crafted attachments, it has become more difficult for users and security
technologies to spot attacks. This has led to a rapid increase in the number of
Adwind jar file submissions from customers to McAfee Labs, with 7,295 in Q4
2015, a leap of 426% from 1,388 in Q1 2015.

Security professionals must protect against increasingly complex attacks. In the
past, they have relied primarily on signature- and behavioral-based defenses to
keep threats at bay. Those methods either block a threat by pattern matching or
stop it based on suspicious behavior. Both methods are effective and prevent a
large percentage of attacks, but what about particularly complex threats, some
of which have yet to be discovered? How do we stop zero-day attacks that slip
under the radar? That is where cyber threat intelligence comes into play.
When we talk about cyber threat intelligence (CTI), we have to understand
that the concept goes much deeper than just a list of IP addresses with poor
reputation scores or hashes of suspected bad files. CTI is evidence-based
knowledge of an emerging (or existing) threat that can be used to make informed
decisions about how to respond. CTI provides more than just the specific bits and
bytes of the threat; it also provides context around how the attack takes place.
It identifies indicators of attack (IoA) and indicators of compromise (IoC) and
potentially even the identity and motivation of the attacker. Security practitioners
and security technology can use CTI to better protect against threats or to detect
the existence of threats in the trusted environment.
Expectations are high that CTI will significantly improve system and network
security when integrated into an organization’s infrastructure and operations.
Security best practices dictate we push any threat as far as possible from the
target. By using CTI, security teams look to not only stop each attack as it
happens, but to also get a better sense of who is attacking, what methods they
are using, and what their targets are. To do this, we need a bigger picture of what
is going on. CTI is key to gaining that level of understanding about the cyber
threat.

We often read about CTI and especially the sharing of CTI. But do security
experts actually see value in sharing? Are they willing to share it themselves and,
if so, what are they willing to share?

Once organizations receive CTI through an exchange, a strong majority of them
find value in the data.
The majority of shared CTI is industry agnostic. Data is shared across
all organizations with no segmentation by industry. We asked whether
organizations would be interested in receiving CTI that was directly related to
their industry. For example, a CTI exchange between companies in the banking
industry or healthcare.

We found that 91% of respondents interviewed are interested in receiving
industry-specific CTI. This makes sense especially in an industry such as banking,
in which malware may target multiple financial institutions in similar ways.
Critical infrastructure is another area that could benefit from industry-specific
information sharing because those organizations might find malware targeted
against a specific type of device used only in that industry, as we have seen in the
past.
Overall, when asked how they felt about sharing and consuming CTI, 86% agreed
that sharing would result in better protection for their company.
Receiving threat data is only part of CTI. For data to be useful to the community,
it also has to be shared. The survey responses shift a bit when we asked if
organizations would be willing to share information with the community.
Among those we surveyed, 63% fell into the “very likely” or “somewhat likely”
categories.

What sort of data are people willing to share? The most common answer was
“behavior of malware,” followed by “URL reputations.” It is interesting that “file
reputation” was the information organizations are least willing to share.
We then took the people who responded that they are unwilling to share
data and asked why. The leading reason, by a big margin, is corporate policy
preventing them from sharing reputation information.

With all the benefits of CTI exchanges (Information Sharing and Analysis Centers,
CERTs, vendor and industry alliances, trusted partnerships, public/private
initiatives, etc.), why are organizations hesitant to share information? Let’s look at
the type of data least likely to be shared (file reputation) and the high percentage
of people responding with “company regulations” as their primary reason for not
sharing.
Although Intel Security has discussed CTI sharing with industry participants
for a number of years and most agree CTI sharing is likely to be valuable, most
balked at sharing file reputation data. We believe the reluctance to share revolves
around a misunderstanding of the type of information offered. When sharing
file reputation, a hash value is created to represent the file in question. This
hash is a unique number used to identify the file, and though it is unique to that
file, the hash cannot be used to recreate the file itself. None of the internal file
information is sent out of the network and no personally identifiable information
(PII) leaves the network. However, when an organization begins to implement a
CTI sharing effort, it runs afoul of policies that dictate that no confidential data or
PII can leave the organization. This is, of course, generally a good policy but the
lack of understanding of the content being shared becomes self-defeating in this
case.

Another reason some organizations do not want to share reputation data is that
it could potentially interfere with an ongoing investigation. Government agencies,
military organizations, and industry leaders with sensitive intellectual property
have an interest in tracking down who is trying to break into their networks. For
these organizations, it often makes sense to allow the exploit to succeed, while
monitoring it—in order to gain more information about who is behind the attack
and its target, as well as to determine a better way to mitigate future attacks. If
the threat data is shared with a CTI community and the attackers participate in
that community, they could be alerted that their activities have been identified—
resulting in new tactics to avoid further detection. This is one situation in which
the evil you know could be better than the evil you do not know.

Sharing is as much a legal problem as a technical one. The legal and trust
frameworks for sharing cyber threat information are not well established, making
it easy for risk-averse corporate lawyers to say no or to set up highly restrictive
policies to limit sharing. Much of the sharing today occurs within trusted
partnerships with NDAs, MOUs, or other contracts, all of which take some time
to be approved by both parties. Often the legal foundation for transient, eventbased sharing between two companies does not exist and cannot be established
in time to be useful for cyber responders.
Some organizations are hesitant to flag a URL or IP address with a poor
reputation due to concerns of potential legal repercussions, such as we have
seen when security products have named certain domains as spam generators
or labeled a program or add-on as spyware. This concern has expanded to the
sharing of CTI.

Privacy is also a major concern. Global laws and norms make sharing an
extremely complicated landscape. Regulated organizations must comply with
governmental regulations requiring strict controls on items such as customer or
patient data. Regulations regarding the sharing of personal information are not
always fully understood. To avoid fines and penalties, many err on the side of
caution and decide not to share any data with outside organizations except as
required to support their business operations.

For any CTI exchange to work effectively, established technical standards for
sharing information are critical. There have been multiple efforts to try to
settle on a single format for sharing cyber threat intelligence but most were
focused within a specific area, such as incident response. In 2010, MITRE,
under the direction of and with funding from the US Department of Homeland
Security, began development of a threat information architecture with the goal
of producing a representation of an automatable cyber threat indicator. This
was the first effort to focus specifically on creating an automatable, structured
representation of the cyber-threat lifecycle, related message format, and
exchange protocol.

With the industry’s need for these evolving consensus standards to become
recognized international standards, the DHS worked with the community to
transition the development and ownership of specifications to the Organization
for the Advancement of Structured Information Standards (OASIS). OASIS has
created the OASIS Cyber Threat Intelligence (CTI) Technical Committee (TC).
The CTI TC created subcommittees for each of the specifications, as well as an
interoperability subcommittee. OASIS will develop, maintain, and release all
future versions of STIX, TAXII, and CybOX.
TAXII is a specification that defines a set of services and message exchanges,
which when implemented will enable automated and secure sharing of
cyber threat information across organizational as well as product/service
boundaries. TAXII allows for the exchange of cyber threat information and is the
recommended method for exchanging STIX-formatted CTI.
STIX is the structured format used to convey specific cyber threat information.
STIX was developed to address the complete cyber threat lifecycle and provide
a consistent machine-readable format. STIX enables automated interpretation
via consistent semantics and advanced analysis capabilities. It offers the robust
expression of relationships among the individual threat lifecycle components.
STIX uses CybOX, a language for encoding “cyber observables,” which may be
seen as part of an attack. CybOX provides a standardized representation of
“facts” in the cyber domain (both network- and host-based). Cyber observables
are elements such as registry keys or key values, file deletions, file hashes, HTTP
requests, network subnets, etc. A cyber observable is a measurable event or a
stateful property in the cyber domain.

Use of STIX has taken off, with more than 60 vendors using the format to ingest,
publish, and exchange cyber threat information. The DHS has standardized
US government–related cyber threat data exchange efforts on STIX and TAXII.
The security industry is actively building and deploying tools and infrastructure
based on these specifications.

The security industry is currently undertaking the development of standards and
best practices for information sharing and analysis organizations (ISAOs). There
are many cyber threat intelligence data feeds, services, and organizations—both
commercial and nonprofit—but currently there is no expectation of consistency
across them or in what they provide. Most data formats are proprietary and
services do not use standard interfaces. Today, sharing organizations are ad hoc
in how they deal with their customers and membership. This lack of standards
has forced a consuming organization to invest a great deal of time and resources
making data useful and actionable—while costing a lot to create and maintain.
Presidential Executive Order 13691 directed the DHS to fund a nongovernmental
organization to serve as the ISAO Standards Organization. The ISAO Standards
Organization was created to identify a set of voluntary standards and guidelines
for the creation, operation, and functioning of cyber sharing and analysis
organizations. The intent is to expand the current sector-based model (financial,
health, energy, etc.) of Information Sharing and Analysis Centers, enabling the
development of innovative types of threat information sharing organizations
using standard interoperable interfaces and data formats. The process of cyber
threat event data enrichment should influence the types of new cyber threat
sharing organizations that will emerge. Although this effort is in the very early
stages, it is establishing foundational guidance that will drive the emerging cyber
threat intelligence sharing and analysis ecosystem.

Where are we headed as an industry with CTI sharing? It is one thing to establish
policies and standards around sharing, but where do we go after that?

A major legal concern is the liability organizations may face if they share CTI
with others. In some cases, we have seen antitrust concerns when a set of
organizations shares only among themselves. The US Cybersecurity Act of 2015
provides, in part, legal foundations for sharing between government and the
private sector and between private sector organizations. The Act directs the
DHS and the US Department of Justice to develop guidelines limiting receipt,
retention, use, and dissemination of CTI containing personal information by
the US federal government. The Act provides liability protection extending
to private entities only for systems monitoring and the sharing and receipt of threat indicators in the manner prescribed by the bill. It includes language that
there is no requirement to share CTI or defensive measures, or to warn or act
based on receipt of CTI or defensive measures. There is also no liability for
nonparticipation. The Act also states it is not an antitrust violation for two or
more private entities to share threat information for cyber protection purposes.
The clarifications around information sharing with the US government and other
entities, as well as the antitrust and the liability protections, allow the security
industry to take advantage of cyber threat data in a way not possible before the
Act was signed. This Act could become a model for global information sharing
legislation. The legal liability relief provided by the Act will help to reduce the
fear of sharing and provide the guidelines corporate attorneys have desired.

Today we share more threat data than ever, but are we gaining insights into what
really matters? Are we finding just opportunistic attacks or are we finding the
campaigns that really threaten our operations? In the past, threat feeds, shared
information, and security products have not used industry-standard formats.
The proprietary nature of data formats has complicated our ability to correlate
and use advanced analytics to discover what we should discover. With standard
threat data representations, communities of cooperation will be able to review
and examine malicious events, attacks, and tools in a much more coordinated
fashion than has been possible in the past. This advantage will increasingly occur
in for-profit, not-for-profit, and open-source organizations.

The automated creation, import, and export of CTI is critical for an organization
to take advantage of a CTI exchange. Although CTI can be used to manually
hunt for threats within an environment, stopping attacks in real time (or near
real time) will require automated tools and processes. In order to provide
adaptive response and make CTI actionable, security-related products must
be able to ingest CTI and act on it without unnecessary human intervention.
Formerly, the discovery that a system had malware was limited to that system;
today, that information needs to be available throughout the enterprise so
an organization can make proper responses. For example, if a malicious file is
discovered on an endpoint, notification must be shared across the enterprise’s
security infrastructure to assure the malware is hunted internally, while blocking
attachments at the boundary whose hashes match that of the malicious file.
Intelligent responses are possible when security vendors take advantage of
standard CTI interfaces and data formats. This standardization allows CTI to be
actionable and help reduce the cost of security operations by assuring human
resources are not a bottleneck and are used appropriately.

New security knowledge services are emerging. Much of the past focus of CTI
sharing has been on identifying and sharing cyber indicators and observables.
A search on “threat intelligence exchange” provides hundreds of results.
Although these results contain valid threat indicators, a big problem has been
their consistency, type, and quality. When comparing multiple threat exchanges,
organizations discover different exchanges provide different content. One may
provide a file hash and IP reputation while another contains registry keys and
domain name reputation for the same threat. We expect to see CTI aggregators
provide standardized feeds in the future.

Although data of this type is vital, we are just beginning to really understand
the entire threat lifecycle. As we learn more about a threat, its associated CTI
becomes more complete and more valuable. Whole businesses will arise whose
only mission is to enrich the data around individual threats to assure their
customers have a better picture of what is occurring and how to rapidly mitigate
threats to their organizations.
One new organization making CTI actionable is the Cyber Threat Alliance
(CTA), which Intel Security helped found. The CTA is a cross-vertical, security
vendor initiative whose members share threat information to improve defenses
against advanced cyber adversaries who threaten the members’ customers.
Members share important individual elements of a threat life cycle—including
vulnerabilities and exploits, new malware samples, and botnet control
infrastructure—that can be incorporated into each member’s security products.
The CTA’s coordinated research allows members to gain insight into the full
attack lifecycle of specific campaigns, including in-depth technical analysis and
the development of recommendations for prevention and mitigation.


CTI is gaining traction within the security industry as a way to combat advanced
threats. As a result of our study, Intel Security found the overall acceptance
and desire for CTI is high, but many companies face hurdles to fully realize the
benefits of sharing threat data with the community. Some of those hurdles
are falling. The use of CTI will become a critical component of organizations’
defenses as structured, enriched data will allow organizations to respond more
quickly, with a better view of the cyber event landscape.

The Adwind remote administration tool (RAT) is a Java-based backdoor Trojan
that targets various platforms supporting Java files. Adwind does not exploit any
vulnerability. Most commonly for an infection to occur, the user must execute
the malware by double-clicking on the jar file that typically arrives as an email
attachment, or open an infected Microsoft Word document. Infection begins if
the user has the Java Runtime Environment installed. Once the malicious jar file
runs successfully on the target system, the malware silently installs itself and
connects to a remote server through a preconfigured port to receive commands
from the remote attacker and perform further malicious activities. The number of
Adwind jar file submissions to McAfee Labs has grown to 7,295 in Q4 2015 from
1,388 in Q1 2015, a 426% increase.

Adwind evolved from the Frutas RAT. Frutas is a Java-based RAT, discovered
in early 2013, that has been widely used in phishing email campaigns against
prominent telecom, mining, government, and finance companies in Europe and
Asia. Frutas allows attackers to create a jar file with backdoor functions that can
be executed on a compromised system. Once run, Frutas parses an embedded
configuration file to connect to its control server. By the summer of 2013, the
name was changed to Adwind. In November 2013, Adwind was rebranded and
sold under a new name: UNRECOM (UNiversal REmote COntrol Multiplatform).

Adwind is typically propagated through spam campaigns that employ malwareladen email attachments, compromised web pages, and drive-by downloads. Its
distribution mechanism has evolved: Earlier spam campaigns lasted days and
weeks and used the same email subject or attachment name. This consistency
helped security vendors quickly detect and mitigate Adwind. Now, spam
campaigns are short lived, with frequently changing subjects and carefully
crafted attachments, allowing Adwind to avoid detection. Two spam email
examples follow:
Example 1: The malicious jar file is embedded in a Word doc that upon
execution will drop and run the backdoor on the system:

The contents of spam email are crafted to lure users using social engineering
techniques.
With an effective subject line and innocently named jar file, an unsuspecting
user could read the email and open the attachment.

Adwind has several variants, which means that the contents of the jar files can
vary.

The Trojan changes the folder and file attributes to system, hidden, and read only.

The Adwind registry key with random names assigned.

Adwind comes in an obfuscated form to hide its malicious intent. Its payload and
configuration file (which serves as an installation file) are encrypted with the DES,
RC4, or RC6 cipher, depending on the variant. The Adwind backdoor will decrypt
itself on the fly during execution.

We can see this code contains the randomly chosen path and filename for the
embedded and encrypted malicious jar file, and half of the RC6 key that will
be used to decrypt it. The other half of the RC6 key is retrieved from the other
available class files. In the preceding code QL1sv1aEo is the RC6-encrypted
malicious jar file containing the Adwind backdoor.
After decrypting the encrypted jar file, we can gain access to the Adwind
backdoor class files and resources.

After Adwind successfully infects a system, we have seen it log keystrokes,
modify and delete files, download and execute further malware, take
screenshots, access the system’s camera, take control of the mouse and
keyboard, update itself, and more.

Keep systems current by applying the latest security technology
updates and antimalware definitions.
Enable automatic operating system updates, or download operating
system updates regularly, to keep them patched against known
vulnerabilities.
Configure antimalware software to automatically scan all email and
instant-message attachments.
Make sure email programs do not automatically open attachments or
automatically render graphics, and turn off the preview pane.
Configure browser security settings to medium level or above.
Use great caution when opening attachments, especially when those
attachments carry the jar, pdf, doc, or xls extension.
Never open unsolicited emails or unexpected attachments—even
from known people.
Beware of spam-based phishing schemes. Don’t click on links in
emails or instant messages.


The number of new rootkit
malware samples dropped
precipitously in Q4, continuing
a long-term downward trend in
this type of attack. We believe
the trend, which started in Q3
2011, is driven by ongoing
customer adoption of 64-bit
Intel processors coupled with
64-bit Microsoft Windows.
These technologies include
such features as Kernel Patch
Protection and Secure Boot,
which together protect against
rootkit malware.
Because we do not expect rootkit
malware to be significant in the
near future, this is the last quarter
in which we will report rootkit
malware sample data. Of course,
McAfee Labs will continue to
monitor rootkit malware and we
will resume our reporting should
it again become significant.

We saw a 26% increase in
new ransomware samples in
Q4 2015. The reason? Opensource ransomware code (for
example, Hidden Tear, EDA2)
and ransomware-as-a-service
(Ransom32, Encryptor) make
it simpler to create successful
attacks. TeslaCrypt and
CryptoWall 3 campaigns also
continue. And as we detailed
in the McAfee Labs Threats
Report: May 2015, ransomware
campaigns are financially
lucrative with little chance of
arrest, so they have become quite
popular.
The number of new malicious
signed binaries has dropped
each quarter for the past year,
in Q4 2015 reaching the lowest
level since Q2 2013. McAfee Labs
postulates that as businesses
migrate to stronger hashing
functions, older certificates with
significant presence in the dark
market are either expiring or
being revoked. Also, technologies
such as Smart Screen (part of
Microsoft Internet Explorer
but moving to other parts of
Windows) represent additional
tests of trust that might make the
signing of malicious binaries less
beneficial to malware authors.


The Kelihos botnet held the top
position during Q4, reaching
about 95% of its Q3 volume.
Alongside its well-known
pharmaceutical spam, Kelihos
took on another flavor by
targeting Chinese recipients with
“job offer” themed campaigns.
Lethic botnet volumes increased
by 60% during Q4, primarily with
campaigns pushing knock-off
designer wristwatches.

Perhaps what is most remarkable
is that these numbers no longer surprise us. As real life and online become
indistinguishable from each other, cybercrime has become a part of our daily
lives. Attacks against businesses and nations hit the headlines with such
regularity that we’ve become numb to the sheer volume and acceleration of
cyber threats.
Most threat reports only scratch the surface of the threat landscape to examine multiple facets, including targeted attacks, smartphone
threats, social media scams, and Internet of Things (IoT) vulnerabilities, as well
as attackers’ tactics, motivations, and behaviors. While there is much to be
learned from this comprehensive view into the threat landscape, the following
are six key findings and trends from 2015.

Advanced attack groups continue to profit from
previously undiscovered flaws in browsers
and website plugins
In 2015, the number of zero-day vulnerabilities discovered
more than doubled to 54, a 125 percent increase from the
year before. Or put another way, a new zero-day vulnerability was found every week (on average) in 2015. In
2013, the number of zero-day vulnerabilities (23) doubled
from the year before. In 2014, the number held relatively
steady at 24, leading us to conclude that we had reached a
plateau. That theory was short-lived. The 2015 explosion in
zero-day discoveries reaffirms the critical role they play in
lucrative targeted attacks.
Given the value of these vulnerabilities, it’s not surprising that a market has evolved to meet demand. In fact,
at the rate that zero-day vulnerabilities are being discovered, they may become a commodity product. Targeted
attack groups exploit the vulnerabilities until they are

publicly exposed, then toss them aside for newly discovered vulnerabilities. When The Hacking Team was exposed
in 2015 as having at least six zero-days in its portfolio, it
confirmed our characterization of the hunt for zero days
as being professionalized.
Vulnerabilities can appear in almost any type of software,
but the most attractive to targeted attackers is software
that is widely used. Again and again, the majority of these
vulnerabilities are discovered in software such as Internet
Explorer and Adobe Flash, which are used on a daily
basis by a vast number of consumers and professionals.
Four of the five most exploited zero-day vulnerabilities in
2015 were Adobe Flash. Once discovered, the zero days
are quickly added to cybercriminal toolkits and exploited.
At this point, millions will be attacked and hundreds of
thousands infected if a patch is not available, or if people
have not moved quickly enough to apply the patch.

Over Half a Billion Personal Records
Were Stolen or Lost in 2015.

Spear-Phishing Campaigns Targeting
Employees Increased 55 Percent in 2015.

At the close of 2015, the world experienced the largest data
breach ever publicly reported. An astounding 191 million
records were exposed. It may have been the largest megabreach, but it wasn’t alone. In 2015, a record-setting total
of nine mega-breaches were reported. (A mega-breach is
defined as a breach of more than 10 million records.)

In 2015, a government organization or a financial company
targeted for attack once was most likely to be targeted
again at least three more times throughout the year.
Overall, large businesses that experienced a cyber attack
saw an average of 3.6 successful attacks each.

The total reported number of exposed identities jumped
23 percent to 429 million. But this number hides a bigger
story. In 2015, more and more companies chose not to
reveal the full extent of the breaches they experienced.
Companies choosing not to report the number of records
lost increased by 85 percent. A conservative estimate by
Symantec of those unreported breaches pushes the real
number of records lost to more than half a billion.
The fact that companies are increasingly choosing to hold
back critical details after a breach is a disturbing trend.
Transparency is critical to security. While numerous data
sharing initiatives are underway in the security industry,
helping all of us improve our security products and
postures, some of this data is getting harder to collect.

There were over one million web attacks against people
each and every day in 2015. Many people believe that
keeping to well-known, legitimate websites will keep them
safe from online crime. This is not true. Cybercriminals
continue to take advantage of vulnerabilities in legitimate
websites to infect users, because website administrators
fail to secure their websites. More than 75 percent of all
legitimate websites have unpatched vulnerabilities. Fifteen
percent of legitimate websites have vulnerabilities deemed
‘critical,’ which means it takes trivial effort for cybercriminals to gain access and manipulate these sites for their
own purposes. It’s time for website administrators to step
up and address the risks more aggressively.

In the last five years, we have observed a steady increase
in attacks targeting businesses with less than 250
employees, with 43 percent of all attacks targeted at small
businesses in 2015, proving that companies of all sizes are
at risk.
It’s not just Fortune 500 companies and nation states at
risk of having IP stolen–even the local laundry service is
a target. In one example, an organization of 35 employees
was the victim of a cyber attack by a competitor. The
competitor hid in their network for two years stealing
customer and pricing information, giving them a significant advantage. This serves as a clear warning that all
businesses are potentially vulnerable to targeted attacks.
In fact, spear-phishing campaigns targeting employees
increased 55 percent in 2015. No business is without risk.
Attackers motivated purely by profit can be just as technically sophisticated and well-organized as any nation
state-sponsored attackers. Take, for example, the Butterfly
gang, who steal information to use in stock manipulation.

Cyber criminals are using encryption as a weapon to
hold companies’ and individuals’ critical data hostage
Ransomware continues to evolve. Last year, we saw Crypto-ransomware (encrypting files) push the less damaging
locker-style ransomware (locking the computer screen) out
of the picture. Crypto-style ransomware grew 35 percent
in 2015. An extremely profitable type of attack, ransomware will continue to ensnare PC users and expand to any
network-connected device that can be held hostage for a
profit. In 2015, ransomware found new targets and moved
beyond its focus on PCs to smart phones, Mac, and Linux
systems. Symantec even demonstrated proof-of-concept
attacks against smart watches and televisions in 2015.

Cyber scammers now make you call them
to hand over your cash
While ransomware continues to grow as a threat, it is not
the only threat that people face. As people conduct more
of their lives online, attackers are finding new ways to lure
victims. Fake technical support scams, first reported by
Symantec in 2010, have evolved from cold-calling unsuspecting victims to the attacker fooling victims into calling
them directly. Attackers trick people with pop-ups that alert
them to a serious error or problem, thus steering the victim
to an 800 number, where a “technical support representative” attempts to sell the victim worthless services. In 2015,
Symantec blocked 100 million of these types of attacks.
Attackers continue to find ways to profit from what can
be stolen online. Last year, Netflix expanded into new
countries, attracting the attention of attackers. Symantec
researchers discovered logins and passwords to legitimate Netflix accounts being sold on the black market. The
account access information was stolen via phishing or
malware. Of course, reselling account access on the black
market is not a new phenomenon. Symantec continues to
see stolen hotel loyalty, airline frequent flyer, and gaming
accounts advertised for sale on the black market.

Smartphones are an increasingly
attractive target for online criminals.
As a result, they are investing in
more sophisticated attacks that
are effective at stealing valuable
personal data or extorting money
from victims. Although Android users
remain the main target, 2015 saw
effective attacks on Apple devices as
well, and iOS devices did not need to
be jail-broken to be compromised.

The world bought more than 1.4 billion smartphones in 2015, up
10 percent from the 1.3 billion units sold in the previous year,
according to IDC’s Worldwide Quarterly Mobile Phone Tracker
(January 27, 2016). Five out of six new phones were running
Android, with one in seven running Apple’s iOS operating
system (Smartphone OS Market Share, 2015, Q2). One mobile
manufacturer, Ericsson, predicts there could be as many as 6.4
billion smartphone subscriptions by the end of 2020, almost one
per person.
At the same time, high-end phones and tablets have powerful
processors and with 4G network, they have high-bandwidth
connectivity. They also contain valuable personal information.
In 2015, Apple Pay launched in more countries around the
world. With Samsung Pay and Android Pay also competing to
manage the cards in your wallet, other mobile payment systems
are likely to follow. All of this makes smartphones very attractive to criminals.

With many app stores, users are able to browse, purchase, and
remotely install apps from their desktop, providing a unique
opportunity for a cross-over of threats. In one example with
Google Play, customers can browse the Play Store from their
computer using a normal web browser, installing apps directly
onto their phone. Recent examples of some Windows malware
have exploited this by stealing browser cookies for Google Play
sessions from the infected desktop computer and using these
stolen cookies (essentially the users’ credentials), impersonating the user to remotely install apps onto the victims’ phones
and tablets without their knowledge or consent.
The number of Android malware families added in 2015 grew by 6 percent, compared with the 20 percent growth in 2014.


In 2012, IOS FinFinancial institutionsh had been the first example of a malicious
iOS app to be discovered in the Apple Store. FinFinancial institutionsh was able to
steal information from a compromised device. OSX Wirelurker
emerged in 2014, which used an attack involving USB connections to a Mac or PC, potentially enabling apps to be installed on
non-jailbroken iOS devices.
However, in 2015, attacks using XcodeGhost and YiSpecter were
revealed not to require vulnerabilities, or to be jail-broken, in
order to compromise an iOS device. We will be taking a closer
look at these and other mobile threats later in this section.

The number of mobile vulnerabilities has increased every year
over the past three years. Unlike Android devices, iOS vulnerabilities have been a critical part of gaining access to an iOS
device, especially for jail-breaking. Jail-breaking enables a user
to install apps that are not authorized on the Apple Store and
bypasses the integral security of iOS. It is much more difficult to
compromise a non-jailbroken device, as this typically requires
an app to be installed by downloading it from the Apple Store.
Apple is well-known for its stringent screening processes, which
is why the number of malicious iOS apps is so much smaller
than for Android.

Vulnerabilities on the iOS platform have accounted for the greatest number of mobile vulnerabilities in recent years, with research often fueled by the
interest to jail-break devices or gain unauthorized access to install malware.

Android malware is becoming stealthier. For example, malware
authors started to obfuscate code to bypass signature-based
security software. Additionally, before they begin their attacks,
some malware can now check to see if it is running on real
phones or the kind of emulators or sandboxes that security
researchers use.

No matter how quickly Google patches critical vulnerabilities in
the Android OS, the speed at which end-users receive updates is
dependent on their device manufacturers, and sometimes this can
take longer. This was highlighted when on July 2015, seven vulnerabilities were patched that could allow attackers to compromise
affected devices by simply sending them a malicious multimedia
message (MMS); all the intended victim had to do was to look at the
malicious message, triggering an exploit.
The seven vulnerabilities involved were collectively known as
the “Google Stagefright Media Playback Engine Multiple Remote
Code Execution Vulnerabilities,” (CVE-2015-1538, CVE-2015-1539,
CVE-2015-3824, CVE-2015-3826, CVE-2015-3827, CVE-2015-3828
and CVE-2015-3829), and all were related to an Android component
known as libStageFright, which handled media playback. Joshua
Drake, from Zimperium zLabs, reported the vulnerabilities to
Google in April and May 2015, raising further concerns that while
Google had provided patches to its partners, many manufacturers
took much longer providing patches to protect their customers. The
severity of these vulnerabilities was compounded by the fact that
despite the availability of a patch from Google, users remained at
risk until carriers and manufacturers rolled out their own patches.
This can often take weeks or months, and many older devices may
never have a patch pushed out to them at all.
However, Google was keen to point out that devices with Android
4.0 and higher (approximately 95% of active Android devices),
have protection against a buffer overflow attack built-in, using a
technology called Address Space Layout Randomization, (ASLR).
Additionally, Android users were able to turn-off the automatic
retrieval of multimedia messages through the built-in Messaging
application, as well as through Google Hangouts.
Whilst this afforded partial mitigation, it could not prevent the
vulnerabilities from being exploited if a malformed or malicious
multimedia message was downloaded and opened.
In October 2015, two more Android vulnerabilities (CVE-20156602 and CVE-2015-3876), similar to the original Stagefright bug,
were disclosed. Again, if exploited they could allow an attacker to
gain control of a compromised device, this time when the intended
victim viewed a preview of an .mp3 or .mp4 file. By creating
malicious audio or video files, attackers could entice a user to
preview a song or video on an unpatched Android device.
Google had previously patched the libStageFright library so it
no longer automatically processed such messages; however, it
remained possible for attackers to exploit libStageFright through
the mobile browser. Dubbed Stagefright 2.0, these new vulnerabilities could also be exploited through man-in-the-middle attacks
and through third-party applications that still used Stagefright.
Discovered and reported in August, the patches for these new
vulnerabilities were included in Google’s October Monthly Security
Update.

Besides familiar tricks such as hiding malicious code inside
ostensibly legitimate apps, or being disguised as something
more useful, attackers are using more sophisticated techniques
to make money from their victims. For example, Symantec
researchers uncovered a new Android phishing Trojan that tricks
users into entering their banking credentials by popping up a
fake login page on top of legitimate banking apps. Similarly, the
latest Android ransomware copies Google’s design style to make
it appear more legitimate and intimidating when it displays fake
FBI warnings on users’ lockscreens. We have also seen phone
ransomware start to encrypt files, such as pictures, rather than
simply change the phone’s access PIN.

Thanks to Apple’s tight control over its app store and operating
system, threats to iPhones and iPads have been infrequent and
limited in scale. This changed in 2015. In 2015, we identified nine new iOS threat families,
compared to four in total previously.

As Apple sells more and more iPads and iPhones, we believe that
criminals will increasingly target them, drawn in part by the
higher disposable income (on average) of their owners. However,
owners and Apple users should no longer assume that Apple
devices are immune from attack. In September 2015, malware
was discovered in a number of iOS applications in China and was
discovered in a number of legitimate Apple Store apps, including
WeChat, a popular IM application. The problem was that these
apps were not specifically designed to be malicious, but rather
their developers had been compromised with malware that was
embedded into the apps they were developing.
The malicious code, known as XcodeGhost (detected as
OSX Codgost), had been discovered in certain unofficial versions
of Apple’s integrated development environment, Xcode. Developers of iOS applications that used these infected versions of Xcode
were unknowingly allowing malicious code to be inserted into
their own official iOS applications, putting their own users at risk.
If a user downloads and installs an infected app, XcodeGhost
uploads information about the device to its command and
control (C&C) server.

It has been estimated that hundreds of iOS apps on the Apple
App Store were infected, potentially affecting hundreds of
thousands of users, particularly in China, where the WeChat app
is particularly popular.
This threat did not require a jailbroken iOS device, as with other
iOS threats previously, making it a new and rather worrying
development in the mobile threat landscape. Symantec blocked
33 attacks in 2015, between September and December. Moreover,
it wasn’t just Apple’s iOS that came under fire in 2015. Mac OS X,
the company’s popular desktop operating system, also saw a rise
in vulnerabilities, exploits, and threats during the year.

In 2015, we saw an escalation in threats targeting the iOS
platform, including YiSpecter (detected as IOS.Specter), which
was also discovered in October 2015. YiSpecter was specifically designed to target Chinese speakers and has affected mainly
users in East Asia, including China and Taiwan.
YiSpecter is a Trojan horse that is able to exploit both jailbroken and non-jailbroken iOS devices; it essentially provides a
back door onto the compromised device and installs adware.
The Trojan allows an attacker to accomplish a range of tasks,
including uninstalling apps, downloading new fraudulent apps,
and forcing other apps to display adverts.

YiSpecter was the first iOS threat that took advantage of Apple’s
enterprise app provisioning framework to compromise non-jailbroken devices. The framework is used by many businesses to
legitimately deploy private apps to their workforce without
having to make them publicly available on the official App Store.
Apps are built and signed with enterprise certificates, and do
not need to be vetted by Apple before being distributed outside
of the App Store. This also affords more scope for businesses
to develop apps with features that would otherwise be rejected
by Apple, but could still be signed and deployed legitimately
through the framework.
However, as demonstrated with YiSpecter, iOS enterprise certificates can also be used to package and sign their malware. It’s not
known exactly how the attackers gained access to certificates,
but it’s possible that they registered with Apple as an enterprise,

paying the necessary fees and following the vetting procedure.
Alternatively, they may have been able to steal legitimate certificates from an already-registered developer or by partnering
with one.
Once the attackers had access to a valid enterprise certificate,
they were able to create, sign, and distribute their malicious
apps, potentially to any iOS device, without any further intervention from Apple. Of course, when Apple learns of the
misuse of any enterprise certificate, it can be instantly revoked,
rendering any apps signed by it useless. Enterprise-signed apps
can generally only be installed once the user accepts the request
to trust the app or developer. From experience, we know that
asking the user whether they trust an app or developer is rarely
an effective security measure, but it is one last line of defense
that needs to be crossed before the malware can be installed.


One of the reasons that YiSpecter included more advanced functionality was because it also used Apple’s own private APIs to
perform activities that standard iOS apps cannot. These “private
APIs” are reserved for Apple’s own apps to be able to perform
a range of system-level actions. Other iOS developers are not
supposed to use these APIs in their apps, and any third-party apps that do so are rejected from the Apple App Store. Of
course, YiSpecter is able to circumvent the official App Store,
instead relying on unofficial distribution channels to spread the
malware. As a result, the threat is able to take advantage of the
private APIs for its own purposes.

In October 2015, Apple pulled as many as 256 apps from its App
Store for apparently violating the company’s privacy guidelines.
The apps had used third-party advertising technology from a
company called Youmi (detected as Android.Youmi), which was
secretly being used to access private information, including
Apple ID email addresses and International Mobile Station
Equipment Identity (IMEI) numbers.
Soon after this, the same advertising library was discovered in a
number of Android apps, where it was being used to perform a
range of actions that could also compromise the user’s privacy,
including harvesting their GPS location and phone number, as
well as downloading additional, potentially unwanted applications.


Adware and its mobile counterpart, mobile Adware (or madware),
has been around for many years and is a popular way of financing
free apps, where the app developer is paid a fee for each of the
adverts presented to their users. Many people are happy relinquish a small area of the screen for advertising in exchange for a
free app; however, this may sometimes happen without consent or be particularly aggressive. Symantec recorded a 77 percent
rise in apps containing unwanted madware.
Ad-blocking tools have grown in popularity as a way to avoid
this, and by blocking mobile ads, they also help to reduce mobile
data costs incurred with madware traffic and minimize the
number of on-screen ads. Furthermore, such software can also
help to improve the security posture of a device by blocking
potentially unwanted madware that may be installed without
the user’s permission or knowledge.

We predict that mobile threats will continue to proliferate in
2016. We may soon see PC-like exploit kits for phones commercialized on the black market.
At the same time, Apple and Google are working hard to secure
their operating systems and wider ecosystems. In particular,
we anticipate improvements in the techniques used to validate
and sign applications, as well as in application delivery. Phone
users will become accustomed to frequent on-by-default application and operating system updates, and the need for security
software on their mobile devices.
This is perhaps an indicator of progress, rather than a cause for
despair. It suggests that security researchers, operating system,
developers, and app writers are, in fact, paying more attention
to mobile security by identifying and fixing more problems.
Although we expect mobile devices to come under growing
attack over the next year, there is also hope that with the right
preventative measures and continuing investment in security,
users can achieve a high level of protection against them.


Internet-connected things are
multiplying rapidly. We saw many
proof-of-concept and real-world
attacks in 2015, identifying serious
vulnerabilities in cars, medical
devices, and more. Manufacturers
need to prioritize security to
reduce the risk of serious personal,
economic, and social consequences.

The Internet of Things has already arrived. We only have to look
around at our own environment to see the impact it is having on our
everyday lives. The average smart phone now has more computing
power than the Space Shuttle; a smartwatch now downloads
updates from the Internet; the point-of-sale terminals at a coffee
shop are all connected to the company’s central financial system;
many cars now have satellite navigation and Bluetooth connections;
an Internet-connected thermostat can control the temperature in
our homes.
In the USA, for example, there are 25 online devices per 100 inhabitants, and that is just the beginning. Gartner forecasts that 6.4
billion connected things will be in use worldwide in 2016, and will

reach 20.8 billion by 2020 (Gartner, Inc., press release, November
10, 2015).
If the Internet of Things is to deliver the promised $2 trillion
economic benefit, designers and manufacturers have to address
fundamental security challenges. The prospects, however, are not
good.

Over the last year, Symantec has seen an increase in proofof-concept attacks and growing numbers of IoT attacks in the
wild. In numerous cases, the vulnerabilities were obvious and
all too easy to exploit. IoT devices often lack stringent security
measures, and some attacks are able to exploit vulnerabilities in
the underlying Linux-based operating systems found in several
IoT devices and routers. Many issues stem from how securely
vendors implemented mechanisms for authentication and
encryption (or not). Here are some examples:
Fiat Chrysler recalled 1.4 million vehicles after
researchers demonstrated a proof-of-concept attack where
they managed to take control of the vehicle remotely. In the
UK, thieves hacked keyless entry systems to steal cars.
Millions of homes are vulnerable to
cyberattacks. Symantec research found multiple vulnerabilities in 50 commercially available devices, including a ‘smart’
door lock that could be opened remotely online without a
password.
Researchers have found potentially deadly
vulnerabilities in dozens of devices such as insulin pumps,
x-ray systems, CT-scanners, medical refrigerators, and
implantable defibrillators.
Hundreds of millions of Internet-connected TVs
are potentially vulnerable to click fraud, botnets, data theft,
and even ransomware, according to Symantec research.
Thousands of everyday devices,
including routers, webcams, and Internet phones, share the
same hard-coded SSH and HTTPS server certificates, leaving
more than 4 million devices vulnerable to interception and
unauthorised access.

We expect to see more stories like this in the coming year. If a
device can be hacked, it likely will be. In addition, where there
are proof-of-concept attacks, real attacks invariably follow. We
may even expect to see IoT devices as the preferred route for
attacking an organization, and potentially the most difficult for
incident response staff to recognize and remove.
Given the present poor state of security on connected devices,
they will present an increasingly attractive target to criminals
who look for easy targets in the same way that burglars prefer
houses without alarms or resident dogs.
Researchers have found potentially deadly vulnerabilities in
dozens of devices such as insulin pumps and implantable defibrillators.
Hundreds of millions of Internet-connected TVs are potentially
vulnerable to click fraud, botnets, data theft and even ransomware.

Fiat Chrysler recalled 1.4 million vehicles after researchers
demonstrated a proof-of-concept attack where they managed to
take control of the vehicle remotely. In the UK, thieves hacked
keyless entry systems to steal cars.

Despite the increased attention and rapid development, the
Internet of Things has not reached a critical mass when it comes
to home automation. Perhaps one of the final hurdles holding
IoT dominance back has to do with standardized communication protocols. So far, we have seen plenty of growth with
interconnected IoT devices using well-established protocols,
such as Wi-Fi and Bluetooth®. Devices that utilize 802 11b/g/n/
ac wireless protocols, including Smart TVs, intelligent thermostats, IP cameras, and other devices, are cropping up everywhere.
Devices that employ Bluetooth 4, such as fitness trackers,
smart watches, and other wearables, have also helped IoT gain
significant traction in that market.
However, these communication protocols fall flat in many home
automation cases. The latest Wi-Fi technologies work great
for quick and efficient wireless connections, but have power
requirements that can put a strain on smaller devices. Bluetooth
does operate better in this scenario, but its short range does
not make it ideal for communication from more than a few feet
away. That’s not to say that it cannot be done. It just has not
been possible to do it cheaply enough to bring the technology
to ubiquity.
A number of vendors have stepped in to address these communications challenges, though none has yet to dominate the market.
This has resulted in a fragmented market of competing wireless
communication specifications tied to specific vendors or vendor
groups. What may finally open the gates for small, low powered
IoT devices is Wi-Fi HaLow™ (IEEE 802 11ah), a new communications protocol for IoT and wearable devices, slated to be finalized
and certified between 2016 and 2018. Once released, router
manufacturers could quickly incorporate the protocol to their
products, as with other communications protocols like 802.11ac,
and in so doing, open the doors for consumers to automate their
homes more easily and cheaply.
Of course, when introducing any new technology, the attack
surface expands, which presents a variety of new problems from
a security standpoint. Proprietary IoT networks have already
been found with multiple security vulnerabilities, some trivial
and some serious. The fundamental question regarding IoT and
home automation is not, “How do we do this?” It is, “How do we
do this securely?”
With the adoption of common standards, it is likely that older
proprietary protocols will fall by the wayside, paving the way
for potentially greater consolidation in the marketplace. While
larger, well-known brand names will continue to release their
own products, smaller, innovative IoT companies will become
attractive targets for organizations seeking to quickly expand
their portfolios into those areas. However, cybersecurity must
be at the core for the adoption of this new breed of IoT technology to succeed. As more homes become connected, it will be

difficult for consumers to ignore the benefits that this new technology will promise.
It is always important to weigh the convenience of remote
control, automation, ease of use, and the benefits they can bring,
against the potential risks introduced that could lead to hackers
opening IoT locks, disabling IoT burglar alarms, or generally
wreaking havoc with IoT devices.

How to Protect Connected Devices
Protecting the Internet of things requires the same holistic
approach as other areas of IT security. Unfortunately, both
Industrial IoT ecosystems, like the Industrial Internet Consortium (IIC), and consumer IoT ecosystems, such as the AllSeen
Alliance, are still very early in defining standards for this
rapidly evolving area. To address this, Symantec published its
Security Reference Architecture, and contributed to the IIC and
AllSeen efforts, along with the Online Trust Alliance (OTA) IoT
Trust Framework, and the US Department of Homeland Security
(DHS) Security Tenets for Life Critical Embedded Systems.
Effective security requires layers of security built into devices
and the infrastructure that manages them, including authentication, code signing, and on-device security (such as Embedded
Critical System Protection technology). Analytics, auditing, and
alerting are also key to understanding the nature of threats
emerging in this area. Finally, strong SSL/TLS encryption technology plays a crucial role in authentication and data protection.

Towards a Secure, Connected Future
As with other aspects of Internet security, some threats are
more dangerous than others are, and while a hacked fitness
monitor may be an inconvenience, a vulnerability in millions of
cars may present a more serious danger. Similarly, a backdoor
in a medical device may give thieves access to medical records,
albeit on a relatively small-scale, or it may lead to serious injury
or potentially even death.
The remedies are well-understood, but manufacturers need to
prioritize security and find the right balance between innovation,
ease-of-use, and time-to-market constraints. Fundamentally,
companies and consumers need to be assured that suppliers are
building security into the IoT devices they are buying. 

If web servers are vulnerable, then
so are the websites they host and
the people who visit them. Attackers
are exploiting any vulnerability they
can to compromise websites and
commandeer their host servers. The
ease of use and wide availability
of web attack toolkits is feeding
the number of web attacks, which
doubled in 2015.
Website owners still aren’t patching and updating their websites
and servers as often as perhaps they should. This is like leaving
a window open through which cybercriminals can climb through
and take advantage of whatever they find.
Over the past three years, more than three quarters of websites
scanned contained unpatched vulnerabilities, one in seven (15
percent) of which were deemed critical in 2015.

Adobe Flash Player has continually been the subject of malicious
exploitation over the years and accounted for 10 vulnerabilities
that were classified as zero days in 2015 (17 percent) compared
with 12 in 2014 (50 percent), and five in 2013 (22 percent). With
such rich pickings, it’s clear to see why attackers are partial to
exploiting Flash. Apple, Google, and Mozilla have all expressed
their concerns with the Flash plugin, and Google recently
announced that Flash will no longer be supported natively in
Chrome. Mozilla continues to support Flash within Firefox as an
exception to the general plugin policy.
From a security perspective, we expect Adobe Flash will
gradually fall out of common usage over the next year.

An average of one million web attacks was blocked each day in
2015, an increase of 117 percent (more than double) compared with 2014.

It’s not only plugins for web browsers that are vulnerable and
exploited. Take WordPress, which now powers a quarter of the
world’s websites, for example. Anyone can write a WordPress
plugin―and they often do. Plugins range from the useful to the
completely ridiculous, such as Logout Roulette: “on every admin
page load, there’s a 1 in 10 chance you’ll be logged out.”

The Angler exploit kit, first seen in 2013, is arguably among
the most sophisticated exploit kits available today, and has
pioneered many technical advances that other exploit kits have
often followed, including the use of anti-cybersecurity countermeasures. For example, Angler is able to download and execute
malware from memory, without needing to write any files to disk,
in an attempt to evade detection by traditional security technology. Additionally, one significant factor in Angler’s incredible
growth in 2015 is that it has been very fast at integrating the
growing number of new zero-day exploits into its arsenal.

The problem is, some plugins are shockingly insecure. Windows
attracts many exploits because of its large user base, and the
same applies to WordPress plugins. Vulnerable plugins found on
WordPress sites can and will be exploited.
Plugins, whether for browsers or servers, need to be updated
regularly as they are vulnerable to security flaws, and out-ofdate versions should be avoided where possible.

In 2015, Symantec also saw the return of Team GhostShell,
which claims to have hacked a significant number of websites.
Earlier this year, the Symantec Security Response team reported:
“From first appearances, the recently released list of hacked
websites seems to be random and there is no indication that any
particular country or sector is being targeted. The group is more
than likely hacking websites based on their vulnerability.
In keeping with its previous modus operandi, it is likely that
the group compromised the databases by way of SQL injection
attacks and poorly configured PHP scripts.”
Again, these are hacks that most likely could have been
prevented with better website and server management. SQL
injection is a long-established attack method, which continues
to work because of an unnecessary weakness in the parameters
administrators establish for search queries.

It is difficult to defend against new and unknown vulnerabilities, particularly zero-day vulnerabilities for which there may
be no patch, and attackers are trying hard to exploit them faster
than vendors can roll out patches.
In 2015, following the breach of Hacking Team, an Italy-based
company, previously unknown zero-day exploits were made
public by the attackers. Exploits for zero-day vulnerabilities
were shared, and within hours, integrated into exploit toolkits.

Angler was the most active exploit kit in 2015, and hundreds
of thousands of attacks by this kit were blocked by Symantec
on a daily basis. In total, the number of Angler-based attacks
blocked numbered over 19.5 million. Angler’s favorite delivery
mechanism was malvertisments, favoring exploited Adobe Flash
vulnerabilities. Windows was the preferred target for Angler
in 2015. Windows 7 in particular accounted for 64 percent of
Angler attacks, and Windows 8.1 accounted for 24 percent.
Moreover, Mac OS X did not appear to be in the firing line for
attackers using the Angler toolkit in 2015, but this is expected
to change as cybercriminals seek to exploit the Apple ecosystem.

In 2015, Symantec recorded an increase in tech support scams,
equivalent to a 200 percent rise compared to the previous year.
Tech support scams are not a new tactic, and hundreds of
thousands of people worldwide are targeted on a daily basis.
The earliest types of tech support scams involved call center
workers cold-calling users, trying to sell them technical support
packages to resolve non-existent problems on their intended
victims’ computers.

These scams have evolved over time, and more recent examples
may display seemingly endless fake warning messages, urging
the intended victims to call a toll-free number for help. On
calling the number, seemingly professional-sounding call center
staff try to convince their intended victims to install malware
and other unwanted applications onto their computers, while
claiming it will fix their problems.
In the latest twist, tech support scammers were found using
the Nuclear exploit kit to drop ransomware onto its intended
victims’ computers. The scammers could distract the user while
the ransomware encrypts files on their computer, perhaps
increasing their chances of earning money from the victim.
While this wasn’t the first time tech support scammers have been
discovered installing ransomware, the most recent examples
include a malicious HTML iframe on their website, redirecting
visitors to a server hosting the Nuclear exploit kit. The exploit
kit was found to be taking advantage of the recent Adobe Flash
Player Unspecified Remote Code Execution Vulnerability (CVE2015-7645), among other vulnerabilities. On success, it either
dropped Trojan Cryptowall (ransomware) or Trojan Miuref B (an
information-stealing Trojan).
This was the first time Symantec has seen tech support scams
used in parallel with the Nuclear exploit kit to deliver ransomware, and if this proves to be an effective combination, this
trend is set to continue. While it may be quite plausible that
tech support scammers and exploit kit attackers have joined
forces, it is possible that the tech support scammers’ own web
servers were compromised by a separate group who are using
the Nuclear exploit kit.

The middle of 2015 was filled with accounts of malvertising
affecting almost every segment of the ad-supported Internet.
One possible explanation is that malvertising is simply an
easier way to infect site visitors than spamming out links to
infected websites. It’s much easier for an attacker to try and
compromise a popular site or seek to host malicious ads on
popular, high-traffic websites because it means they don’t need
to consider the complex nuancing of social engineering, eliminating one more step in the bad guys’ “pipeline.”
Ad companies often don’t request a lot of information from
people submitting ads, making it easy for criminals to masquerade as legitimate businesses and upload malicious ads, which
can appear on any number of sites.
Thanks to the use of cookies, malware authors can also tailor
their malicious code or redirects to target almost any subset of
users, by geography, time of day, company, interests, or recent
Internet activity.

Unfortunately, malvertising is notoriously difficult to track
and criminals have become increasingly clever, removing the
malicious code from their ads after an hour or two, making it
almost invisible. Since it is powerful, effective, and hard to
analyze, we expect the use of malvertising to continue to grow.
Consequently, an increased demand for ad-blockers may in turn
help to reduce the negative impact of malvertising.

Whether it’s the way we shop,
work, or pay our tax bill, trust and
confidence in online services has
become critical to our way of life.
Thankfully, changes are coming to
the way we use and secure the
Internet to reinforce trust in online
privacy, security, and transactions.

Website security encompasses more than the information in
transit between a server and visitors to a website. Organizations need to think about their websites as parts of an entire
ecosystem that needs constant care and attention if they want
to retain people’s trust and confidence.
The consequences of failing to bolster website security are likely
to extend beyond the costs to an individual company: it will
damage consumer confidence and the wider economic fallout
could be huge.

Put Your Money Where Your Mouse Is.
The scales finally tipped during the 2015 Thanksgiving holiday
weekend in the US, as the number of consumers shopping online
exceeded those shopping in store, according to the National
Retail Foundation.
E-commerce is big business, and Ecommerce Europe reported
that global business-to-consumer ecommerce turnover grew
by 24 percent, reaching $1.9 billion in 2014. However, that may
seem small compared to the $6.7 trillion that Frost & Sullivan
estimates the business-to-business e-commerce market will be
worth by 2020. Frost & Sullivan’s forecast includes all forms of
electronic commerce including using Internet and electronic
data interchange systems.
Even governments are becoming increasingly dependent on
digital services to keep their books balanced. The British govern-

ment, for example, recently revealed that it had saved £1.7
billion through digital and technology transformation in 2014.
While SSL/TLS certificates, trust marks, and good website
security all help maintain the online economy, all this economic
activity could be at risk if people lose trust and confidence in the
security foundations of the online economy.

Websites Are Still Vulnerable to Attacks.

Websites are a critical element in major attacks: they are a way
into the network, they are a way into sensitive data, and they are
a way to reach customers and partners.
For example, the rise in malware aimed at Linux web servers―
including website hosts―proves that criminals have realized
that the infrastructure behind websites is as valuable, if not
more so, than the information encrypted by SSL/TLS certificates.
Many attacks against website infrastructure could be prevented
with regular maintenance and patching, but the numbers
suggest that website owners just aren’t managing to keep up.
Three quarters of the websites Symantec scanned in 2015 had
vulnerabilities―a number that hasn’t shifted in years.
Cybercriminals continued to find vulnerabilities in the underlying infrastructure of website security in 2015, including FREAK,
which allowed attackers intercepting a secure connection to
force the server to downgrade to encryption an easier-to-crack
protocol.
Distributed-denial-of-service (DDoS) attacks have also
continued to prove disruptive to businesses 2015. While largescale attacks such as the one that hit the BBC at the end of 2015
tend to grab headlines, businesses of every size are a target for
attack and often smaller sites can suffer as part of the collateral
damage when a host has to shut down a server, taking multiple
sites offline, because of an attack on just one of its clients.
Mitigation tactics and tools exist to defend against DDoS attacks,
but website managers need to take the time to understand and
deploy them if they are to keep their websites safe.

It’s not all bad news. There have been several advances in both
the strength and adoption of SSL/TLS certificates in 2015
as well as initiatives by Certificate Authorities (CAs) to make
issuing SSL/TLS certificates more transparent.
Crucially, nearly 40 percent of all downstream Internet traffic in
the US is now encrypted, according to research from Sandvine,
and this is expected to grow to more than 70 percent of the
world’s Internet traffic over the coming year.
Unfortunately, in a world where everything is encrypted,
consumers have a false sense of security that whenever they see
HTTPS in the browser, the website that they are on has been validated and authenticated and must therefore be genuine.
In reality, online fraud has historically occurred on Domain
Validated (DV) sites, which offer no validation of the organization behind the site.
With DV certificates, the CA will verify that a contact at the
domain in question approves the certificate request, usually via
email or telephone, and this is often automated. Consequently, DV certificates are usually cheaper than the more rigorous
Extended Validation (EV) SSL certificates, which require more
vetting and validation.
While DV certificates verify the consent of a domain owner,
they make no attempt to verify who the domain owner really is,
making it ideal for both phishing and MITM (man-in-the-middle) attacks. Symantec expects to see a move by organizations,
particularly those driven by PCI compliance, to strengthen the
requirements for stronger authentication, and the adoption of
EV SSL certificates providing greater levels of assurance.
Encryption of SSL/TLS will also become stronger with the shift
from SHA-1 to SHA-2. Historically, SHA1 is a very popular
one-way hashing function, where each hash generated from a
source is intended to be unique. There should be no “collision”
where two different sources will generate the same hash;
however, the first weaknesses were identified as early as 2005.
This came to a head in 2014 when Google announced it would
soon no longer support sites using SHA1 and will display
security warnings to visitors trying to access sites with SHA-1
certificates expiring after 1st January 2017. Several other
browser vendors followed suit, spelling the inevitable end for
SHA-1.
The security community is making great progress, and there is a
real opportunity to significantly reduce the number of successful website attacks, but it will only happen if website owners
step up and take action too.

Nearly 40 percent of all downstream Internet traffic in the US is
now encrypted, according to research from Sandvine, and this is
expected to grow to more than 70 percent of the world’s Internet
traffic over the year. Google announced in 2014 that
the adoption of ‘HTTPS everywhere’ would have a positive
impact on search rankings, encouraging site owners to adopt
it to get an edge in search engine rankings.
The Internet Engineering Task Force (IETF),
the organization in charge of creating standards for the
Internet, published a new version of the Hypertext Transfer
Protocol in 2015. Dubbed HTTP/2, it will likely be adopted as
standard in the near future and, as the draft states, HTTP/2
enables a “more efficient use of network resources,” meaning
HTTP/2 is designed to deliver better, faster responsive
performance for websites out of the box. And every major
browser has said its support for HTTP/2 is only going to be
over SSL/TLS. In effect, this makes encryption mandatory
for sites using this new standard.

The hope is that within the next few years, every page on the
Internet will have an SSL/TLS certificate. Symantec is already
working with web hosting providers to help them provide
encryption as part of their service to website owners.

Several major browsers are also changing their security indicators―the colours and symbols used in the address bar to
indicate to visitors how safe a site is―to make it clear when an
SSL/TLS-secured web page includes unsecured content that is
vulnerable to man-in-the-middle tampering. In other words,
this will make it clearer when a site fails to achieve always-on
encryption and the danger this poses.
This is just one example of the drive to offer added reassurance
to websites visitors and online shoppers, which also includes
trust marks and shopping guarantees, which help to allay the
fears many shoppers have when they shop online and can’t see
the store owner in person or hold the goods they’re buying in
their hands.
Organizations need to be more proactive around SSL/TLS implementation. It’s not a one-and-done task. Tools that automate
and streamline the process are essential.
Updates are released regularly for SSL/TLS protocol libraries,
such as OpenSSL, to protect against such vulnerabilities, but
website owners still have to install them. The move from SHA-1
certificates to the much stronger SHA-2 is also accelerating, but
again organizations have to deploy the new certificates properly
for the change to be effective.
Rather than thinking solely about protection, website managers
need to think about protection, detection, and response. They
need to use automation tools to monitor their websites continually for signs of vulnerability or attack, block those attacks, and
then report, update, and patch accordingly.

SSL/TLS remains at the heart of
online privacy, authentication, and
encryption, but around them is an
infrastructure of trust that requires
maintenance and vigilance if it is to
remain effective. The industry must
learn and adapt.
On August 11, 1994, Daniel Kohn sold a CD to a friend in Philadelphia. His friend used his credit card to spend $12, plus
shipping costs, in a transaction that, for the first time ever,
was protected by encryption technology. The site Daniel ran at
the time required customers to download a special browser to
conduct secure transactions, which employed the PGP encryption standard that his website relied on.
Reporting the next day, the New York Times commented:
“Alarmed by increasing reports of security breaches on the
Internet, many people and businesses are reluctant to transmit
sensitive information, including credit cards numbers, sales
information, or private electronic mail messages, on the
network.”
Twenty years later, people’s concerns remain the same, although
their behaviour suggests they’re willing to take the risk of
relying on their bank for help if something goes wrong. Without

a consistent and secure SSL/TLS infrastructure, however, this
fragile state of trust will crumble and ecommerce simply won’t
be able to function.

The strength of SSL/TLS has come a long way since 1994, and
this year saw the switch from SHA-1 to SHA-2 as the industry
standard moving forward.
As computing power has increased, so has a hacker’s ability
to break hashing algorithms through sheer brute force. Many
experts predict that SHA-1 will become vulnerable in the very
near future. That’s why the major browsers have agreed to stop
supporting SHA-1 certificates during the next two years so that
any visitors trying to access a site continuing to use them will
see a security warning.
“The current plan is to [stop accepting SHA-1 certificates] on
January 1, 2017. However, in light of recent attacks on SHA-1,
we are also considering the feasibility of having a cut-off date as
early as July 1, 2016,” says Mozilla, and there has been discussion of bringing those dates even further forward to accelerate
the change.
Symantec offers a free upgrade service, but large organizations
need to ensure they have a full migration plan in place to update
any devices and applications that may not currently recognize
SHA-2.

The vulnerability known as FREAK was discovered back in

March 2015. Attackers who intercepted the setting up of a secure
connection between an affected server and client could force them
to use ‘export-grade’ encryption, a much weaker form of encryption
than is usually used today, therefore making the transacted
message easy to break with the computing resources available
today.

It’s estimated that servers supporting 9.6 percent of the top one

million website domains were initially vulnerable to attack and nine
months later, 8.5 percent remain so.

Despite encryption getting stronger, many of the attacks aimed
at SSL/TLS this year have focused on weaknesses in the wider
SSL/TLS ecosystem.
Symantec has seen a much greater focus in the last year on the
code libraries related to SSL/TLS implementations, and as a
result, we have seen a regular stream of vulnerability updates
and fixes.
That’s the good news. But the most common unpatched vulnerabilities on web servers in the last year reveal that website
owners aren’t keeping up with the releases. It’s vital that website
managers maintain the integrity of their SSL/TLS implementations. It’s not a fit-and-forget task.

Although we didn’t see any vulnerabilities as potentially
dangerous as 2014’s Heartbleed, OpenSSL released several
updates and patches throughout 2015. OpenSSL is one of the
most widely-used implementations of the SSL and TLS cryptographic protocols and is used on two-thirds of all web servers.
The updates it released were for vulnerabilities that ranged
from low risk to high severity and which could allow attackers
to carry out man-in-the-middle attacks to eavesdrop on secure
communications or to launch denial-of-service attacks.


In order to strengthen the SSL/TLS ecosystem, Symantec
has pushed for the widespread adoption of DNS Certification
Authority Authorization (CAA). This allows an organization, or
DNS owner, to specify which certificate authority (CA) it will buy
SSL/TLS certificates from. If a malicious actor, or an employee
who doesn’t know company policy, tries to purchase a certificate
from a CA not on the approved list, that CA can check the CAA
and alert the DNS owner of the request.
This reduces the risk of rogue certificates being issued in a legitimate organization’s name without its knowledge, which in turn
would reduce the risk of criminals being able to set up certified
phishing sites.
In an effort to better spot rogue certificates, Symantec is also
complying with Google’s request to log all EV certificates we
issue on its Certificate Transparency log. As of March 2016,
Symantec is also logging OV and DV certificates. Along with
software that can monitor and audit certificates and their use,
this creates, as its authors say, “an open framework that lets
anyone observe and verify newly issued and existing SSL certificates in nearly real time."

Trust Services, Electronic Identification (eID),
and Electronic Trust Services (eTS)
In September 2015, the European Commission completed the
adoption of all the implementing acts required for adoption
of the new eIDAS Regulation. This regulation marks a major
change in the regulatory environment to enable secure and
seamless electronic interactions between businesses, citizens,
and public authorities across Europe.
Moreover, it is also an important step forward in promoting
greater security requirements for Certificate Authorities (CAs)
with the implementation of an EU Trust Mark for Qualified
Trust Services. The new trust mark will help in clearly differentiating qualified trust services from others in the market,
fostering greater transparency and confidence in such essential
online services.
The sophistication and ruthlessness
of some of the attacks and tactics
used by cybercriminals in 2015
have demonstrated how vulnerable
individuals are online and chipped
away at public confidence in online
security. Data breaches, government
surveillance, and good old-fashioned
scams came together to further
encroach on personal privacy,
whether it is personal photos, login
credential or medical histories.
Personal data is anything but private.
In 2015, Symantec saw plenty of traditional scams and
malware attacks intended to gather personal information. For
example, one scam promised large numbers of followers for
free on Instagram, while seeking to fool people into revealing
their passwords. Some attacks impersonated tax officials in
an attempt to trick people into downloading malicious email
attachments.
In their simplest form, many scams still rely on the poor security
habits of the general public to succeed. However, we have also
seen how poor website security can expose customer data. In the
latter example, it doesn’t matter how strong a password may be
if the website is vulnerable to a data breach.
More concerning are attacks in 2015 that made use of sophisticated social engineering to bypass the two-factor authentication
systems designed to safeguard users.
By going through a legitimate password-reset process and
posing as Google via SMS, however, one scam was able exploit
the public’s trust in a reputable brand to gain access to email
accounts without raising the victims’ suspicions.

The attacker can then
reset the password and
once they have what
they want or have set up
forwarding, can inform
the victim—again posing
as Google—of their new
temporary password,
leaving the victim none
the wiser.
The victim therefore expects the
password-reset verification code
that Google sends out and
passes it on to the attacker.

Online ‘sextortion’ has been around for years, and more
recent examples, particularly prevalent in Asia, have turned
to malicious Android apps. These scammers, using an attractive avatar or profile picture, encourage the intended victim to
share sexually-explicit videos. The criminals then encourage
the victim to “continue the liaison” using an Android app, which
also gathers the victim’s phone number, account details, and all
of their contacts.

Now with an incriminating video, and a list of the victim’s
friends and family, the gang threatens to send the sexually
explicit content to the victim’s entire contact list unless they
pay up. Because of the sensitive nature of the threat, victims
often find it difficult to go to the authorities and end up sending
hundreds, if not thousands, of dollars to the attacker.
In the wake of the Ashley Madison attack, a spike in spam
messages with subject lines like “How to Check if You Were
Exposed in Ashley Madison Hack” or “Ashley Madison hacked,
is your spouse cheating?” were reported. The hack was perhaps
more unusual in that its ramifications went well beyond the
financial sphere to affect people’s personal relationships and
reputations.

Each tweet from a Mockingbird account received nearly 1,000
retweets and 500 favorites, which were not genuine, as they originated from a secondary account, which we called the Parrot. In
turn, Parrot accounts, follow anyone and everyone in the hope
that genuine Twitter users will follow them back, a remarkably
effective tactic.
If these Parrot accounts only retweeted spam from the Mockingbird accounts, they would quickly be spotted, which is why
they also posted other tweets too, typically copying tweets and
retweeting memes from genuine Twitter users.
On the other hand, the majority of Egg accounts never composed
a single tweet. Instead, they would simply be used to bolster the
number of followers of the Parrot accounts in the hundreds.
This complex operation centered on weight-loss spam. The
operators went to great lengths to avoid anti-spam measures
and were able to operate for a long time.

Social media remains a favored target
of scammers, as criminals seek to
leverage the trust people have in their
own social circles to spread scams,
fake links, and phishing. To succeed,
the social engineering involved must
be convincing, and so we see more
progressive and ingenious tactics to
dupe potential victims.
One scam in particular went to great
lengths to create an entire family
tree of hundreds of thousands of fake
Twitter accounts, each branch boosting
the credibility of the one above, to gain
followers, and retweets from genuine
Twitter users. At the top of the family
tree were accounts impersonating news
outlets and celebrities, even curating
real tweets from the genuine accounts
to make them seem more credible.
Through the discovery of these
imposter accounts, we identified three
account types that were being used:

Social networking scams require some form of interaction,
and manual sharing remained the main route for social media
attacks in 2015, expanding on the technique that had snowballed in the previous year.

Similar localized attacks around the world show that cybercriminals are putting in the effort to manipulate victims no
matter the location or the language. Adapting phishing scams
using phishing toolkits makes it extremely easy to conduct a
campaign against a target in one country, change the templates,
and quickly target another elsewhere. Often the language used
in such localized attacks has been automatically translated
through the templates and may appear convincing to a non-native speaker.
Joining
often requires the user to share credentials with the attacker or send
a text to a premium rate number.
Likejacking – Using fake “Like” buttons, attackers trick users into

clicking website buttons that install malware and may post updates
on a user’s newsfeed, spreading the attack.

Fake Apps – Users are invited to subscribe to an application that

appears to be integrated for use with a social network, but is not as
described, and may be used to steal credentials or harvest other
personal data.

Fake Plugin – Users are invited to install a plugin to view a video, but

the plugin is malicious and may spread by re-posting the fake video
message to a victim’s profile page without permission. Examples
include installing a fake YouTube premium browser extension to
view the video, or noticing that a DivX plugin is required, and the fake
plugin masquerades as such.

Other forms of attack seen in 2015 also prove just how sophisticated and ruthless criminals are willing to be to make a profit.
Wherever you live or whatever language you speak, you could
still be under threat from cyber attackers. Take Boleto, a payment
system used in Brazil for example. Boleto may be considered a

Cybercrime costs the global economy up to US $575 billion
annually according to BofA Merrill Lynch Global Research,
whose report goes on to say that in a potential worst-case 2020
‘Cybergeddon’ scenario, cybercrime could extract up to a fifth of
the value created by the Internet.
It is everyone’s responsibility to do all they can to prevent that
from happening.
For consumers, it’s time kick bad habits. Many people know
the basics of good cybersecurity, yet people continue to share
their passwords. In fact more than a third of people who share
passwords in the United States have shared the password to
their online banking account. People need to start taking more
responsibility for shoring up their online security.
Users should more wary of who they follow on social media. Bots
can appear more and more like a real person, and are sometimes
difficult to spot. Be skeptical of new followers. If a random person follows
you, do not automatically follow them back. Look at their
tweets. Are they retweeting content that looks like spam? If
they are, they are most likely a bot.

Even if these random followers have tens
of thousands of followers, those numbers can easily be faked.
Do not base your decision to follow them back based on how
many people follow them.

Twitter users should always
check to see if a well-known brand or famous celebrity has
been verified by Twitter before following. The blue verified
badge denotes that Twitter has authenticated the true owner
of an account.

Taking risks with cybersecurity is not acceptable, and we should
reject the misconception that privacy no longer exists. Privacy is
something precious, and should be protected carefully.
For businesses, this means approaching security in terms of
education, cybersecurity awareness training, and good digital
hygiene. Every employee should be part of the effort to stay
digitally healthy. CIOs and IT managers need to be aware of
just how many risks they face and start proactively monitoring
for symptoms so that they can diagnose digital diseases before
putting customer data and customer confidence at risk.

IT systems continue to come
under attack from rapidly evolving
malware. Email remains the medium
of choice for cybercriminals and
email volumes continue to grow, as
phishing and spam decline—the latter
of which accounted for more than
half of inbound email traffic. Phishing
attacks were more targeted and
malicious emails grew in number
and complexity, highlighting how
email remains an effective medium
for cybercriminals.

Email continues to dominate digital communications, regardless of the rising popularity of instant messaging technology
for both business and consumer use. Symantec estimates there
were approximately 190 billion emails in circulation each day in
2015, a number that we predict to grow by as much as 4 percent
by the end of 2016. On average, each business user sent and
received 42 emails each day, and a growing number of individuals were reading email on mobile devices. For cybercriminals
who want to reach the largest number of people electronically,
email is still the favored way to do it.
No wonder it is still widely used by Internet criminals for spam,
phishing, and email malware. In 2015, Symantec saw email
threats decline. Email-based attacks from phishing and malware
are categorized as spam, and accounted for approximately one
percent of all spam email. Symantec provides further analysis of
spam classified as malware and phishing, as these threats have
potentially significant, harmful consequences.
Symantec scans a significant proportion of the global business
email traffic, giving us a unique insight into this medium and
the security threats it poses. Many business emails will never
be sent outside of an organization, with approximately three
quarters of external business email traffic being inbound, more
than half of which was spam.

More than half of inbound business email traffic was spam in
2015, despite a gradual decline over recent years. In 2015, spam
reached its lowest level since 2003. However, the spam problem
is not going away. Spammers are finding other ways to reach
their audiences, including the use of social networking and
instant messaging, two of the most popular types of applications found on mobile devices. In exploiting them in addition to
email, spammers continually seek to evolve their tactics.
In addition, Symantec has observed an increase in what
is commonly known as “snowshoe spam.” As an analogy,
snowshoes are designed to spread the wearer’s weight across a
wide area, and snowshoe spamming distributes large volumes of
spam across a wide range of IP addresses. As the name implies,
this technique seeks to circumvent anti-spam technology, such
as propagation latency and IP address reputation, by sending
large volumes of spam messages in very short bursts. By also
quickly rotating domains and recirculating IP addresses, this
can make them more difficult to block quickly.

As with phishing fraud, malware distributed in emails requires
social engineering to convince its recipient to open the attachment or to click on a link. Attachments can be disguised as fake
invoices, office documents, or other files, and often exploits an
unpatched vulnerability in the software application used to open
that type of file. Malicious links may direct the user to a compromised website using a web attack toolkit to drop something
malicious onto their computer.
Threats like Dridex exclusively use spam email campaigns, and
incorporate real company names in the sender address and in
the email body. The vast majority of Dridex spam masquerades
as financial emails, such as invoices, receipts, and orders. The
emails include malicious Word or Excel attachments with a
payload that drops the actual malware designed to target online
banking information.
The cybercriminal group behind this particular attack has used
many different techniques for sending spam and malware: from
simple malware attachments, hyperlinks in the message body
that point to an exploit kit landing page, malicious PDF attachments, and document macros.
Email malware has not been in decline in the same way as
general spam, and because of its relatively low volume in
comparison, it is more subject to fluctuation. Spikes occur when
large campaigns are undertaken.

Email encryption is valuable because it protects the privacy
of messages and can help to authenticate senders. It is under
threat because of vulnerabilities in the underlying technology
(see above) but also because it is not widely used.
Although webmail systems such as Microsoft’s Outlook.com
and Google Mail use encryption on the clients, and almost all
email systems prioritize encrypted transmission, a surprising proportion of email is sent in the clear using unencrypted
SMTP transfers. Google reports that in 2015, around 57 percent
of inbound emails were encrypted compared with 51 percent
the year before. The number of outbound encrypted emails
rose from 65 percent to 80 percent in the same period. It isn’t
unusual for some spam to be sent using encryption. As long ago
as 2010, the Rustock botnet used TLS encryption as a means to
disguise the spam it was sending.
Good desktop and gateway email encryption tools do exist,
including Symantec’s own, but companies need to make better
use of the technology available to them to protect email in
transit and at rest.

Organizations and individuals need to realize that even if they
do not think they are an obvious target for cybercriminals, it
does not mean they are immune.

With a continual three-year decline, we expect phishing attacks
to remain at least at current levels, if not decline further.
Phishing attacks have become more targeted, and less scattergun. Many attacks have shifted towards social media, adding to
the decline in email numbers. Some parts of the world suffer
more from email phishing attacks than others―with the greatest
decline in many English-speaking countries, North America and
parts of Western Europe.

People will continue to do more and more online, and because
Internet access and online transactions are growing in popularity among developing countries, we may even see growth
in phishing attacks in these areas. For example, paying utility
bills, booking doctors’ appointments, applying to a university,
managing frequent flyer accounts, and taking out insurance all
provide fruitful inspiration for phishing attacks.
As organizations deliver more services online they need to be
mindful of the need for security, and they have to work with
customers to educate them further and build trust. In addition,
they may need to consider two-factor authentication to ensure
customer confidence and reduce the cost of phishing fraud.
As we have noted, cybercriminals are increasingly moving
towards more complex email threats, where malware authors,
ransomware creators, phishers, and scammers will seek to
exploit what they perceive to be the weakest link in the chain―
humans. Social engineering, or “head hacking,” is a vital
ingredient to any would-be attacker that is trying to gain access
to systems that hold potentially valuable information. 

Widespread, persistent, and
sophisticated attacks against
government organizations and
businesses of all sizes pose greater
risks to national security and the
economy. The number of zero-day
vulnerabilities grew, and evidence
of them being weaponized for
use in cyberattacks was revealed.
Spear-phishing campaigns became
stealthier, targeting fewer individuals
within a smaller number of select
organizations.
In February 2015, 78 million patient records were exposed in
a major data breach at Anthem, the second largest healthcare
provider in the US. Symantec traced the attack to a well-funded attack group, named Black Vine, that has associations with
a China-based IT security organization, called Topsec. Black
Vine is responsible for carrying out cyberespionage campaigns
against multiple industries, including energy and aerospace,
using advanced, custom-developed malware.
Other high-profile targets of cyberespionage in 2015 included
the White House, the Pentagon, the German Bundestag, and the
US Government’s Office of Personnel Management, which lost
21.5 million personnel files, including sensitive information
such as health and financial history, arrest records, and even
fingerprint data.
These attacks are part of a rising tide of sophisticated, well-resourced, and persistent cyberespionage attacks around the
world. Targets include state secrets, intellectual property such
as designs, patents, and plans, and as evidenced by recent data
breaches, personal information.
Symantec’s continuing investigation into the Regin Trojan
gives us a further glimpse into the technical capabilities of
state-sponsored attackers. It revealed 49 new modules, each
of which adds new capabilities, such keylogging, email and file

access, and an extensive command-and-control infrastructure.
Symantec analysts commented that the level of sophistication
and complexity of Regin suggests that the development of this
threat could have taken well-resourced teams of developers
many months or years to develop.
Currently, spear-phishing and watering-hole attacks that
exploit compromised websites are the favored avenues for
targeted attacks. However, as additional layers of technology
are introduced to an organization, its attack surface expands.
With businesses turning more to cloud technology and the prevalence of IoT devices, we expect to see targeted attacks seeking
to exploit vulnerabilities in these systems within the next year
or two. Cloud services particularly vulnerable to exploits, such
as SQL injection flaws, will likely be targeted first. Spear-phishing campaigns exploiting misconfiguration and poor security by
users, rather than cloud service providers, will bear low-hanging
fruit for the attackers.
In order to remain below the radar, spear-phishing campaigns
have increased in number, but have become smaller with fewer
individuals targeted in each campaign. We expect spear-phishing campaigns will soon consist of just a single target, or a few
select individuals at the same organization. Moreover, the larger
spear-phishing campaigns will likely all be conducted using
web-based watering hole attacks, with compromised websites
exploiting highly-coveted zero-day vulnerabilities.

Zero-day vulnerabilities are particularly valuable to attackers.
Indeed, because zero-day vulnerabilities are such a seemingly
rare commodity, attackers will closely guard their exploits so
that they may be used for longer and remain undetected.
Sophisticated watering-hole attacks, using compromised
websites, activate only when a visitor to that website originates
from a particular IP address. Reducing collateral damage in this
way makes it less likely that the covert attack is discovered.
Moreover, this approach also makes it more difficult for security
researchers who may visit the website from a different location.
Once an exploit is disclosed publically by the relevant vendor,
these watering-hole sites will often switch to using another
unpublished exploit for a different zero-day vulnerability in
order to remain hidden.
The breach of Hacking Team in 2015 stood out because the
attackers weren’t after money or identities; they were after
cyberweapons, such as zero-day exploits. Hacking Team is
an Italian outfit that specializes in covert surveillance and
espionage software marketed at government users. Previously
unknown zero-day exploits were uncovered in the attack and
made public by the attackers. Details of weaponized zero-day
vulnerabilities and numerous Trojans used by the group were
shared within days on public forums, and within hours, exploit
kit authors had integrated them into their exploit toolkits.

There was an unprecedented 54 zero-day vulnerabilities found
throughout 2015, more than doubling the number found in the
previous year. Discovering unknown vulnerabilities and figuring
out how to exploit them has clearly become a go-to technique for
advanced attackers, and there is no sign of this trend changing.

Most of the zero days seen in 2015 target old, “faithful” technologies that have been targeted for years. Attackers racked up
10 individual zero-day vulnerabilities against Adobe’s Flash
Player during the year. Microsoft received equal attention from
malicious zero-day developers, though the 10 zero day vulnerabilities found targeting their software was distributed across
Microsoft Windows (6x), Internet Explorer (2x), and Microsoft
Office (2x). The Android operating system was also targeted
through four zero-day vulnerabilities during 2015.

What is concerning, though not surprising, is that there were
11 zero-day vulnerabilities that were used to exploit open
source software. Some exploits targeted common libraries and
packages, while others went after open source web development tools, like content management systems and e-commerce
platforms. Networking protocols were also highly targeted, with
continued attacks against OpenSSL, as well as Samba.
However, what should give most people cause for concern is
that attackers appear to be discovering and exploiting zero-day
vulnerabilities in industrial control systems (ICSs)—devices
used to control things ranging from industrial manufacturing to
power plants. There were seven known zero-day vulnerabilities
during 2015 targeting a variety of different manufacturers and
different devices.

The motivations behind such attacks are not clear, and could
range from geopolitical disputes to ransom-related attacks.
Regardless, if not monitored carefully, such attacks could have
serious consequences in the future, and it doesn’t look likely to
go away anytime soon.

In the case of CVE-2015-5119, Symantec already had signatures
that were able to detect exploits four days before the vulnerability was publically disclosed. Sometimes, existing signatures can
be successful in blocking attacks exploiting new vulnerabilities,
and signatures are frequently updated to block more attacks
even where protection exists beforehand. Additionally, this
vulnerability was among those exposed in the breach against
Hacking Team.

It’s not only websites that may contain hidden exploits. A previously-unknown vulnerability may be exploited to attack an
organization using an infected document attached in an email.
Such an attack is known as spear phishing, and relies heavily on
very good social engineering in order to dress-up the email to
appear convincing.
Spear-phishing emails are sent in waves, or campaigns, to a
very small group of people, often not all at once, but individually or where more than one person in an organization may be
targeted. Over time, different exploits may be used against the
same people, should these attacks prove ineffective. However, in
recent years attackers quickly switch tactics after a few failed
attempts in order to remain undetected. In previous years,
they were more likely to continue with different exploits or by
targeting different individuals within the organization.

Typically, such an organization may expect to be targeted at least
four times during the year. The attackers only have to succeed
once, whereas the businesses must thwart each and every attack
to remain secure. Businesses should already be thinking about
what to do when (not if) such a breach occurs.

Spear-phishing attacks are less likely to arouse suspicion with
campaigns that are smaller, shorter, and target fewer recipients. A few years ago, a targeted attack campaign may have been
directed to a hundred or more individuals, any one of whom may
become suspicious and raise the alarm. With fewer people, this
probability is greatly reduced.
In 2015, the Finance sector was the most targeted, with 34.9
percent of all spear-phishing email directed at an organization
in that industry, 15 percentage points higher than the previous
year. The likelihood of an organization in this sector being
targeted at least once in the year was 8.7 percent (approximately 1 in 11).
Perhaps surprisingly, executable file types are still popular, however,
accounting for at least 36 percent of the spear-phishing attachments in
2015. In non-targeted email malware, executable file attachment accounted
for approximately 1.3 percent of malicious attachments.

Butterfly is a group of extremely well-organized, highly-capable
hackers who are spying on companies with a view to profiting on
the stock market by selling market-sensitive information to the
highest-bidder. The types of information the attackers potentially had access to included emails, legal documents, policy
documents, training materials, product descriptions, and data
harvested from specialist security systems. Stolen materials
such as these could also be valuable for insider-trading purposes.
Symantec first saw these attacks in 2012 and 2013 when they
compromised some well-known companies including Apple,
Microsoft, and Facebook. However, they also employ sophisticated counter-measures to cover their tracks, including encrypted
virtual command and control servers.

A Black Swan event is an event that was unprecedented and
unexpected at the time it occurred; however, after further
analysis, experts sometimes conclude that it could have been
predicted. The term originates from the belief that all swans were
white, until in 1697, black swans were discovered in Australia.
If advanced cyberespionage is so common, it is perhaps curious
that cybersabotage is not. The capabilities required to inflict
physical damage are similar to those needed for cyberespionage,
and the target set is growing thanks to the proliferation of Internet-connected devices, including industrial control systems.
The British Government’s 2015 security and defense review
sums up the challenges neatly:
“The range of cyber actors threatening the UK has
grown. The threat is increasingly asymmetric and global.
Reliable, consistent cyber defense typically requires
advanced skills and substantial investment. But growing
numbers of states, with state-level resources, are developing advanced capabilities which are potentially
deployable in conflicts, including against CNI [Critical
National Infrastructure], and government institutions.
And non-state actors, including terrorists and cyber
criminals can use easily available cyber tools and technology for destructive purposes.”
The Stuxnet cyberattack on the Iranian nuclear program is the
best-known example of an Internet attack on physical infrastructure. It may be that other successful attacks have occurred
in the shadows or that infections are in place, but haven’t been
activated yet. It seems unlikely that the world’s critical infrastructure is immune. An attack at the end of 2014 on a German
steel mill is a warning of potentially more serious attacks to
come.
Speculations about possible cybersabotage continued into 2015
with the discovery of an information-stealing threat named
Trojan.Laziok. This particular threat appears to have been
designed for reconnaissance style attacks aimed at the energy
sector, particularly in the Middle East. Laziok wasn’t implicitly
designed to attack and bring down critical infrastructure, but
rather to gather information about the systems it compromised.
As we discussed in ISTR 20, these attacks can be just as potent as
direct attacks against critical systems, improving an attacker’s
ability to press further into an environment simply by learning
more about the types of systems they are traversing. Simply
put, if an attacker knows what types of computers he or she has
or can compromise, they can decide how to proceed in order to
carry out their malicious goals.

The notion of hybrid threats has been around for a long time in
cybersecurity, traditionally referring to malware that has many
different attack vectors―such as dropping malicious Trojan code
onto an infected device and infecting other code on the system,
while spreading itself through email or some other means. The
term “hybrid warfare,” however refers to a type of warfare that
is a combination of conventional and unconventional information and cyber warfare. According to NATO, “the term appeared
at least as early as 2005 and was subsequently used to describe
the strategy used by the Hezbollah in the 2006 Lebanon War.”
It wasn’t until the end of 2015 where speculations about cybersabotage turned into real indications of one such attack. On
December 23, a power failure hit the Ivano-Frankivisk region
in western Ukraine. Details emerged over the coming days and
weeks of a multi-pronged cyber attack that not only disabled
power in eight provinces in the region, but also masked the
activity of the attackers and made it difficult to assess the extent
of the outage.
The malware behind the attack appears to be a potent combination of the BlackEnergy Trojan (Backdoor.Lancafdo) and
Trojan.Disakil. In order to carry out the attack, the BlackEnergy
Trojan was most likely used to traverse the network, allowing
the attackers to gather information about the computers they
compromised until they reached the critical systems that
allowed them to disconnect breakers, resulting in the loss of
electricity in the region. However, it doesn’t appear as though
the Trojan itself disconnected the power. Rather, it allowed the
attackers to discover the critical systems and then gain full
control of them, after which they could use the original software
on these systems to take down the power grid.
While noteworthy to this point, the attackers responsible appear
to have planned the attack to such an extent that they were able
to prolong the outage beyond the point it was pinpointed as an
actual cyberattack. One way they were able to do this was by
performing a telephone denial-of-service (TDoS) attack against
the power supplier’s call center, preventing customers from
calling in, and leaving operators in the dark as to the extent of
the outage.
However, the one-two punch in the attack appears to be tied
to the use of Trojan.Disakil in the attack. A highly destructive
Trojan, Disakil was likely used to overwrite system files and wipe
master boot records on computers that operators would turn to
in order to bring the power back online. So not only was the
power taken down, so too were the systems used to restore it,
forcing operators to manually restore power in circumstances
they normally would be able to do so through available software.
As with any cyber attack, attribution can be difficult to determine.
Based on circumstantial evidence and current geopolitical
disputes, it is fairly easy to draw conclusions; however, there is

no smoking gun in this case. What is known is that the group
behind the BlackEnergy Trojan has been active for many years
and has targeted multiple organizations in the Ukraine, as well
as Western European countries, NATO, and others. Around the
time of these attacks, this group was also discovered attacking
media organizations in the Ukraine. It is likely this won’t be the
last we hear of them.
The cybersabotage attacks in Ukraine generated much debate
about the use and effectiveness of hybrid warfare, and it is likely
this won’t be the last we hear of these types of attacks, particularly as international tensions remain high in some parts of the
world, and managing the risks from cyberterrorism moves up
the agenda for many national governments.

Of course, small businesses have smaller IT budgets, and consequently spend less on cybersecurity than their large enterprise
counterparts. However, this trend has continued for years,
in spite evidence that shows a greater proportion of targeted
spear-phishing attacks each year are intended for small businesses.
In 2015, 43 percent of targeted spear-phishing blocked by
Symantec were destined for small businesses, compared with
34 percent in 2014. Additionally, the attackers focus narrowed,
concentrating on fewer companies, and approximately 3 percent
of small businesses were targeted in 2015, compared with 45
percent in the previous year. On average, these organizations
were targeted at least twice during the year. This shift from a
scattergun approach of more widely dispersed attacks in 2014,
to a more sniper-style line of attack converging on fewer targets
in 2015 also helps to keep these attacks below the radar.
One of the most difficult challenges is knowing when your organization is in the sights of cyber attackers, particularly when
most cybersecurity headlines focus on nation states vying for
company secrets, and the tens of millions of credit card details
and other personal data exposed in breaches. It’s all too easy to
believe that a targeted attack only happens to other companies.
However, no business is too small or too obscure to become a
target and one good example that shows this is the Dirty Linen
Attack.
Perhaps an unlikely target, General Linens Service, Inc. is a
very small company, with only one location and 35 employees.
They provide a linen service to restaurants and the hospitality
industry, including uniforms and carpet cleaning. As unlikely a
targeted as it would seem for a nation state, it was a competitor, General Linen Services, LLC. that had been hidden in their
network for two years. Perhaps the similar choice of company
name was deliberate, because for two years they were able to
steal customers by accessing the targeted company’s invoices,
allowing them to see how much they were charging, giving them
a significant advantage. The question was how they achieved
this; a small business conducting cyberattacks on a rival seemed
extreme. However, it transpired that the attackers noticed that
both companies used the same software for their web portal,
and the targeted company had not changed the default administration password. This enabled the attackers to access their
data 157 times. The good news is that General Linen Services,
LLC was caught and convicted, and General Linens Service, Inc.
discovered the importance of following security best practices.

Industrial control systems (ICSs) are found in many areas of
industrial production and utility services worldwide, and are
routinely connected to the Internet for remote monitoring and
control. Uncovering vulnerabilities in these systems is a major
area of research, emphasized by the growth in the numbers of
these vulnerabilities in 2015.
The actual number of vulnerabilities affecting ICSs is estimated
to be much higher, since many organizations standardize their
platforms by using commercial off-the-shelf (COTS) products,
such as Windows or Linux that are also subject to vulnerabilities,
but which are not counted here. Furthermore, ICS management
systems connected with enterprise networks can increase the
potential exposure to threats more typically associated with
these operating systems.

The most valuable form of protection against cyberespionage is
simply to be aware that it is possible. All businesses are potentially vulnerable to targeted attacks using techniques such
as watering hole attacks and spear phishing. Small size and
obscurity are no protection.
Indeed, in 2015 small businesses accounted for a greater proportion (43 percent) of spear-phishing attacks, but the likelihood
of being targeted diminished. While more attacks were destined
for that group, they were focused on a smaller, more discreet
number of businesses (3 percent).
Contrast this with large enterprises, which accounted for 35
percent of the spear-phishing attacks, and 1 in 2.7 (38 percent)
were targeted at least once. This suggests a much more extensive
scale where campaigns were more scattergun in their approach.
Having acknowledged the risk, organizations can take steps
to protect themselves by reviewing their security and incident
response plans, getting advice and help if required, updating the
technical defenses, putting good personnel policies and training
in place, and staying up to date with the latest information.
Whether an insider attack, or
criminal fraud focused on websites
and point-of-sale devices, data
breaches continued in 2015, costing
victims more than ever. The number
of mega-breaches climbed to
the highest level since 2013. The
number of breaches where the full
extent of a breach was not revealed,
increased; fewer companies declined
to publish the numbers, unless
required to do so by law.

Symantec figures show the total number of breaches has
risen slightly by 2 percent in 2015. The year also saw nine
mega-breaches, surpassing 2013’s record of eight breaches
containing more than 10 million identities each. Another new
record was set near the end of the year when 191 million identities were exposed, surpassing the previous record for the largest
single data beach.
Helped in no small part by this massive breach, the overall total
number of identities exposed has jumped 23 percent to 429
million. What’s more concerning is that this number is likely
much higher due to the increasing tendency of organizations to
limit the information released about the extent of the breaches
they suffer. In 2015, the number of breaches reported that
did not include a figure for identities exposed increased by 85
percent, from 61 to 113. Symantec estimates the total number
of identities exposed, had these breaches been fully reported, is
likely to be at least half a billion.
It’s a staggering number, but also one full of speculation based
on incomplete data. The median number of identities exposed
per breach has decreased by around a third to 4,885 identities
per breach. However, this does not lessen the cause for concern,
but rather suggests the data stolen across breaches is more
valuable and the impact to the business greater than in previous
years.

Looking at industries across the broadest of categories, the
Services sector was impacted by more data breaches than any
other industry, both in terms of the number of incidents and the
number of identities exposed. However, the reasons in each case
differs when looking at the sub-sectors contained within these
high-level classifications.
The largest number of breaches took place within the Health
Services sub-sector, which actually comprised 39 percent of all
breaches in the year. This comes as no surprise, given the strict
rules within the healthcare industry regarding reporting of data
breaches. However, the number of identities exposed is relatively small in this industry. Such a high number of breaches with
low numbers of identities tends to show that the data itself is
quite valuable to warrant so many small breaches.
The sub-sector responsible for the most identities exposed was
Social Services. However, this is largely due to the record-breaking data breach responsible for 191 million identities exposed.
Removing this one breach drops Social Services to the bottom
of the list.
Average premiums for retailers surged 32 percent in the first
half of 2015, and the healthcare sector saw some premiums
triple. Reuters also reports that higher deductibles are now
common and even the biggest insurers will not write policies for
more than $100 million for risky customers.

As a result, cyber insurance claims are becoming more common.
This year’s NetDiligence Cyber Claims study saw claims
ranging up to US$15 million, while typical claims ranged from
US$30,000 to US$263,000. But the cost of insuring digital assets
is on the rise, contributing further to the rising overall cost of
data breaches.

This calls into question how risk factors into a data breach. An
industry may suffer a large number of data breaches or expose
a large number of identities, but does this mean that the data
itself is being used for nefarious purposes?
For instance, 48 percent of data breaches were caused by data
accidentally being exposed. Personal data in these cases were
indeed exposed, be it by a company sharing data with the wrong
people or a misconfigured website that inadvertently made
private records public. But was this data obtained by people with
malicious intentions? In many cases, it’s likely that it was not.
A retired grandmother who accidentally receives someone else’s
healthcare record by email is unlikely to flip this information
for identity theft. That’s not to say it never happens―just that a
large majority of such data breaches are of a lower risk.
What is a much higher risk are cases where either hackers or
insider theft was the cause of a breach. These are instances
where the motive was very likely to steal data. To that end, here
are some examples of high risk industries.

The Health Services sub-sector still tops the list for number of
incidences, but it is now followed by the Hotels & Other Lodging
Places sub-sector. Interestingly, 100 percent of breaches in this
particular sub-sector included credit card information, but only
seven percent actually reported the number of identities stolen.
The Business Services sector dropped from second to third place
when looking at high-risk causes. The companies breached in
this sector are primarily dominated by online businesses and
software manufacturers.

The more details someone has about an individual, the easier it
is to commit identity fraud. Criminals are targeting insurance,
government, and healthcare organizations to get more complete
profiles of individuals.
The types of information that thieves are persuing has not
changed in 2015, save some minor changes in ranking. Real
names are still the most common type of information exposed,
present in over 78 percent of all data breaches. Home addresses,
birth dates, Government IDs (like SSN), medical records, and
financial information all appear in the 40 to 30 percent range, as
in 2014, though their order of appearance has changes slightly.
Rounding out the top 10, email addresses, phone numbers,
insurance information, and user names/passwords again appear
in 10 to 20 percent range.
This isn’t to say credit card data isn’t still a common target. Its
black market value isn’t especially high on a per-card basis, since
credit card companies are quick to spot anomalous spending
patterns (as are credit card owners) and stolen card data and
other financial information has a limited shelf life. However,
there is still an evergreen market for stolen credit card data.

While insider theft only accounted for around 10 percent of data
breaches in 2015, the NetDiligence Cyber Claims study reported
that there was insider involvement in 32 percent of the claims
submitted in 2015. According to its CEO, a disgruntled insider
was alleged to have been responsible for one of the most publicized data breaches of the year, at Ashley Madison. Although
this has not been confirmed, if true, it highlights the potential
damage a malicious insider can inflict.

Retail remains a lucrative sector for criminals, although the
introduction of the EMV standard, or ‘chip-and-PIN’ payment
technology, in the US means the information criminals will
be able to scrape from point-of-sale (POS) devices will be
less valuable. EMV is a global standard for cards equipped
with microchips, and the technology has been in use in some
countries since 1990s and early 2000s. EMV is used to authenticate chip-and-PIN transactions, and following numerous
large-scale data breaches in recent years, and increasing rates
of credit card fraud, credit card issuers in the US are migrating
to this technology in a bid to reduce the impact of such fraud.
Previously, criminals could get hold of ‘Track 2’ data, which
is shorthand for some of the data stored on a card’s magnetic
strip. This made it easier to clone credit cards and use them in
stores, or even in ATMs, if they had the PIN. Track 1 stores more
information than Track 2, and contains the cardholder’s name,
as well as account number and other discretionary data. Track 1
is sometimes used by airlines when securing reservations with a
credit card. The value of this data is reflected in the online black
market sale prices, with Track 2 data costing up to US$100 per
card.
As of October 2015, 40 percent of US consumers have EMV
cards, and 25 percent of merchants are estimated to be EMV
compliant.
The proportion of identities exposed that was accidentally made
public increased to 48 percent from 22 percent in 2014.
Insider threats have always been a hot topic in cybersecurity,
but in 2015, government bodies not only started to take notice―
and take action.

More than three-quarters of US government agencies
surveyed in the MeriTalk Federal Insider Threat Report say
their agency is more focused on combating insider threats
today than one year ago.

Cybercriminals are not only interested in ‘who can hack,’ but
also ‘who can leak.’ Whether data may be stolen in a data
breach, accidentally leaked, or even posted online legitimately in the past, personal data has a value in the underground
shadow economy. Until relatively recently, many people did not
recognize the potential value in personally identifiable information, and often were very lackadaisical in safeguarding it.
The advent of social media in the last decade has enabled more
people to share more personal data than at any time in history,
and privacy controls were not at the forefront of many social
networking applications.
Personal data can and will be used to commit crimes, whether
to conduct identity fraud, or to enhance the social engineering
in phishing scams, or even as part of the reconnaissance in the
prelude to a targeted attack. The recognition of the potential
value of this data in the wrong hands has resulted in social

networking services enhancing and tightening their privacy
controls, and more people regarding their personal data with
greater respect. For example, the European Court of Justice’s
“right to be forgotten” ruling rippled through the data-gathering community in May 2014 and by the end of 2015, Google had
received 348,085 requests to delist specific search results.
While many thought this would only be of benefit to those
wanting to hide scandal or avoid incrimination, according to
Google’s FAQ, some of the most common cases for removal
are sites that contain personal contact or address information
or “content that relates solely to information about someone’s
health, sexual orientation, race, ethnicity, religion, political
affiliation and trade-union status”.
And the European Court of Justice sharpened the public’s focus
on privacy again this year when it ruled the 2000 “Safe Harbor”
agreement to be invalid. As Monique Goyens, director general
of the European Consumer organization explained, the ruling
confirms that “an agreement which allows US companies to
merely declare that they adhere to EU data protection rules
without any authority screening this claim is clearly not
worth the paper it is written on.” As The Guardian newspaper
commented at the time, it may “help stop the US government
from being able to gain access to user data from the EU” and
“may open the door to further probes, complaints, and lawsuits
from users and data regulators.”
However, in February 2016, The European Commission and the
US agreed on a new framework for transatlantic data flows:
the EU-US Privacy Shield. The new framework was designed
to address the requirements set out by the European Court of
Justice after ruling the old Safe Harbor framework invalid. The
press release states, “The new arrangement will provide stronger
obligations on companies in the US to protect the personal data
of Europeans and stronger monitoring and enforcement by the
US Department of Commerce and Federal Trade Commission
(FTC), including through increased cooperation with European
Data Protection Authorities.”
Surveying seven thousand people across Europe, Symantec’s
2015 State of Privacy Report shows that in the UK alone, 49
percent of consumers are worried their data is not safe. And
across the EU, technology companies (22 percent), retailers (20
percent) and social media companies (10 percent), were the least
trusted. Symantec sees the lack of trust in these companies as a
reputational issue, possibly stemming from recent high-profile
data breach incidents.
We expect that reluctance to share personal information will
grow and begin to change online behavior among consumers.
One of the major reasons data privacy is becoming such a
concern is because there is now a clear understanding amongst
consumers that their data holds value. Providers of technology
services should take heed when it comes to data privacy, because
until the technology sector can be trusted to do the right thing
by its consumers to safeguard that data, more work will need

to be done in the coming years to build and sustain the level of
trust needed.
As data breaches proliferate and people’s lives increasingly
move online, we expect to see more regulation and more judicial
interest in the protection of individual privacy in 2016 and
beyond. Businesses need to be more transparent with customers
on how they are keeping data secure. Security needs to be
embedded into a company’s value chain, but it should also be
viewed internally as a customer-winning requirement, and not
just a cost.
Ilias Chantzos, senior director in government affairs at
Symantec commented, “There is a real consistency emerging
that privacy is a competitive advantage for businesses and that
privacy concerns also determine consumers’ behaviour. It is
critical to ensure consumers are empowered to understand what
their data is being used for and how it is protected.”

Of course, all of these relate to preventing outsider attacks.
When it comes to mitigating the risk of malicious or accidental insider threats, organizations need to focus on employee
education and data loss prevention.
Basic security hygiene should be drilled into employees the
same way the public are told to cover our mouths when we
cough or sanitize our hands in hospitals. Organizations should
also be making use of data loss prevention technology to locate,
monitor, and protect their data―wherever it is within the organization―so that they know who is doing what, with what data,
in real time. DLP can block certain types of data from leaving an
organization, such as credit card numbers and other confidential documentation.
Security should be an essential part of operations and employee
behavior, rather than an add-on or something to appease
auditors. Data breaches are unlikely to stop any time soon, but
the scale and impact of them could certainly be reduced if organizations recognized that security goes well beyond the bounds
of the CIO or the IT manager. Security is in every employee’s
hands.

Cybercriminals are more professional and are much bolder, not
only in the targets they go after, but also the sums of money they
seek. These criminal enterprises see themselves as a fully-functioning business, covering a multitude of areas, each with their
own speciality. Just as legitimate businesses have partners, associates, resellers, and vendors, so do those enterprises operating
in the shadows.
While prices for email addresses on the black market have
dropped in recent years, credit card prices have remained
relatively low but stable. However, if they come with ‘luxury’
data—verification that the seller’s accounts are still active or
that a credit card has not yet been blocked—they now fetch a
premium price.
At the other end of the market, a drive-by download web toolkit,
which includes updates and 24x7 support, can be rented for
between US$100 and US$700 per week, while distributed
denial-of-service (DDoS) attacks can be ordered from US$10
to US$1,000 per day. And at the top of the market, a zero-day
vulnerability can sell for hundreds of thousands of dollars.
Moreover, these figures have changes very little since 2014.

Ransomware has become increasingly dominant in recent years
and in 2014 many expected to see this trend continue. However,
while we have seen ransomware attacks diversify, the growth in
volume has not been seen. Attacks have moved to mobile devices,
encrypting files, and anything else an owner will pay to recover.
One crypto-ransomware tactic that seeks to increase the
pressure on victims to pay-up, threatens to destroy the only
copy of the secret key after a certain time, with the encrypted
data potentially lost forever. Crypto-ransomware employs very
strong, ostensibly unbreakable key-based cryptography to hold a victim’s
personal files to ransom by encrypting them with a key that only the
criminals have access to.

Some ransomware now also threatens to publish the victim’s
files online unless they pay―an interesting and sinister twist,
which is likely to increase since the traditional advice of keeping
effective backups, does not help in this scenario.

Credit card fraud involves several people to conduct, and
consumer legislation ensures the victim’s financial loss is
minimized. In contrast, an attacker can obtain a ransomware
toolkit from an underground source, and target their intended
victims, who may have few alternatives but to pay-up. There are

Never before in the history of human kind have people across
the world been subjected to extortion on a massive scale as they
are today. But why are criminals favoring ransomware, especially crypto-ransomware? With the glut of stolen information on
the black market and the introduction of the more secure EMV
standard (chip-and-PIN) payment cards for card payments in
the US, the potential profit criminals can gain by exploiting
stolen credit card details had reduced.

With the build up to the presidential elections in the US, spam
that leads to malware has been circulating that uses the US
presidential primaries as bait. Spammers know how to play into
visceral, emotive themes, like global events, the refugee crisis
in the Middle East, immigration, and foreign policy issues, the
economy, and even terrorism.
In January 2015, the Twitter and YouTube accounts of the US
military command were hacked by self-styled supporters of the
jihadist terrorist group, ISIS (a.k.a. IS, ISIL or Daesh). US Central
Command commented that it was, “cyber-vandalism” rather
than a serious data breach.
However, in April 2015, French television network TV5 Monde
reported that it had been hacked by a group claiming to belong
to the terrorist group, ISIS. According to reports, its TV station
was brought to a standstill, and its website and social media
pages were also disrupted in the attack. The hackers posted

Ransomware also targeted Linux web servers in 2015, encrypting
files associated with web applications, archives, and back-ups.
The evolution of Linux ransomware has also mirrored that of
Windows ransomware: initial versions were basic, and often
used poor encryption, making it relatively simple to recover
encrypted files. However, just like with Windows ransomware,
we can expect the criminals behind this new trend to quickly
learn from their mistakes, and become more sophisticated in the
future.

Both examples highlight a clear-cut case of terrorists using
cyberthreats as an instrument to amplify their messages. The
Internet has become not only tool only for online radicalization,
but also for communication between terrorist groups, and for
financing their operations. As a consequence, the calls for law
enforcement to break encryption protocols are likely to have a
wider and long-lasting impact on the technological integrity of
Internet communications as a whole.
In a refereence to terrorism, one recent email campaign impersonated local law enforcement officials in the Middle East and
Canada, tricking people into downloading malware by posing
as security tips that would keep the intended victim safe from
potential terror attacks in their location. The email spoofed the
addresses of law enforcement agencies and included the names
of officials who were all still in office at the time of the campaign.
The subject lines in the emails often reflected the name of an
employee who worked within the targeted company.
To make this type of attack convincing requires some degree of
research, and here we have seen that this group did so before
sending these phishing emails. Furthermore, without any
employee information, they would email other people in the
company as an entry point, such as customer services or IT
personnel.

This level of research and localisation indicates a growing professionalism, and is becoming increasingly common in botnet
scams. The underground economy isn’t just about selling stolen
goods: it’s an entire industry with the talented professionals and
organizations you would expect in a legitimate business sector.

Botnets and the Rise of the Zombies
As with many other industries, up and coming economies,
such as China in particular, has become a favoured as target for
cybercrime in 2015. One significant factor has been a growth
in broadband adoption in the last year. In 2013, the Chinese
Government announced plans to expand broadband coverage
for both rural and urban areas by 2020. One of the milestones
for the multi-pronged strategy aimed to bring fixed broadband
connections to 400 million Chinese households by 2015. In
addition, prices have been kept low as broadband speeds have
increased. All of this make the country an attractive target for
cybercriminals seeking to compromise a fresh source of highspeed, internet-connected computers.

The Dyre Consequences and Law Enforcement
After police shut down several major financial botnets in 2014,
Dyre stepped up to take their place. Not only could Dyre hijack
common web browsers and intercept Internet banking sessions
to steal information, it could also download additional malware
to the victim’s computer, binding it to the perpetrator’s network
of botnet computers.

China was the origin of much more bot activity in 2015, seeing a sharp rise of 84 percent in bot-related activity in that country. Bot activity in the
US by contrast, fell by 67%. Successful law enforcement activity against
cybercriminals, and heightened cybersecurity awareness are both
contributing factors in the decline of bots in general.

Dyre had initially emerged as one of the most dangerous
financial fraud operations, configured to defraud the customers
of more than 1,000 banks and other companies worldwide.
However, the cybercrime group controlling the Dyre financial
fraud Trojan suffered a major blow following a Russian law
enforcement operation in November. As outlined in a Security
Response blog, Symantec telemetry has confirmed a virtual
cessation of the group’s activities. Dyre (detected by Symantec
as Infostealer.Dyre) was spread through email campaigns and
no Dyre-related email campaigns have been observed since
November 18, 2015. Detections of the Dyre Trojan and associated malware dropped dramatically soon after. Previously, the
number of infections was estimated to be above 9,000 per month
in early 2015. In November it fell to below 600 per month.
Law enforcement has become more effective at catching cybercriminals like these, and high-profile successes at disrupting
them shows how coordinated, international efforts can pay
dividends. Rarely is an attack group confined to one country, and
with major groups spanning multiple jurisdictions, cross-border cooperation with law enforcement is an important factor
to ensure that these successes continue to strike a blow against
cybercriminals. We expect to see still more successful law
enforcement operations against cybercriminals in the next year.

As the risks for the cybercriminals intensify, the potential
rewards will diminish, raising the barrier to entry for any
would-be cybercriminals. Other notable successes in 2015
included:
takedown. The Dridex botnet specialized in stealing
bank credentials. In October, an international law enforcement operation coordinated efforts to sinkhole thousands of
compromised computers, cutting them off from the botnet’s
control, and saw one man charged. However, this may have
been a partial success as Dridex continues to propagate,
indicating that many key elements of the operation are still
functioning. As such, we expect the group to continue to
pose a serious threat during 2016.
In April, infrastructure owned by the Simda botnet’s controllers, including a number of command and control servers, was seized by law enforcement. According to
Interpol, “Simda was used by cyber criminals to gain remote
access to computers enabling the theft of personal details,
including banking passwords, as well as to install and spread
other malware.”
seizure. In February, a law enforcement operation
led by Europol and assisted by, among others, Symantec and
Microsoft, seized servers and other infrastructure owned by
the cybercrime group behind the Ramnit botnet.

Federal authorities indicted at least four men in
connection with hacking incidents that resulted in the theft
of over 100 million customer records. They were charged
with hacking into multiple financial institutions and for
operating a stock pump-and-dump scheme. One of the
attacks occurred in 2014, and netted more than 80 million
customer records, a breach that the US Justice Department dubbed the “largest theft of customer data from a US
financial institution in history.”

Organizations and individuals need to realise that even if they
don’t think they’re an obvious target for cybercriminals, it
doesn’t mean they’re not one.

IT systems continue to come
under attack from rapidly evolving
malware. No operating system is
automatically immune, and malware
threats against Linux and Mac
OS X are increasing. Even cloudhosted and virtualized systems are
vulnerable. Malware is able to seekout virtualized environments and
infect them.

The days of an operating system avoiding attacks simply by not
being Windows is long behind us. Attacks against Mac OS X and
Linux have both increased considerably in 2015 and cybersecurity is a necessity across the board for all operating systems―not
just for Windows―to avoid the consequences of attack.
Cybersecurity affects everyone. Businesses need to protect their
computers and IT infrastructure to stop data theft, fraud, and
malware attacks. Likewise, businesses and consumers should
be concerned about ransomware holding their data hostage,
identity theft, and attackers using their computers as a springboard to attack others.
At a fundamental level, cybersecurity is about protecting the
sinews of IT everywhere: computers, servers, and networks. The
problem is that malware is ubiquitous. In 2015, we have seen
many more systems come under attack, including Linux, Macs,
virtualized computers, and cloud systems. Each year, the cloud
handles more of our data, whether it is for customer relationship management, invoicing services, social networking, mobile
email, and a whole gamut of other applications
One route for attacks is through exploiting vulnerabilities, and
most systems have vulnerabilities. These exist in the operating
systems and applications used on them, and are an important
aspect of cybersecurity. If left unpatched, a vulnerability may leave the path clear for would-be attackers to exploit them
and use them for malicious purposes. Each year, researchers
uncover new vulnerabilities, and the most coveted of these are
zero-days, a special type of vulnerability for which a patch is not
yet available.

In the last year, Symantec has seen threats to almost every kind
of computer, operating system, and other essential IT services,
including:
OS X. In addition to more vulnerabilities being
uncovered in 2015, proof-of-concept ransomware and
several methods for Trojans to gain unauthorised access to
affected computers were also discovered.

Symantec researchers discovered malware that
attacks MySQL―a very popular database system―and uses it
to launch denial-of-service attacks on other systems.

There was a rapid growth in Linux malware in 2015,
including attack kits that hackers can use to infect unpatched
Linux web servers.

Even virtualised systems are not
immune. Sixteen percent of malware is routinely able to
recognize and exploit a virtual machine environment, and
vulnerabilities such as VENOM could allow an attacker to
escape an infected virtual machine and attack others on the
same system, or even attack the host hypervisor.

Apple’s Mac OS X operating system was targeted for a variety
of attacks in 2015, including a proof-of-concept ransomware
threat called Mabouia (detected as OSX.Ransomcrypt), the first
effective file-based ransomware threat against OS X. Previously,
browser-based threats against Macs have been found, including
ransomware targeting Safari through a malicious website.
Moreover, the volume of OS X malware has doubled (100%
growth) since the start of 2015. In Q1, Symantec blocked approximately 3,650 attacks each day, rising to 7,255 by the end of Q4.

Attackers will often contaminate compromised web servers with
code that links to exploit toolkits, or they to send spam emails
and steal usernames and passwords. Additionally, compromised
web servers are often a springboard from which an attacker will
conduct a wide variety of other attacks, including very powerful
DDoS attacks, where the bandwidth of a hosting provider is
considerably greater than that of a home-user with a broadband
connection.
A proliferation of specialized, automated attack toolkits have
emerged, making it easier for cyber criminals to carry attacks
against Linux systems. These toolkits help attackers to sniff-out
potentially vulnerable servers, scanning for insecure content
management systems and other exposed web applications.
Ransomware targeting Linux was also uncovered in 2015,
targeted in particular files with extensions associated with web
applications. The program also encrypted archives and directories that contained the word ‘backup,’ making it particularly
difficult for anyone without offsite backups.

The term “cloud computing” covers a wide variety of technical
solutions and environments, including software-as-a-service
(SaaS), platform-as-a-service (PaaS), or infrastructure-as-a-service (IaaS) models. IaaS is growing in popularity among
businesses, and as more data and services move to the cloud,
it is attracting more attention from security researchers and
cybercriminals. As with any system, each time a new layer
is introduced to a service stack, the attack surface increases.
While cloud environments may suffer from common vulnerabilities, such as SQL injection flaws, they may also be impacted
by other issues. For example, in 2015, Symantec found that
misconfiguration and poor management (by users, not cloud
service providers) left cloud-hosted systems vulnerable to
unauthorized access. Additionally, 11,000 publicly accessible
files―some containing sensitive personal information―were
also unearthed. Stolen credentials for cloud-based systems are
regularly traded on underground markets, typically for less than
US$10.

Cloud Vulnerabilities
It is not necessarily the case that cloud systems are inherently
less-secure than traditional IT services. Nevertheless, administrators need to ensure that the cloud services they use are
properly configured and all data is adequately protected. They
should take care to control access to their cloud systems, preferably with two-factor authentication.

Vulnerabilities, like VENOM, could allow an attacker to escape
from a guest virtual machine (VM) and access the native host
operating system, along with other VMs running on the same
platform. Attackers exploiting the VENOM bug could potentially steal sensitive data on any of the virtual machines on the
affected system, and gain elevated access to the host’s local
network and its systems. The VENOM bug (CVE-2015-3456)
existed since 2004 in the open-source hypervisor QEMU, which
is often installed by default in a number of virtualized infrastructures using Xen, QEMU, and KVM. However, it is important
to note that VENOM does not affect VMware, Microsoft Hyper-V,
and Bochs hypervisors.
To date, the VENOM bug has not known to have been exploited
in the wild, and QEMU’s developers and other affected vendors
have since created and distributed patches for VENOM.
One in six (16 percent) malware variants is able to detect the
presence of a virtualized environment, compared with one in
five (20 percent) in 2014. This ability can help the malware to
better evade detection, particularly on security sandboxing
systems using virtualization. More concerning is that an attack
may detect when it is able to exploit and infect other virtual
machines on the same system.

Having a robust security profile for virtual systems is now more
important than ever. Virtual machines and cloud services need
securing in the same way as other services and devices. Policies
should cover the virtual infrastructure as well as the physical
one, and the use of integrated security tools across all platforms
will help to mitigate such problems in the future.

It is important for the CIO to understand what the organization
is doing, and whether certain teams are looking for services or
applications that are not provided for, then determine how to
address that need and offer that service in a secure fashion.
Having the right processes is key to protecting information and
data, even when it is not housed inside the enterprise.

Be concerned about cloud systems too.
As companies move their IT systems to virtual and cloud-hosted environments, they face new security challenges. In addition,
as ever, human nature itself is a threat, with poorly-managed
security leading to shadow IT systems. Shadow IT refers to
solutions used inside organizations without explicit organizational approval, and solutions used by departments other than
the IT department. It can sometimes be all too easy for a group
of employees to turn to external products to fulfil an immediate
need. IT decision makers should understand what is influencing their employees to turn to these solutions, and when the IT
department should be involved to help shape those decisions.

Distributed denial-of-service (DDoS)
attacks are growing in number
and intensity, but most last for 30
minutes or less. The availability of
botnets-for-hire has fueled this
increase and we are likely to see
the Internet of Things provide more
fodder for these botnet armies.

Some DDoS attacks can still afford criminals many opportunities for financial reward through extortion and blackmail by
disrupting an organization’s website. Following the money trail
made this more difficult and DDoS mitigation technologies
meant the attackers needed greater and greater bandwidth in
order to make an impact. More recently, however, it is hacktivist
groups and sometimes state actors that are complicit in some of
the biggest attacks.
The recent attack on the BBC, which saw its website and associated services including iPlayer (the BBC’s Internet catch-up
TV and radio service in the UK) taken down for several hours
on New Year’s Eve, is a prime example. It is thought to be the
biggest ever DDoS attack, according to New World Hacking, the
anti-Islamic State organization that claimed responsibility. The
attackers claimed that the BBC’s scale offered a chance for them
to test their capabilities and claim the attack reached a peak of
602 Gbps.
There are rewards to be gained through a DDoS attack, the
most obvious being blackmail. Victims are threated to pay or
have their sites remain under attack. DDoS has also been used
as a “distraction” tool in conjunction with some high-profile
targeted attacks in 2015, where attackers flooded the website of
the targeted organization, leaving the IT team believing it was
the prelude to a ransom demand. In reality, another, stealthier
attack was quietly taking place at the same time.

Different attack groups have different preferences for their
DDoS campaigns, and ICMP flood attacks were one of the main
methods used by the Darkness/Optima botnet. Some methods,
particularly amplification attacks, may no longer work that
well over time. For example, when the media extensively covers
a high-profile attack, more people will patch their servers. In
addition, botnets that were used to perform previous attacks
may be taken down or upgraded to newer versions that provide
new functionality.

So why are DDoS attacks so popular? The answer is the same
now as it was when we first wrote about them in December 2002:
they are simple to set up, difficult to stop, and very effective.

Botnets-for-hire were implicated in roughly 40 percent of all
DDoS network layer attacks in the second quarter of 2015,
according to Incapsula, a Symantec partner. While criminals
can go to the effort of infecting multiple vulnerable devices and
creating their own botnet to carry out DDoS attacks, it’s often
much easier to hire pre-made botnets for a set amount of time.
Prices remained fairly steady in the black market in 2015, where
DDoS attacks can be ordered from just US$10 to US$1,000 per
day. The cost to a business will be significantly higher, perhaps
as much as a thousand times greater, depending on the nature
of the business and the importance of the company’s website.
In 2015, Incapsula reported a DDoS attack can cost an organization as much as US$40,000 per hour. Consequently the potential
rewards for an attacker successfully holding a company to
ransom in this way will more than compensate for their costs.
For example, one Australian email provider was attacked and
attackers demanded a payment of 20 Bitcoins, worth about
US$6,600. Another company that paid the demand was soon
subjected to another assault shortly afterwards.

The rise in popularity of DDoS-as-a-service corresponds with
the significant drop in network layer attack duration in the third
quarter of 2015 compared with the second quarter. Some of
these DDoS-for-hire services refer to themselves as “stressers,”
because conducting a DDoS attack is illegal, they hide behind a
veil, inferring they can be used for “stress testing” server resilience.

These shorter hit-and-run style attacks are indicative of a shift
towards the greater use of DDoS being offered as a service,
where subscribers are allotted limited access to the overall
botnet resources, which are shared with other subscribers.
This will usually be sufficient for them to conduct a few shorter-duration, mid-sized attacks. This can also help the attackers
determine how effective the target infrastructure is at mitigating such attacks, and whether they need to increase the
volume. Incapsula also reported that 100+ Gbps attacks became
commonplace and a 100+ Gbps attack was mitigated once every
other day.

Botnets are key to DDoS attacks, whether they’re hired or
created by the criminals carrying out the attack. The bigger
the botnet, the more simultaneous requests it can send and the
more disruptive the attack will be.
But it’s not just infected PCs that are providing criminals with
their robot army. In October, we saw malware target MySQL
servers, which often offer a much larger bandwidth capacity for
an attack than traditional consumer PCs. This method isn’t new,
but it shows criminals are continuing to create bigger and better
botnets.
In 2015, we also saw criminals making increasing use of the
Internet of Things (IoT) to strengthen their botnet ranks. CCTV
cameras proved particularly popular, likely because they are one
of the most common IoT devices, with 245 million professionally installed video surveillance cameras active and operational
globally in 2014.
Looking ahead, it’s likely that criminals will make increasing use
of vulnerable IoT devices to execute large-scale DDoS attacks.
While solutions exist to mitigate against DDoS attack, organizations will also face new challenges in implementing appropriate
security on non-traditional devices to ensure they don’t become
part of the problem. Perhaps more concerning, without the right
security in place, it will be even more difficult to know when
your printer, or refrigerator, thermostat, or toaster is actually
part of a toxic global botnet.

This is the 21st edition of the Symantec Internet Security Threat
Report and much has changed since the first one. Each year we
take a fresh look at the structure and contents of the report. As
well as focusing on the threats and reporting the findings from
our research, Symantec also tracks industry trends, and in the
report, we try to highlight the important developments and
look to future trends. This goes beyond just looking at computer
systems, smartphones, and other products, and extends into
broad concepts like national security, the economy, data protection, and privacy.

No system is automatically immune from cyber threats, and
in this report, the consequences of ignoring the risks from
complacency, negligence, and incompetence are clear. In 2015,
an unprecedented number of vulnerabilities were identified as
zero-day exploits that have been weaponized, and web attack
exploit kits are adapting and evolving them more quickly than
ever. As more devices are connected, vulnerabilities will be
exploited. Safeguarding Internet-connected devices will become
critically important to ensuring the safety of industrial control
systems (ICS) and medical devices in the community.

Cybersecurity Matters.

Alongside the rising number of software vulnerabilities, and
the parade of attacks on different systems, the future will bring
with it a greater range of diversity as threats against Windows
systems will extend to other operating systems, mobile, and
other IoT devices.

This report takes a high-level view of cybersecurity and Internet
threats, underlining the notable changes and developments.
However, we must not forget that cybercrime is not victimless.
For example, ransomware locks people out of their computers,
holding treasured family photos to ransom, hijacking unfinished manuscripts for novels, and blocking access to tax returns,
banking records, and other valuable documents. Moreover,
there are no guarantees that paying the ransom will release
those padlocks. Businesses, as well as home users, have become
victims, and relying on backups is often the last line of defense
when cybersecurity should really be the first.
Targeted attacks steal invaluable intellectual property from
businesses, and a data breach can shred an organization’s reputation―even threatening its survival. Cyber insurance claims
are growing in number and cost, pushing premiums even higher.
In the broadest sense, cybersecurity problems threaten national
security and economic growth, which ultimately affects us all.

Updates to protect against such vulnerabilities are released
regularly, including for SSL/TLS protocol libraries, such as
OpenSSL, but website owners still have to install them. We
have seen in this report and over the past few years that this
is still not happening quickly enough. The number of vulnerable websites continues to persist year after year, with very little
improvement to show. While the move from SHA-1 certificates
to the much stronger SHA-2 is gaining momentum, organizations must deploy the new certificates properly in order for the
changes to be effective.
Criminals continued to find vulnerabilities in the underlying
infrastructure of website security in 2015, exploiting weaknesses in the underlying encryption systems, allowing attackers
to intercept and control secure connections. The wider debate
around security, privacy, and strong encryption will ultimately
affect all of us.

In cybersecurity, we often talk about infections and viruses.
But the state of ubiquitous attacks, epic data breaches, and
advanced threats we have seen this year suggest that there are
better medical analogies. Instead of infection, we might think of
disease both chronic and acute, serious, and benign.
Instead of thinking in binary terms of infection-free and compromised, we should move to a wellness model that considers
susceptibility, resilience, wellness, vulnerability to infection,
and recoverability. As IT security professionals, we should
emphasize prevention, detection, and mitigation, as well as a
complete cure. Concepts borrowed from epidemiology, incident
response planning, and tools such as security simulation are
becoming more important and useful.
For individuals and companies, Internet security is going to
be much more like ‘wellness’ and ‘hygiene’ than ‘medicine,’
and focused on the routine of prevention rather than looking
for a panacea or cure. We all need to stay digitally healthy and
digitally clean, and habits of security will need to be relearned,
over and over again.
Similarly, IT departments need to be proactive in reducing
the risk from persistent intrusions and malware, and identify
breaches quickly. Unfortunately, discovering attacks quickly
requires constant, active vigilance. Information security can’t
wait for support tickets to open or for a favored security tool to
identify an issue conclusively. Security needs to start digging
through the data proactively during non-breach response time.

As an industry, we need to start moving into a more investigative, clinical-study mindset where we are constantly researching
the habits or artifacts that cause the “digital diseases.” Taking
risks with cybersecurity will be seen as unacceptable, perhaps
anathema akin to driving a car while under the influence of
alcohol.
Cybersecurity is not just about employing the right kind of
technology, it also requires good digital hygiene on the part of
everyone; both at home, and in the office. Education and greater
awareness of cybersecurity issues will help everyone to become
more digitally healthy. By being aware of just how many risks
you face, you can reduce them, and learn how to recognize
symptoms, and diagnose “digital diseases” before they put your
data, and your customers’ data at risk. We should reject the
misconception that privacy no longer exists. Privacy is precious,
and should be protected carefully.

Emphasize multiple, overlapping, and mutually supportive
defensive systems to guard against single-point failures in any
specific technology or protection method. This should include
the deployment of regularly updated firewalls as well as gateway
antivirus, intrusion detection or protection systems (IPS),
website vulnerability with malware protection, and web security
gateway solutions throughout the network.

Monitor for Network Incursion Attempts,
Vulnerabilities, and Brand Abuse
Receive alerts for new vulnerabilities and threats across vendor
platforms for proactive remediation. Track brand abuse via
domain alerting and fictitious website reporting.

Antivirus on Endpoints Is Not Enough.
On endpoints, it is important to have the latest versions of
antivirus software installed.

Implement and enforce a security policy whereby any sensitive
data is encrypted. Ensure that customer data is encrypted as
well. This not only serves to prevent data breaches, but can also
help mitigate the damage of potential data leaks from within an
organization.

Access to sensitive information should be restricted. This
should include a Data Loss Protection (DLP) solution that can
help prevent data breaches and minimize their impact.
Implement a DLP solution that can discover where sensitive
data resides, monitor its use, and protect it from loss.

Monitor the flow of information as it leaves the organization over the network, and monitor traffic to external
devices or websites.

DLP should be configured to identify and block suspicious
copying or downloading of sensitive data.

Where practical, restrict unauthorized devices, such as external
portable hard-drives and other removable media. Such devices
can both introduce malware and facilitate intellectual property
breaches, whether intentional or unintentional. If external
media devices are permitted, automatically scan them for
viruses upon connection to the network and use a DLP solution
to monitor and restrict copying confidential data to unencrypted external storage devices.

Update, patch, and migrate from outdated and insecure
browsers, applications, and browser plugins. This also applies
to operating systems, not just across computers, but mobile,
ICS, and IoT devices as well. Keep virus and intrusion prevention definitions at the latest available versions using vendors’
automatic updates.
Most software vendors work diligently to patch exploited
software vulnerabilities; however, such patches can only be
effective if adopted in the field. Wherever possible, automate
patch deployments to maintain protection against vulnerabilities across the organization.

Ensure passwords are strong. Passwords should be at least 8-10
characters long and include a mixture of letters and numbers.
Encourage users to avoid re-using the same passwords on
multiple websites and sharing passwords with others should be
forbidden. Passwords should be changed regularly, at least every
90 days.

Configure mail servers to block or remove email that contains
file attachments that are commonly used to spread viruses, such
as VBS, BAT, EXE, PIF, and SCR files. Enterprises should investigate policies for PDFs that are allowed to be included as email
attachments. Ensure that mail servers are adequately protected
by security software and that email is thoroughly scanned.

For example, if Windows users see a warning
indicating that they are “infected” after clicking on a
URL or using a search engine (indicative of fake antivirus
infections), educate users to close or quit the browser using
Alt-F4, CTRL+W or to use the task manager, and then notify
the helpdesk.


We recommend that people and employers treat mobile
devices like the small, powerful computers that they are.
We have seen the number of mobile vulnerabilities increase
every year over the past three years―although this is perhaps
an indicator of progress rather than a cause for despair. It
is an indication that security researchers, operating system
developers and app writers are, in fact, paying more attention
to mobile security by identifying and fixing more problems.
Although we expect mobile devices to come under growing
attack over the next year, there is also hope that with the right

preventative measures and continuing investment in security,
users can achieve a high level of protection against them.

The diverse nature of ICS and IoT platforms make host-based
intrusion detection systems (IDS) and intrusion prevention
systems (IPS), with customizable rulesets and policies that are
unique to a platform and application, suitable solutions.
However, manufacturers of ICS and IoT devices are largely
responsible for ensuring that security is built into the devices
before shipping.
Building security directly into the software and applications
that run on the ICS and IoT devices should prevent many
attacks that manage to side-step defenses at the upper layers.
Manufacturers should adopt and integrate such principles
into their software development processes.
Business users and consumers need to be assured that
suppliers are fundamentally building security into the IoT
devices that they are buying, rather than it being considered
as a bolt-on option.

Consumer confidence is built up over multiple interactions
across numerous websites owned by countless different organizations. But it only takes one bad experience of stolen data
or a drive-by download to tarnish the reputation of every
website in the consumer’s mind.
As we said at the start of the report, there is a real opportunity in the coming year to reduce the number of successful
web attacks and limit the risks websites potentially pose to
consumers, but it will take commitment and action from
website owners for it to become a reality.
Adopt Complete Website Security in 2016, and together with
Symantec, make it a good year for cybersecurity and a very
bad one for cybercriminals.

For website security to be effective, it has to be implemented with
care and attention and it has to be monitored and maintained
continually.
While there are tools to help you keep your website ecosystem
secure, it all starts with education. You’ve read about the risks―
now find out what you can do about them.

Get in line with industry standards
always-on SSL. Implement SSL/TLS on every
page of your website so that every interaction a visitor has
with your site is encrypted. Switching to ‘HTTPS everywhere’,
as it’s also called, with OV or EV SSL/TLS certificates demonstrates your credibility and can also improve your search
rankings and paves the way for an upgrade to HTTP/2, delivering better performance.

Implement SHA-2. As discussed in the report, certificate
authorities should have stopped issuing SHA-1 certificates as
of 1 January 2016, but you need to ensure any legacy certificates are also upgraded and that any devices and applications
that may not currently recognize SHA-2 are upgraded too.
Symantec also offers the use of the
ECC encryption algorithm. All major browsers, even mobile,
support ECC certificates on all the latest platforms, and
compared to an industry-standard 2048-bit RSA key, 256-bit
ECC keys are 64,000 times harder to crack.

Use SSL/TLS Correctly.
Keep track of what certificates you have, from which certificate authority, and when
they are due to expire.
Manage your SSL/TLS keys properly. Limit the number of people
with access to them; have separate administrators for managing
the passwords for the server where they’re kept and for managing
the systems they’re actually stored in; and use automated certificate and key management systems to reduce human involvement.
Any breach affecting SSL keys should be notified to the CA
quickly, so that corresponding certificates can be revoked.

Keep an eye on your web servers and watch
for vulnerabilities or malware. Automation tools can help
with this.
Antivirus software isn’t just for PCs and smartphones―it’s for servers too and could help prevent a serious
malware attack against your entire website infrastructure.

The software you use to manage
your website comes with vulnerabilities too. The more
third-party software you use, the greater your attack surface;
so only deploy what’s absolutely necessary.

Have you deployed a Web
Application Firewall to defend against injection attacks? Is
your code signing secure for your web apps? Do you have
automated tools to detect and defend against the increasingly
common problem of DDoS attacks?

Consumer confidence is built up over multiple interactions across
numerous websites owned by countless different organizations. It
only takes one bad experience to tarnish the reputation of every
single one in the consumer’s mind.
As we said in the report, there exists a real opportunity in the
coming year to reduce the number of successful web attacks and
limit the risks your website potentially poses to consumers, but
it will take commitment and action from website owners for it to
become a reality.

Actively manage (inventory, track, and
correct) all software on the network so
that only authorized software is installed
and can execute, and that unauthorized
and unmanaged software is found and
prevented from installation or execution.

Continuously acquire, assess, and take action
on new information in order to identify
vulnerabilities, remediate, and minimize
the window of opportunity for attackers.

Establish, implement, and actively manage
(track, report on, correct) the security
configuration of laptops, servers, and
workstations using a rigorous configuration
management and change control process in
order to prevent attackers from exploiting
vulnerable services and settings.

The processes and tools used to track/
control/prevent/correct the use,
assignment, and configuration of
administrative privileges on computers,
networks, and applications.

Collect, manage, and analyze audit
logs of events that could help detect,
understand, or recover from an attack.

Minimize the attack surface and the
opportunities for attackers to manipulate
human behavior though their interaction
with web browsers and email systems.

Control the installation, spread,
and execution of malicious code at
multiple points in the enterprise, while
optimizing the use of automation to
enable rapid updating of defense, data
gathering, and corrective action.

Manage (track/control/correct) the
ongoing operational use of ports,
protocols, and services on networked
devices in order to minimize windows
of vulnerability available to attackers.

The processes and tools used to properly
back up critical information with a proven
methodology for timely recovery of it.

Establish, implement, and actively manage
(track, report on, correct) the security
configuration of network infrastructure
devices using a rigorous configuration
management and change control process in
order to prevent attackers from exploiting
vulnerable services and settings.

Detect/prevent/correct the flow of
information transferring networks
of different trust levels with a focus
on security-damaging data.

The processes and tools used to prevent
data exfiltration, mitigate the effects of
exfiltrated data, and ensure the privacy
and integrity of sensitive information.

The processes and tools used to track/
control/prevent/correct secure access
to critical assets (e.g., information,
resources, and systems) according to the
formal determination of which persons,
computers, and applications have a need
and right to access these critical assets
based on an approved classification.

The processes and tools used to track/
control/prevent/correct the security use
of wireless local area networks (LANS),
access points, and wireless client systems.

Keep attackers from impersonating
lActively manage the life cycle of
system and application accounts – their
creation, use, dormancy, and deletion
- in order to minimize opportunities
for attackers to leverage them.

For all functional roles in the organization
(prioritizing those mission – critical to
the business and its security), identify the
specific knowledge, skills, and abilities
needed to support defense of the enterprise;
develop and execute an integrated plan
to assess, identify gaps, and remediate
through policy, organizational planning,
training, and awareness programs.

Manage the security life cycle of
all in-house developed and acquired
software in order to prevent, detect,
and correct security weaknesses.

Protect the organization’s information,
as well as its reputation, by developing
and implementing an incident response
infrastructure (e.g., plans, defined roles,
training, communications, management
oversight) for quickly discovering an
attack and then effectively containing
the damage, eradicating the attacker’s
presence, and restoring the integrity
of the network and systems.

Test the overall strength of an organization’s
defenses (the technology, the processes,
and the people) by simulating the
objectives and actions of an attacker.

There is no shortage of other examples.
The online world was rife
with the clashing of ideals, taking the form of activism,
protests, retaliation, and pranks.

This re-imagined and re-invigorated
specter of “hacktivism” rose to haunt
organizations around the world.

While these activities
encompassed more than data breaches (e.g., DDoS attacks), the theft of corporate and personal information was certainly a core tactic. This re-imagined and re-invigorated
specter of “hacktivism” rose to haunt organizations around the world. Many, troubled by the shadowy nature of its
origins and proclivity to embarrass victims, found this trend more frightening than other threats, whether real or
imagined. Doubly concerning for many organizations and executives was that target selection by these groups
didn’t follow the logical lines of who has money and/or valuable information. Enemies are even scarier when you
can’t predict their behavior.
It wasn’t all protest and lulz, however. Mainline cybercriminals continued to automate and streamline their method
du jour of high-volume, low-risk attacks against weaker targets. Much less frequent, but arguably more damaging,
were continued attacks targeting trade secrets, classified information, and other intellectual property.

No big surprise here; outsiders are still dominating the scene
of corporate data theft. Organized criminals were up to their
typical misdeeds and were behind the majority of breaches. Activist groups created their fair share of misery and
mayhem last year as well—and they stole more data than any
other group. Their entrance onto the stage also served to
change the landscape somewhat with regard to the
motivations behind breaches. While good old-fashioned
greed and avarice were still the prime movers, ideological
dissent and schadenfreude took a more prominent role
across the caseload. As one might expect with such a rise in
external attackers, the proportion of insider incidents
declined yet again this year to a comparatively scant 4%.

Incidents involving hacking and malware were both up
considerably last year, with hacking linked to almost all
compromised records. This makes sense, as these threat
actions remain the favored tools of external agents, who, as
described above, were behind most breaches. Many attacks
continue to thwart or circumvent authentication by combining
stolen or guessed credentials (to gain access) with backdoors
(to retain access). Fewer ATM and gas pump skimming cases
this year served to lower the ratio of physical attacks in this
report. Given the drop in internal agents, the misuse category
had no choice but to go down as well. Social tactics fell a little,
but were responsible for a large amount of data loss.

Findings from the past year continue to show that target
selection is based more on opportunity than on choice. Most
victims fell prey because they were found to possess an
(often easily) exploitable weakness rather than because they
were pre-identified for attack.

Whether targeted or not, the great majority of victims
succumbed to attacks that cannot be described as highly
difficult. Those that were on the more sophisticated side
usually exhibited this trait in later stages of the attack after
initial access was gained.

Given this, it’s not surprising that most breaches were
avoidable (at least in hindsight) without difficult or expensive
countermeasures. Low levels of PCI DSS adherence highlight a
plethora of issues across the board for related organizations.

While at least some evidence of breaches often exists,
victims don’t usually discover their own incidents. Third
parties usually clue them in, and, unfortunately, that typically
happens weeks or months down the road.

Once again, this study reminds us that our profession has
the necessary tools to get the job done. The challenge for
the good guys lies in selecting the right tools for the job at
hand and then not letting them get dull and rusty over time.
Evidence shows when that happens, the bad guys are quick
to take advantage of it.

As you’ll soon see, we contrast findings for smaller and larger
organizations throughout this report. You will get a sense for
how very different (and in some cases how very similar) their
problems tend to be. Because of this, it makes sense that the
solutions to these problems are different as well. Thus, most
of the recommendations given at the end of this report relate
to larger organizations. It’s not that we’re ignoring the smaller
guys—it’s just that while modern cybercrime is a plague upon
their house, the antidote is fairly simple and almost universal.

If a third party vendor is handling the two items above,
make sure they’ve actually done them
Larger organizations
Eliminate unnecessary data; keep tabs on what’s left
Ensure essential controls are met; regularly check that
they remain so

Larger organizations exhibit a more diverse set of issues that
must be addressed through an equally diverse set of
corrective actions. We hope the findings in this report help to
prioritize those efforts, but truly tailoring a treatment
strategy to your needs requires an informed and introspective
assessment of your unique threat landscape.

855 data breaches analyzed this year, and there are several things worth noting.
When we observe the overall dataset from a threat management perspective, only 40 of the 315 possible threat
events have values greater than zero (13%). Before going further, we need to restate that not all intersections in
the grid are feasible.
With respect to threat diversity, it’s interesting that the
grid for larger organizations shows a comparatively more even distribution across in-scope threat events (i.e., less
extreme clumping around Malware and Hacking). Based on descriptions in the press of prominent attacks leveraging
forms of social engineering and the like, this isn’t a shocker.

We’ve come to the realization that many
of the organizations covered in this
report are probably not getting the
message about their security. We’re
talking about the smaller organizations
that have one (or a handful) of POS
Not to mention, it’s a message that the rest of us need them to hear too.
These tips may seem simple, but all the evidence at our disposal suggests a huge chunk of the problem for smaller
businesses would be knocked out if they were widely adopted.

The cutout below was created especially for smaller organizations
and we need your help. We invite you, our reader, to cut it out, and
give it to restaurants, retailers, hotels, or other establishments
that you frequent.
You were given this card because someone likes your establishment. They wanted to help
protect your business as well as their payment and personal information.
It may be easy to think “that’ll never happen to me” when it comes to hackers stealing your information. But
you might be surprised to know that most attacks are directed against small companies and most can be
prevented with a few small and relatively easy steps. If hackers can’t reach your system, they can’t easily steal from it.
Each entry contained a description, associated threat agents, related assets,
commonalities, indicators, mitigators, and a case study.

Pretexting is a social engineering technique in which the attacker invents a scenario to persuade,
manipulate, or trick the target into performing an action or divulging information. These
attacks exploit “bugs in human hardware” and, unfortunately, there is no patch for this.

It is designed to exploit human weaknesses and bypasses
technological alerting mechanisms. Unusual communication, requests outside of normal
workflow, and instructions to provide information or take actions contrary to policies should
be viewed as suspect. Call logs; visitor logs; e-mail logs.

A brute-force attack is an automated process of iterating through possible username/password combinations until
one is successful.

The online world was rife
with the clashing of ideals, taking the form of activism, protests, retaliation, and pranks. While these activities
encompassed more than data breaches (e.g., DDoS attacks),
the theft of corporate and personal information was
certainly a core tactic. This re-imagined and re-invigorated
specter of “hacktivism” rose to haunt organizations around
the world. Many, troubled by the shadowy nature of its origins and proclivity to embarrass victims, found this trend more frightening than other threats, whether real or
imagined. Doubly concerning for many organizations and executives was that target selection by these groups
didn’t follow the logical lines of who has money and/or valuable information. Enemies are even scarier when you
can’t predict their behavior.
It wasn’t all protest and lulz, however. Mainline cybercriminals continued to automate and streamline their method
du jour of high-volume, low-risk attacks against weaker targets. Much less frequent, but arguably more damaging,
were continued attacks targeting trade secrets, classified information, and other intellectual property.
How do breaches occur?
Incidents involving hacking and malware were both up
considerably last year, with hacking linked to almost all
compromised records. This makes sense, as these threat
actions remain the favored tools of external agents, who, as
described above, were behind most breaches. Many attacks
continue to thwart or circumvent authentication by combining
stolen or guessed credentials (to gain access) with backdoors
(to retain access). Fewer ATM and gas pump skimming cases
this year served to lower the ratio of physical attacks in this
report. Given the drop in internal agents, the misuse category
had no choice but to go down as well. Social tactics fell a little,
but were responsible for a large amount of data loss.

What commonalities exist?
Findings from the past year continue to show that target
selection is based more on opportunity than on choice. Most
victims fell prey because they were found to possess an
(often easily) exploitable weakness rather than because they
were pre-identified for attack.

Given this, it’s not surprising that most breaches were
avoidable (at least in hindsight) without difficult or expensive
countermeasures. Low levels of PCI DSS adherence highlight a
plethora of issues across the board for related organizations.

While at least some evidence of breaches often exists,
victims don’t usually discover their own incidents. Third
parties usually clue them in, and, unfortunately, that typically
happens weeks or months down the road.

Once again, this study reminds us that our profession has
the necessary tools to get the job done. The challenge for
the good guys lies in selecting the right tools for the job at
hand and then not letting them get dull and rusty over time.
Evidence shows when that happens, the bad guys are quick
to take advantage of it.

As you’ll soon see, we contrast findings for smaller and larger
organizations throughout this report. You will get a sense for
how very different (and in some cases how very similar) their
problems tend to be. Because of this, it makes sense that the
solutions to these problems are different as well. Thus, most
of the recommendations given at the end of this report relate
to larger organizations. It’s not that we’re ignoring the smaller
guys—it’s just that while modern cybercrime is a plague upon
their house, the antidote is fairly simple and almost universal.

If a third party vendor is handling the two items
above, make sure they’ve actually done them
Larger organizations
Eliminate unnecessary data; keep tabs on what’s left
Ensure essential controls are met; regularly check
that they remain so

Larger organizations exhibit a more diverse set of issues that
must be addressed through an equally diverse set of
corrective actions. We hope the findings in this report help to
prioritize those efforts, but truly tailoring a treatment
strategy to your needs requires an informed and introspective
assessment of your unique threat landscape.

Based on the feedback we receive about this report, one of the things readers value most is the level of rigor and
honesty we employ when collecting, analyzing, and presenting data. That’s important to us, and we appreciate your
appreciation. Putting this report together is, quite frankly, no walk in the park (855 incidents to examine isn’t exactly
a light load). If nobody knew or cared, we might be tempted to shave off some
time and effort by cutting some corners, but the fact that you do know and do
care helps keep us honest. And that’s what this section is all about.

“Organizational data breach” refers to incidents involving the compromise (unauthorized access, theft, disclosure, etc.) of non-public information while it was stored, processed, used, or transmitted
by an organization.

On the other hand, certain trends are almost certainly more related to turmoil in the
sample than significant changes in the external threat environment. As
in previous reports, the chosen approach is to present the combined
dataset intact and highlight interesting differences (or similarities)
within the text where appropriate. There are, however, certain data
points that were only collected for Verizon cases; these are identified
in the text and figures. This isn’t ideal, but
it happens.

Imagine, as a risk
manager, having access to all security incidents within
your organization classified using VERIS (if you really
want to let your imagination run wild, think about also
having similar data from other organizations like your
own). Over time, a historical dataset is created, giving
you detailed information on what’s happened, how often
it’s happened, and what hasn’t happened within your
organization. Unknowns and uncertainties begin to
recede. You give it to your data visualization guy who
cranks out a grid for your various business groups
similar to Figure 9. Hotspots on the grid focus your
attention on critical problem areas and help to properly
diagnose underlying ailments. From there, treatment
strategies to deter, prevent, detect, or help recover from
recurring (or damaging) threat events can be identified
and prioritized. But you don’t stop there; you actually

measure the effectiveness of your prescriptions to
track whether incidents and losses decrease after these
treatments are administered. Thus, you achieve a state
where better measurement enables better management.
Colleagues start referring to you as the “Risk Doctor”
and suddenly your opinion matters in security spending
discussions. This could be you.
Obviously, this is meant to be tongue in cheek, but we
truly do believe in the merit of an approach like this. We
like to refer to this approach as “Evidence-Based Risk
Management” (EBRM), borrowing from the concept of
evidence-based medicine. Essentially, EBRM aims to
apply the best available evidence gained from empirical
research to measure and manage information risk.
Security incidents, whether large or small, are a huge
part of that “best available evidence.” This is why we
assert that meticulously analyzing them is a highly
beneficial practice.

Entities that cause or contribute to an incident are known as threat
agents. There can, of course, be more than one agent involved in any
particular incident. Actions performed by them can be malicious or nonmalicious, intentional or unintentional, causal or contributory, and stem
from a variety of motives (all of which will be discussed in subsequent
agent-specific sections). Identification of the agents associated with an
incident is critical to taking specific corrective actions as well as informing
decisions regarding future defensive strategies. They are not nearly as frequent (one might even say “constant”) as
mainline cybercrime, but as will be seen below, they can be quite damaging.
We would be remiss if we did not point out that in 2011, there were several investigations involving internal agents
that did not meet the definition of a data breach. When insiders misuse access or information provided for their job
duties, but did not disclose information to an unauthorized party, then no loss of confidentiality has occurred.

With less than 1% of breaches caused by a partner, it will be hard to go anywhere but up in the next report. Similar
to insiders, the dramatic increase in external agents helps to explain this decline, but there are other factors as
well. Notice that the downward trend began in 2008, which precedes the major shift towards highly-scalable
attacks by outsiders. We have given several hypotheses in past reports, including increased awareness, regulation,
and technology advancements. More significant is how we define causal and contributory agents. Partners that did
not have a causal role in the incident are not included in these percentages. More discussion on such scenarios can
be found in the Partner and Error sections of this report.
It is also entirely possible that malicious insiders and/or partners are flying under the radar and thus avoiding
discovery. However, compromises of non-financial data do not have these mechanisms to
trigger awareness, and are therefore more difficult to discover.
A frequent example of this is a bank employee who uses system privileges to make an unauthorized withdrawal or transfer of funds. This is certainly a security violation, but it is not a data breach.
Some may rightly remember that the percentage tied to partners was substantially higher in prior reports. Keep in mind that those reports showed Verizon data separately, whereas this is the
combined data from all participating organizations “retrofitted” to historical data. It definitely changes the results.

It’s not the whole story, however. Nor is it the most important one. The most significant change we saw in 2011 was
the rise of “hacktivism” against larger organizations worldwide. The frequency and regularity of cases tied to
activist groups that came through our doors in 2011 exceeded the number worked in all previous years combined.
That is not to say that hacktivsm is new; the term has been standard lexicon since it was coined by the Cult of the
Dead Cow hacker collective in the late 90’s. Back then, it mostly consisted of website defacements, coordinated denial of service attacks, and other antics to express disagreement, obtain bragging rights, or “just because.” The
major shift that occurred in 2011 was that activist groups added data breaches to their repertoire with much heightened intensity and publicity. In other words, 2011 saw a merger between those classic misdeeds and a new
“oh by the way, we’re gonna steal all your data too” twist.
But even that’s not the whole story.
Why the disparity between the total records
stolen by professional cybercriminals versus

activist groups? Looking through the case data, it is apparent that money-driven crooks continue to focus more on
opportunistic attacks against weaker targets. This may be at least partly because a good number of their brethren
are enjoying jail time. Instead of major (and risky) heists, they pilfer smaller hauls of data from a multitude of
smaller organizations that present a lower risk to the attacker. Think of it as a way to streamline business processes.
Find an easy way to prey on the unsuspecting, the weak, and the lame, and then simply repeat on a large scale. This
high-volume, low-yield business model has become the standard M.O. for organized criminal groups.
An important observation before we close this discussion is that nearly all data stolen by activist groups were
taken from larger organizations. Furthermore, the proportion of breaches tied to hacktivism-related motives rises
to 25 percent. This stands to reason, since a low-profile brand is less likely to draw the ire of these groups.
Just like the security professionals with whom they contend, criminals are constantly assessing risk—the risk of
apprehension. One of the greatest challenges for law enforcement in the fight against cybercrime is merging a
criminal’s real world identity with their online identity. Unfortunately, across 10% of the 2011 caseload,
investigators were unable to identify a specific variety of external agent. There are several valid reasons for this.
First and foremost, many clients do not maintain sufficient log data that would enable attribution. In many cases,
the determination cannot be made through disk forensics alone. Many victims (for various reasons) do not wish to
expand the investigation to include this line of inquiry once the breach has been successfully contained. Sometimes
the perpetrator is able to erase his tracks or hide them among a host of intermediary systems. Every now and then,
just as we think we’ve correctly identified the assailant—nope! Chuck Testa (just look it up—it’s worth the break).

As is always the case, determining the geographic origin of external attackers based solely on IP address can be
problematic. Even if the country of the source IP addresses can be pinpointed, this may not be where the attacker
actually resides. It’s quite likely that it’s just a host in a botnet or another “hop” used by the agent. In some cases,
various types of additional data, such as those provided by law enforcement and/or netflow analysis, can help to determine the attacker’s true origin.
Either way, examining the geographic origin of attacks is valuable for a number
of reasons.

As discussed in the Threat Agent Overview section, the decline of internal agents as a percentage of our dataset is
due more to the continued rise of industrialized attacks than the demise of all insider crime. We hypothesize that
many insider crimes go unreported because the organization is unaware of them, or because they decide for political
reasons to handle it internally in lieu of calling for a third-party forensic investigation or referring it to law enforcement.
Nevertheless, when insiders do directly cause or contribute to a data breach, they do so in multiple ways. For our
purposes, we classify them according to three main roles. Insiders either acted deliberately and maliciously,
inappropriately (but not maliciously), or acted unintentionally. For the third year in a row, nearly all the internal
breaches were a result of deliberate and malicious actions (each year ~90%). It should be noted, however, that there

We hypothesize that many insider
crimes go unreported because the
organization is unaware of them,
or because they decide for political
reasons to handle it internally.

There are many ways that an insider may indirectly contribute
to an incident, but they are not considered a threat agent in such circumstances.

What we’re dealing with here are scenarios where insiders were the direct or primary cause of data loss within
their organizations.
The data is then passed up the chain
to criminals who use magnetic stripe encoders to fabricate
duplicate cards. Not surprisingly, such incidents are almost
entirely associated with smaller businesses or independent local
franchises of large brands.
On the other hand, when regular corporate end users are involved
(12%), their actions are quite different. In most instances, these
employees abuse system access or other privileges in order to
steal sensitive information. Almost all of the scenarios listed
above are motivated by financial or personal gain.
Outside of the varieties mentioned above, we observed a mixture
of executives, managers, and supervisors (totaling 18%). Like the

regular employees and end users, these individuals are also exploiting system access and privileges for personal
gain. For three years running, we have seen a decline in finance and accounting staff. Still, the daily responsibilities
of these folks, which involve the oversight of and/or direct access to valuable assets and information, put them in
a position to engage in a multitude of misdeeds. One can’t help but wonder what the data would show if we were to
track these types of insiders through the ever-changing regulatory landscape, from before Glass-Stegall, to
Graham-Leach-Bliley, and now to Dodd-Frank. The ebb and flow of these numbers would have been very interesting
to witness.
Finally, it might be negligent of us if we didn’t provide some mention of system or network administrators. These
trusty technological warriors help make IT organizations around the world hum with productivity, and they
oftentimes possess the proverbial “keys-to-the-kingdom.” Though we have seen cases in which they were
responsible for data breaches, they have barely registered more than a blip on the radar in the last couple of years.
We mentioned in an earlier section that we have analyzed the incidents for a single organization over the course of
a year. In these datasets, admin-related incidents occur frequently, but they are mostly of the availability and
downtime variety.

If the partner’s actions are the direct cause of the incident, they ARE a threat agent.
If the partner’s actions create a circumstance or condition that—if/when acted upon by another agent—allows
the primary chain of threat events to proceed, the partner is NOT a threat agent. We consider this to be a
conditional event, and the partner can be viewed as a contributing agent. Their actions had more to do with the
victim’s security or vulnerability than the threat itself.
If the partner owns, hosts, or manages the victim’s asset involved in an incident, it does NOT necessarily follow
that they are a threat agent. They may be (if their actions led to the incident), but they are not guilty simply by
this association. To further
illustrate what we mean, let us consider the following scenario.
Suppose a third party remotely administers a customer’s devices over
the Internet via some kind of remote access or desktop service.
Further suppose this partner forgot to enable or misconfigured a
security setting (let’s pick something no admin would ever do, like
neglecting to change default credentials). Then, lo and behold, that
device gets popped within 30 seconds of being identified when an

organized criminal group operating out of Eastern Europe guesses the username/password. All of this, of course,
is purely figurative; this would never actually happen in the real world (wink, wink). In such circumstances, the
criminal group would be the only threat agent. One could capture the partner’s [indirect] contribution using the
VERIS-specified role of “contributed to conditional event(s)” along with a suitable “Error” threat action. This
essentially notes that the partner created a vulnerability (the conditional event) that was exploited by the external
threat agent.
All in all, the assertion made for the last two years remains true: organizations that outsource their IT management
and support also outsource a great deal of trust to their chosen partners. A partner’s lax security practices and
poor governance—often outside the victim’s control or expertise—are frequently catalysts in security incidents.
Nevertheless, outsourcing can have many benefits, and the best way to counteract the associated risk is through
third-party policies, contracts, controls, and assessments. One caveat of outsourcing is that you can outsource
business functions, but you cannot outsource the risk and responsibility to a third party. These must be borne by
the organization that asks the population to trust they will do the right thing with their data.

Hacking and malware have traditionally led the pack, but this year they’ve
pulled away from the group even further while waving “Hi Mom!” to the
camera. Out of the 855 incidents this year, 81% leveraged hacking, 69%
included malware, and an impressive 61% of all breaches featured a
combination of hacking techniques and malware. Out of the 602 incidents
with two or more events, hacking and malware were used in 86% of the
attacks. Misuse and social tactics stepped up their
game in 2009 while physical techniques made a respectable appearance the year after that. The rather sharp drop
in physical attacks this past year may be due to global law enforcement agencies successfully flipping the freedom
bit on those involved with skimming incidents. They focused heavily on the criminal rings behind these skimming
activities rather than individual incidents themselves, and we may be starting to see the fruits of those efforts. Perhaps it’s
because enterprises have the IT staff to address some of the low-hanging fruit (or, what is often more apropos, the
fallen fruit rotting in the yard). Companies, big and small, saw a fair amount of malicious code designed to capture user inputs, commonly called
keyloggers—they were present in almost half of all breaches (48%). This most likely contributed to the use of
stolen credentials in roughly one out of three incidents. Another consistent threat action for large and small
companies was the installation (and exploitation) of backdoors.
Malware is any malicious software, script, or code developed or used for the purpose of compromising or harming
information assets without the owner’s informed consent.
Much as it has in the past, the most common malware
infection vector continues to be installation or injection by a
remote attacker. This covers scenarios in which an attacker
breaches a system via remote access and then deploys
malware or injects code via web application vulnerabilities.
Over the past few years, the data shows that this infection
vector continues on an upward trend. Attackers utilized this
vector in slightly more than half of malware-related cases in
2009, about 80% in 2010, and a staggering 95% in the past
year. Its popularity as an infection vector likely stems both
from the attacker’s desire to remain in control after gaining access to a system, and its use in high-volume automated attacks against remote access services. This is most
evident in the broader financially-motivated crimes (such as payment card breaches) where malware is not typically
the initial vector of intrusion, but rather is installed by the attacker after gaining access. This is not always true for
other genres of attacks. With IP theft scenarios, malware often provides the entry point after a successful social
attack such as a phishing e-mail. In both cases, good defense-in-depth controls, not just antivirus software, could
aid in keeping the attacker out in the first place.
When focusing on data compromise situations, e-mail is less common as an infection vector. Many organizations
employ antivirus products and other filtering mechanisms to successfully block or quarantine millions of malware
strains floating around the Internet. It is highly likely that e-mail would be a much larger vector if these controls
were revoked.
Infections via the web decreased again this past year in proportion to other vectors. We divide web-based malware
into two subcategories: code that is auto-executed (a.k.a. drive-by downloads) and software that the user needs to
execute (clicking on a malicious hyperlink). We realize that web-based malware results in countless infected
systems, but only a portion of those lead to confirmed data thefts.
For many web-based types of malware, a user is required to visit a certain infected website. This certainly works for
some scenarios, such as password-stealing Zeus malware, but not for large-scale compromises of payment
systems. Most of the infected systems appear simply to join the thousands of botnets used for DDoS and other types of attacks.
For larger organizations, the distribution of malware
infection vectors is less one-sided; the data shows higher
frequencies of web and e-mail infection vectors and lower
frequencies of malware installed directly by attackers. Our
leading theory for this shift is that attackers may find it
easier to get users to install malware rather than breach
the perimeter defense of larger organizations through a
direct attack. The amount of “unknown” infection vectors is
attributable to many different factors. Most often it is due

to a lack of evidence (no log data, anti-forensics by the attacker, and/or premature clean-up) on the system. In
these cases, it is known that malware was present, but the infection vector cannot be conclusively determined.
Malware Functionality
Of equal importance to the pathway of malware infection are the functions exhibited once deployed within a
victim’s environment. We mostly focus on malware that directly relates to the breach, but we often find all sorts of
extraneous malicious or unwanted files during the course of an investigation. This serves as an additional indication
of inadequately managed systems and a lack of security processes. Although malware frequently utilizes several
methods to harm a system, it still serves one or more of three basic purposes in data breach scenarios: enable or
prolong access, capture data, or further the attack in some other manner.
Per Figure 20, the three most commonly found functions of malware continue to be logging keystrokes (and other
forms of user input), sending data to external locations, and backdoors. It is important to note that none of these
functionalities are mutually exclusive and it’s common for a single piece of malware to feature several components.
As mentioned, keyloggers appeared in over two-thirds of malware-related cases, slightly more than the previous
year. These tools include commercially available software packages, which are freely available on the web, and for
which fully functional pirated versions can be found on peer-to-peer (P2P) networks and torrent sites. Some of
these keyloggers also allow the attacker to build a pre-configured remote installation package that can be deployed
on a target system. Their availability, ease of use, and configuration, as well as their anti-forensic capabilities, such
as hiding from a list or running processes, make them attractive for attackers to use.
The next most common functions relate to data exfiltration. In general, there are two ways for an attacker to
leverage malware to accomplish this. The first is programming the malware to send the data out of the victim’s
environment. This method was more prominent in last year’s report (79% compared to this one (43%). This can be
accomplished both in real-time (as soon as the data is captured), or it can be done in batches at certain intervals or
after certain actions (such as starting a program). It’s quite common to see this functionality bundled with
keyloggers, as shown in Appendix A.
The second method of exfiltrating data calls for the attacker to re-enter the network and retrieve it. If the attacker
doesn’t return via their original vector of intrusion, backdoors are a favored tool to remotely retrieve captured
data. In addition, this type of malware also allows full control of compromised systems, which can be used to install
additional malware, maintain persistence in the victim’s environment, use the system to launch further attacks, and
so on. Backdoors have consistently been one of the most common malware functions within our dataset for
removing data from the victim’s environments.

Sending data to an external site is more likely to be seen in malware that affects smaller
organizations. This is because these functions are typically bundled with malware seen in large-scale automated
attacks to which small and medium businesses are more prone. Large organizations, however, are more likely to see
attackers utilize backdoors to exfiltrate data. Larger organizations typically have stronger authentication and
access control in place for externally-facing remote services. Therefore, a popular way for an attacker to gain
access into a larger organization’s network is to get a foothold on an inside system through a backdoor and when
they have a foothold the attacker installs multiple backdoors to maintain access over a long period of time.
Another difference seen within breaches of large organizations is the use of certain non-malicious network utilities,
such as the SysInternals tools. Sometimes, system/network administrators use these tools for conducting normal
maintenance on a system. However, if an attacker places them on a victim’s system, we categorize them as malware.
Many times antivirus products do not rate these as malicious.
As mentioned in previous reports, we highly encourage organizations to run systems in a least-privilege mode,
monitor both ingress and egress points of their networks, and look for unauthorized changes to systems. Employing
these practices can provide evidence of foul play and could potentially limit the impact of a data breach or any
security incident. Additionally, look within your environment for indicators of compromise; seek out PSTools,
growing archive files such as ZIPs and RARs, and new files with hidden and read-only attributes.

Although it’s become passing fancy to kick dirt on the antivirus vendors, it’s also important to investigate and
follow-up on antivirus alerts. In many instances, the proverb “where there’s smoke, there’s fire” holds true. Detecting
and responding to malware quickly may be the difference between a malware infection and a full-blown data
breach. Multiple investigations have shown that incidents could have been stopped early if organizations followed
up more promptly on antivirus alerts.
Malware Customization
This year about one-third of the malware investigated in the Verizon caseload was customized, which is quite a
change from our last few reports. Malware customization is much more in line with statistics seen in our 20052007 caseload. Agents used customized malware slightly more against larger organizations, but not significantly
so. When customization is used, in large organizations as well as in the overall dataset, it typically involves malware
written from scratch by the attacker or involves the modification of existing code. We suspect that one of the main
reasons for the change in malware customization is the usage of commercial applications within “industrialized”
attacks. In these large-scale, multiple victim compromises, attackers simply don’t need to bother with customizing
malware since they can successfully use “canned” attacks against thousands of victims.

Hacking is considered all attempts to
intentionally access or harm information assets without authorization or in
excess of authorization by thwarting logical security mechanisms. Hacking
is advantageous as an attack method for several reasons. It is usually
conducted remotely, allowing attackers the benefits of anonymity and
scalability. Automated tools and basic scripting, often written by someone
else and made available to attackers, make many varieties of hacking
extremely easy to conduct, and allow for attacks against multitudes of potential victims.

Also, the re-appearance of “mega-breaches” this year has
shattered any correlation of breach and record loss percentages
that may have existed. Some techniques, varieties such as abuse
of functionality, were responsible for significant amounts of
compromised records in one or two incidents. This can be explained
either by the lack of available logs, anti-forensic measures taken by the attackers, or the limitation in scope of the
investigation based on client requirements.

The low-level and highly automated attacks are still used on a wide scale but yield a relatively low number of
records per attack. Typically these attacks involve smaller businesses mainly in the retail trade and hospitality
sectors. Also, there are few obvious distinctions between the methods used for hacking into small companies
compared to those utilized to compromise large ones. Larger companies do seem to be more adept at warding off
the easier-to-prevent attacks; however, approximately 98% of all records breached via stolen credentials occurred
in larger organizations.
As there’s typically not a great deal of data to harvest within
smaller companies, hackers revert to what could be
considered “army ant” tactics. In other words, they strip the
server clean of whatever data happens to reside there, but
do not bother to make a home for themselves. Larger
companies offer a lusher environment, and from the
attacker’s point of view, warrant a bit more investment. Like
sedentary ants, attackers build tunnels and backdoors to
easily and safely get to their pastures. As expected, such attacks require more technical proficiency—but typically don’t fall into the “advanced” category of attacks—and
yield more data in the long run.
Both brute force attacks and exploitation of default or easily guessable credentials were down from last year’s
report, but still endemic in the retail and hospitality industries—typically smaller businesses. Unfortunately it’s
still possible to go to a vendor’s site, get the client list and just hit those with the default or guessable usernamepassword combination. These are relatively easy attacks that require little in-depth knowledge or creativity. They
are usually scripted, aimed at many targets, and, if unsuccessful, exhibit little persistence.

In fact, the thief often doesn’t even know what he’s stolen until checking the remote server to which his scripts have
been sending the captured data. The targets simply are not worth much effort to the attacker, since few records
are stolen in such incidents. Scale of targets is what matters.
One particular case illustrates the lack of
individual attention that goes along with
most of these attacks. In this scenario, an
online FTP server that had been
misconfigured to allow anonymous FTP
access was under constant attack by
brute-forcing tools (like most online
systems). What was noticeable for these
scans was that many exotic usernamepassword combinations were attempted,
but very few tried to use anonymous access,
which would have gotten them in.

Luckily, larger enterprises typically have basic authentication and
access control configurations in place and enforce these policies
on both their personnel and vendors. This becomes apparent when
comparing the statistics for larger enterprises, where brute
forcing is found in only 14% of cases (compared to 29% of all
cases), and exploitation of default and guessable credentials is
present in less than 10% of cases (<1% of data loss). We still see
very few hacks exploiting patchable or 0-day vulnerabilities. As
long as authentication is easy to fake or bypass, we don’t expect
this to change.
Hacking Vectors

Remote access services (e.g., VNC, RDP) continue their rise in prevalence, accounting
for 88% of all breaches leveraging hacking techniques—more than any other vector. Remote services accessible
from the entire Internet, combined with default, weak, or stolen credentials continue to plague smaller retail and
hospitality organizations. Often these victims share the same support and/or software vendor. Scripted attacks
seeking victims with known remote access ports (TCP 3389, RDP or VNC), followed with issuance of known default
vendor credentials, allow for targets of opportunity to be discovered and compromised in an automated and
efficient manner.
Backdoors must be brought up yet again (we have reviewed the installation and utilization of backdoors earlier) as
an attack vector. A quarter of all hacking cases feature a device that has a backdoor installed at some point in the
event chain, typically post-compromise. Over 90 percent of known record loss is associated with attacks that
exploit backdoors.
Web applications remain the third most
common vector overall, and while they
realized a decrease in the percentage of
breaches, they were associated with over a third of total data loss. The inherent need
for many web applications to be Internetvisible makes them a logical target; the
potential to use them as an entry point into
a corporate database makes them an
attractive one.
When focusing on larger companies, there
are some obvious shifts in the attack
vector landscape. Most notable is that only
20% of the cases involve remote access services. Large organizations with more
mature security practices have done a better job of limiting the accessibility of these services and have
implemented better authentication controls. Backdoors are
slightly more prominent in large organization breaches and almost
all data loss can be associated with cases involving them. Web
applications abound in many larger companies, and remain a
popular (54% of breaches) and successful (39% of records) attack vector. As stated earlier, large companies have done a good job of
limiting Internet visibility so attackers will either go after what is available (web applications) or attempt to bypass
perimeter security with malware (backdoors, command and control).

The centuries-old tradecraft of exploiting people in order to further criminal aims is alive and well in the information
security field. The “carbon layer” of information assets (the user) is notoriously susceptible to social tactics such as
deception, manipulation, and intimidation, and savvy threat agents know how to use this to their advantage. Even
though humans are the most complex creatures on earth, criminals have consistently been able to outwit or
otherwise leverage them in the process of stealing data. Although a little less common in 2011 (down from 11% in
the previous year), the amount of data stolen during incidents employing social tactics rose dramatically from less
than 1% to 37%—the highest in the DBIR’s history (though it must be acknowledged that most of this ties to one
of those “mega-breaches” we spoke of earlier).

There are countless ways that imaginative and resourceful criminals can utilize pretexting, but
the ploy most observed targeted a string of small business owners over the phone by posing as
support services for the payment card brands.

The “carbon layer” of information assets
(the user) is notoriously susceptible
to social tactics such as deception,
manipulation, and intimidation, and
savvy threat agents know how to use
this to their advantage.

Solicitation, on the other hand, was down markedly
from 74% of all social tactics in 2010 to 29%.
Quite a few
arrests were made last year that busted up some of
the more prolific crime rings behind such activities.

Phishing was relatively stable in the past year. This is fairly interesting, and in line with many
recent media reports detailing the use of malware-baited phishing lures cast toward some bigger and well-known
enterprises. We believe this is a strategy designed to circumvent the typically more mature security measures in place
at larger organizations. Why spend time searching for a way to exploit the specific technologies and weakness of a
single company when every company contains people with the same basic vulnerabilities? The good ol’ telephone served as the medium for nearly half of these cases. This coincides
with the earlier observation of utilizing pretexting for extracting information from unsuspecting victims. Coming
in second place were in-person tactics, typically used for soliciting waiters and cashiers within small businesses.
It should come as no surprise to learn that e-mail and phone share the top place as the most common vector of
choice for orchestrating social tactics against larger organizations—this concurs with higher levels of pretexting
and phishing against them. We observed attacks in 2011 that appeared to specifically target e-mail addresses at
larger (or well-known) enterprises for use in subsequent attacks and targeted phishing campaigns. In some of
these, the time separating the theft of e-mails and leveraging them in another attack was quite short. Law
enforcement information shows some are offloaded to fill an order
from a generic phisher, while some are used by the original thief for more directed or specific purposes.


Misuse is defined as the use of entrusted organizational resources or privileges for any purpose or in a manner
contrary to that which was intended. These actions can be malicious or non-malicious in nature. This category is
exclusive to parties that enjoy a degree of trust from the organization, such as insiders and partners. In 2009,
Misuse was the most common of all threat actions, but dropped substantially in 2010 (48% to 17%). In 2011 the
percentage of breaches involving misuse continued to decrease. For the entire caseload, misuse was found to be
involved in only 5% of cases and was responsible for less than 1% of compromised records.
Although the percentage of cases declined substantially, the categories of misuse we did encounter are quite
similar to what we have seen in past years. Embezzlement, skimming, and related fraud, for instance, topped the
list with regard to frequency, appearing in 57% of incidents that involved misuse. Despite this relatively high
percentage, the amount of records stolen in this manner was almost nil. Use of unapproved hardware and devices
came in second at 50%. These two actions mostly consist of payment card handlers (cashiers, restaurant servers)
using small handheld skimmers to capture data (though scenarios involving other kinds of rogue devices occur as
well). Finally, abuse of system access and privileges came in third with 43% of all misuse. As you may recall, these
have been the top three for the last two years, only in a slightly different order (e.g., last year: Embezzlement/
Skimming—75%, Abuse of system access—49% and Use of unapproved hardware—39%). We anticipate that we
will continue to witness these three varieties, as they correlate with financially motivated insiders with physical or
logical access to valuable information.

This year we began to record a new data point: the path or vector involved in misuse. While we realize that one year
of tracking a few cases will not enable us to provide much in the way of conclusions, we hope that over time it will.
Our aim is to ascertain where criminals are perpetrating their crimes so that corrective measures may be taken to
prevent them from being successful. Based on our data, about half of all misuse was carried out through physical
access within the corporate facility. This is intuitive, and is also closely related to the high amount of embezzlement
and skimming crimes seen in our caseload. Interestingly, internal network access and remote access (e.g., VPN)
show identical numbers as a vector of misuse (21%). It’s interesting to note that these results do not seem to
support the common concern that working from home is riskier than working from within the corporate perimeter.
It will be interesting to see if the same results hold true in future samples.

In spite of all the warnings given by us and other industry insiders, we still see a number of organizations that fall
victim to data theft through the malice of disgruntled ex-employees. In VERIS, these are classified as external
agents (because they are no longer insiders) but when they are able to breach the victim because their privileges
were never actually revoked, it still constitutes misuse. As we have said before, if you don’t want these folks on your
payroll or representing your company then DON’T LET THEM RETAIN ACCESS TO YOUR SYSTEMS.

The physical threat action category encompasses human-driven threats that employ physical actions and/or
require physical proximity. Prior to the 2010 caseload, physical attacks were historically one of the least prevalent
and least damaging threat actions. We attribute this to the nature of many physical actions, such as stolen user
devices, which often result in data-at-risk rather than confirmed data compromise. Also, if physical access to a
device is available as part of an insider’s normal job duties, then it is classified under Misuse in VERIS rather than
Physical (see the Misuse section and the discussion around embezzlement).
Beginning in the 2010 caseload, however, widespread ATM and gas pump skimming operations were responsible
for a significant increase in physical attacks by percentage of breaches (29%) and compromised records (10%).
Organized criminal groups usually conduct these operations, and one “spree” can hit numerous separate locations
and/or businesses. These cases fall into a category that would not typically be pursued by Verizon investigators,
but certainly fall under the jurisdiction of USSS and other law enforcement agencies.

Accounting for only 10% of the combined caseload, the statistics
this year show a decline in the percentage of incidents involving
physical actions, including skimmers. The question becomes: “Was
last year an outlier?” The answer: “Probably not.” There are
explanations behind the drop in the percentage of incidents and
data loss associated with physical actions.
There was no shortage of payment card skimming in 2011, and
there were many notable arrests. “Operation Night Clone” is an
example of how large and far-reaching skimming groups can
become.
Also, difficulty in quantifying record loss associated with payment card skimming contributed to this decline since
our last report. Skimming attacks do not target data in storage and when card numbers are totaled, it is often
based on the amount used in fraudulent activity. This year featured more skimming cases where a confirmed
incident was established, but the specific number of payment cards compromised was not known. However, the
biggest reason for the lower proportion of data loss associated with physical attacks is simply the massive increase
in the total amount of data stolen (by other threat actions) in the 2011 dataset.
Physical threat actions observed over the past year were limited to tampering, surveillance, or theft. Tampering is
a physical action type defined as unauthorized altering or interfering with the normal state or operation of an
asset. Tampering was found in every physical breach this year. Installation of skimming devices falls under this
action and is responsible for the majority of tampering cases. Another form of tampering is operations where
organized criminal groups swap legitimate PIN entry devices and Point of Sale (POS) terminals with “new” devices. These devices are identical in appearance and designed to continue to perform their intended functions, but they
are also redesigned to capture payment card data. They discreetly collect input from the swipe reader and/or the
PIN entry keypad. Next to tampering, surveillance was the second-most common action. While the definition is
much broader, all forms of surveillance observed in this dataset were associated with the installation of pinhole
cameras to capture and transmit users’ entry of their PINs on ATM machines. Used in conjunction with skimming
devices, they are present in 35% of physical incidents, double the previous year (17%). In this context, theft (3%)
refers to physically taking a device. Obviously, everything in this report falls under the label of “theft,” but this
specifically addresses forms of physical theft. All physical actions took place in public indoor or outdoor areas
where, as one would expect, most gas pumps and ATMs are located.


Instead of trying to judge whether every
bad decision, poor practice, or mental
lapse deserves the label of “error,” we only
focus on critical errors that were either
the primary cause of the incident or a
significant contributing factor to it (in
other words, error as a threat rather than
a vulnerability).

The environmental category not only includes natural events such as earthquakes and floods, but also hazards
associated with the immediate environment or infrastructure in which assets are located. The latter encompasses
power failures, electrical interference, pipe leaks, and atmospheric conditions. As in the last few years,
environmental hazards did not factor into any breach scenarios across our combined dataset. There is, however, a
public example of an environmental action that contributed to both the loss of confidentiality and availability this
past year.

The hospital’s strategy to move to Electronic Health Records with a backup site outside of their geographical area
appears to have paid large dividends with regard to their recovery.


This section offers insight into the types of information assets compromised by breaches during 2011. This is an
important piece of the A4 model because it focuses on the target of the agents and actions already discussed in
this report. It turns the conversation from an external “Who are they and what are they doing?” to a more personal
“What do we have that needs protecting and what harm might arise if we fail?”

They store and process gobs (technical term) of data, and that fact isn’t lost on data thieves. This point is even
clearer when one considers the total amount of data stolen when servers are involved versus other asset categories.
The running comparison is shown in Figure 28. If you’re wondering about the dip in 2010, we attribute this to the
lack of “mega-breaches” during that year (a topic we discussed extensively in the 2011 DBIR). Almost all incidents
in which very large amounts of data are compromised involve servers. Since we observed no such incidents in 2010,
data loss was a little more distributed. As a testament to this, Figure 35 in the Compromised Data section reveals
a remarkably similar pattern.
We all know, of course, that user devices store and process information too. Furthermore, most organizations have a
lot more of them than they do servers, and they’re often widely distributed, highly mobile, less restricted, and—perhaps
more importantly—controlled by end users (a shudder travels down the spine of all the admins out there). For all of
these reasons and more, user devices frequently factor into data breaches in some manner or another and contribute
to a hefty chunk of overall data loss.

Because people know things that should not be disclosed and do things (e.g.,
to other people or assets) that could contribute to disclosure (or some other form of harm), they must be protected.
It’s worth noting that people contribute to 20% of incidents and 36% of data loss in larger
organizations. Moving on, the percentage of breaches involving offline data declined for the third year in a row
(from 25% to 12% to 3%). We believe this relates to another trend during that timeframe in which insider misuse
has also seen a decline; such activities often targeted documents, media, and other things scattered about the
workplace. Attributing some of this drop to the “green initiative” is probably a stretch, but it does give us opportunity
to contribute to a healthier, happier planet by reminding everyone that less printing leads to fewer stolen documents
(which, of course, begs the question of whether databases are more secure than paper, but never mind that). Finally,
the following explanation should clarify why “networks” are tied to so few breaches. It’s absolutely accurate that a
huge proportion of breaches infiltrate networks. However, we’re specifically referring to network infrastructure
devices like routers, switches, cabling, etc. Traffic might pass through these devices during a breach, but the
devices themselves are not compromised.
Table 10 gives a more detailed accounting of the kinds of assets compromised across the 850+ breaches we
studied. Calling out larger organizations from the rest of the dataset is particularly helpful here. POS servers and
terminals account for a large share of incidents across the whole dataset, but are
basically a non-factor for larger organizations. Intuitively, web applications and
databases play a much larger role at the enterprise level.
Some readers may find the lack of mobile devices like tablets and smartphones in
Table 10 surprising. We confess that we also expected a stronger trend to emerge, but
so far it has not. To be clear, we have conducted forensic engagements on mobile
devices (for malware, suspected misuse, tampering, etc.), but confirmed data
compromises remain rare. We can’t help but think, however, that given the explosion of
mobile users, applications, payments, etc., things may pick up in the future.
BYOD is a model of allowing
employees to use their personal devices for work purposes. This notion is both scary and attractive at the same time.
It could boost employee happiness while saving money and administrative overhead, but it also exposes enterprise
data and systems to devices that may not conform to enterprise security policies (and that’s being gracious). Do we see breaches that compromise assets in an externally-hosted
environment that is not managed by the victim? Yes; absolutely. Do we see successful attacks against the
hypervisor in the wild? No; not really. We’ve said it before, and we’ll say it again here: it’s really more about giving up
control of your assets and data (and not controlling the associated risk) than any technology specific to the cloud.
That a question is difficult to answer is not an excuse to avoid asking it or attempting to answer it. In thinking about
how to study the relationship between data breaches and the cloud, our approach has been to identify traits
inherent to the cloud, and then to observe how those traits coincide with breaches. Some will consider it oversimplistic, but ownership, hosting, and management are three important characteristics of cloud-based assets. An
asset that is “in the cloud” is often one that you do not own, is hosted in an external facility, and is managed or
administered by a third party (or some combination of these).

We all know that correlation is not causation, but we also know that patterns can be pertinent.Whether these particular patterns are relevant is unclear from the data before us, but
we can make some observations. For one, this marks the third year in which we’ve seen an increase in the proportion
of externally-hosted and managed assets involved in data breaches. That may be important, especially if you crossreference this with issues we bring up every year regarding third-party POS vendors that can’t seem to figure out
how to configure an access control list or change default credentials. It’s also worth nothing that assets within
larger organizations were more likely to be internally hosted and managed (around the 80% mark for both).
Employee-owned devices were rare, but if the BYOD movement catches on, we should see that begin to rise.

Over the years, we’ve received feedback questioning the value of counting the number of records compromised in
this report. After all, not all records are created equal. Should a record containing a name and e-mail address be
counted on the same scale as the source code to a flagship product? This is a valid concern and we encourage the
reader to consider this when promoting or using this number in derivative works. Nevertheless, there is value in
tracking the number. First, it is a number required by some third parties (e.g., payment card brands) and counts help
keep perspective within the specific record types. Plus, we do suspect there is a relationship between number of
records lost in a breach and the impact of the breach (at least for certain data varieties), and we won’t be able to
know if we don’t collect the data and ask the questions. Finally, it acts not only as a kind of “wet finger in the wind”
measurement, but the variety and amount of data loss also contributes to our understanding of motivation and
activities of the attacker.
So, long story short, the “mega-breach” is back after a one-year hiatus, albeit with new types of threat agents and
motivations associated with them. In lieu of focusing first on the really big scary number of compromised records
(174 million, by the way), we will start by comparing the number of incidents and amount of losses associated with
each variety of data. The distribution of record loss, the inherent imperfections around quantification of
compromised records, and comparisons to prior years are also reviewed in this section.

Payment card information was again involved in more breaches (48%) than any other data type, but unlike previous
years, it was not a runaway winner. Authentication credentials were a close second (42%) in the current dataset.
This may allow payment cards to retain the title of “most stolen” but the title of “largest hauls” now belongs to the
personal information variety, which includes name, e-mail, national IDs, etc. While only 4% of breaches included the
loss of personal information, it comprised 95% of the records lost in this report. This is an enormous change from
previous years; 4% of record loss in 2009 involved personal information and 2010 showed only 1%.

It is intriguing that personal information was stolen en masse by both activist and organized criminal groups in
2011, but for very different reasons. The former seek this kind of data from victims because it makes a statement;
it’s embarrassing and it’s…personal. The scale of the breach, sensitivity of the data, and the manner in which it is
released are far more important to their goal than the actual value of the stolen data.
Yes, the mega-breach is back and they are occurring, as one might predict, in larger organizations. It should be no surprise that large organizations with more data have bigger breaches. In this year’s data, organizations
with over 1000 employees account for all breaches of more than 10 million records and over two-thirds of all
breaches of more than one million records.


The second disclaimer is that, in some cases, it’s pretty dang hard to do.
The struggle to count stolen records has several causes, some as a result of attack methods, some based on the
data type involved, some due to lack of log collection by the victim. We (and our contributing partners) had many
cases in which the existence of a breach was clear and evidence was found of data being exfiltrated
(e.g., finding an encrypted payload file), but determining the record type and quantity proved difficult or impossible.
Another common example is with organizations that suffer payment card breaches, but only the cards used in
fraudulent activity can be confirmed and the total number of payment cards stolen remains unknown. Some types
of information, such as trade secrets, are inherently difficult to quantify. Breaches of certain data varieties are
more difficult to discover, as they don’t come with the luxury of fraud detection to provide the victims with
notification. Moreover, our law enforcement partners focus more on tracking and capturing the attacker than they
do on counting an exact number of records lost with
each and every victim. All but one of the large breaches (over 1M records) this year were
attributed to activist groups rather than financially-motivated agents. This supports the theory that organized
crime is quietly targeting the softer, smaller targets while activists are waging more of a “shock and awe” campaign.

The best breach is the one that never occurs in the first place. Preventing a breach during the initial compromise
would obviously have the least amount of repercussion. However, security controls are neither absolute nor binary;
they range from quick-fixes all the way up to complex-and-costly. Understanding how much to allocate requires an
understanding of the pressure the attacker is applying and where, during the attack, more pressure is used.
To explore this notion in a bit more detail this year, we opted to alter our usual assessment of the difficulty of the
overall attack. We’ve provided separate ratings for the initial compromise and the subsequent actions that follow
it. The initial compromise refers to any actions leading up to the attacker gaining unauthorized access to an asset,
whereas subsequent actions covers anything done after that, including the compromise and exfiltration of data.
We’ve often said the latter phase is usually more sophisticated, but we wanted to put our numbers where our
mouths are.
Gaining access is at least
moderately difficult in just under a quarter of incidents, but close to double that for subsequent activities. By the way,
don’t let the “unknowns” be a distraction; sometimes logs are sparse and the exact techniques utilized simply aren’t
clear enough to assess their difficulty.
If one refers back to the list of common hacking varieties in Figure 21, this finding makes a lot of sense. Many of the
methods listed there—especially those tied to first gaining access—are not particularly sophisticated. What occurs
after that initial entry in a typical data breach scenario is where things usually get a bit more interesting. The assailant
will often install various types of malware (which may have some
degree of customization), escalate privileges, setup remote entry
and control mechanisms, and explore the victim’s network to find
where the goodies are stored. And find them they do. As depicted in
Figure 37, an astonishing 99% of all data was compromised via
moderate to difficult post-infiltration methods. This statistic
underlines our original assertion of the critical importance of
preventing access in the first place.
So, what about larger organizations? Surely they’re a lot more difficult
to infiltrate, right? Sadly, our data seems to suggest otherwise; it
does not appear that cybercriminals have to work much harder to
compromise larger organizations than they do for smaller ones.

So, what about larger
organizations? Surely they’re a lot
more difficult to infiltrate, right?
Sadly, our data seems to suggest
otherwise; it does not appear
that cybercriminals have to work
much harder to compromise larger
organizations than they do for
smaller ones.

Subsequent actions, however, did slide a bit more toward the difficult end of the spectrum, with about three of four
in the moderate to high difficulty rating. This is probably the result of a more diverse range of assets, better internal
defenses, and (typically) more mature detection and response capabilities.
As stated at the beginning of this section, rating the difficulty of attacks does involve a degree of subjectivity. However,
it still serves as a useful indicator of threat agent capabilities and what must be done to protect against their attacks.
The most effective and efficient approach is almost always to stop assailants before they get in the door. Most
opportunistic criminals (and in the next section we’ll see that most breaches are opportunistic) will not expend their
resources on a hardened target while a softer one of similar perceived value is available. If information is highly desired—
or targeted for other reasons—covering the basics, while still essential, may not be enough. Thus, understanding the
motives, skills, and methods of our adversaries is important to any well-considered and well-prepared defense.

It’s commonplace in the information security industry to classify attacks into two broad categories: opportunistic
and targeted. Opportunistic Attacks are when the victim isn’t specifically chosen as a target; they were identified and attacked
because they exhibited a weakness the attacker knew how to exploit. Targeted Attacks occur when the victim is specifically chosen as a target; the attacker(s) then determines what
weaknesses exist within the target that can be exploited.

Analysis of the data showed the attackers not only had no routine workweek, but they only worked an average of three
days a week. During one particular three-day work week, they punched the clock on Saturday, Sunday, and Monday.
They compromised 22 organizations across nine countries; Monday was the most productive, with 15 confirmed
breaches registered that day (in purple). We would joke about “nice work if you can get it” but the jail time these guys
are facing doesn’t make for very nice work at all.
To veteran incident responders it will come as no
great surprise when we assert that the timespan of events in breach scenarios can vary greatly depending upon a
multitude of factors. As with any other scientific endeavor, the longer one studies these factors, the more the
model designed to capture them will evolve, along with our understanding of it. In past DBIRs, we separated events
into three major phases, which we believed aligned quite well with the typical incident response process. Even so,
we decided to give the timespans a bit of a tweak in this iteration. By splitting up the former “point-of-entry to
compromise” phase, we hope to draw out a few more details around the initial compromise and data exfiltration
events.
This initial phase depicts the span of time from the first malicious action taken against the victim to the point at
which a security attribute (e.g., confidentiality) of an information asset is negatively affected. This is not necessarily
the culmination of the incident; it often represents the initial intrusion into the network. Please note that this phase
is only applicable to deliberate and malicious actions by agents who did not already have access/privileges to
the asset.
Keep in mind that the previously-published statistics for “point-of-entry
to compromise” are not comparable here. Isolating the time it takes to gain unauthorized access highlights with
startling clarity the speed at which a great many attacks occur. This result is largely (but not exclusively) the
byproduct of the many automated, quick attacks against smaller organizations in the 2011 caseload. It just doesn’t
take that long to pop a POS system using a scripted list of known usernames and passwords.

The second phase covers the time period from when the first asset is negatively affected to the point when nonpublic data is removed from the victim’s environment. One important distinction to note here is that the statistics
presented for this phase are solely based on Verizon’s 2011 caseload. Just as criminals are quick to get in the door,
the data shows that not a great deal of time typically passes before the loot is in hand. Well over half the time, the
victim’s data was removed within hours of initial compromise. This drops to 25% for larger size organizations, but
we’re not quite ready to start patting them on the back yet. It’s entirely likely that the difference is simply because
a little more searching (and thus time) is required to find the loot rather than the existence of ironclad controls
preventing data loss.
If we can say something positive about these findings, it is this: In over 40% of incidents we investigated, it took
attackers a day or more to locate and exfiltrate data. This gives some hope that reasonable time exists for more
than one shot at detecting/stopping the incident before data is completely removed from a victim’s control.

This phase deals with the span of time from when the first asset is negatively affected to the moment when the
victim learns of the incident. It saddens us to report that, yet again, breach victims could have circumnavigated the
globe in a pirogue (not from the bayou? Look it up.) before discovering they were owned. In over half of the incidents
investigated, it took months—sometimes even years—for this realization to dawn. That’s a long time for customer
data, IP, and other sensitive information to be at the disposal of criminals without the owners being aware of it.
Nothing to see here folks.
Let’s keeping building on that.

It’s important to remember that incident response speed and execution are equally critical. Acting fast just for the sake
of speed increases the risk of making mistakes, resulting in higher costs or needlessly extending the time necessary
for full incident mitigation. Being effective in this respect does not mean having “x” number of incident responders on
staff. Rather, it means the mechanisms are in place to properly initiate the response, classify the incident, and make
timely decisions regarding when to engage external assistance or involve law enforcement. It means the practical
elements—like the network diagram—are prepared and ready, or can be prepared when necessary. It also means
working jointly with all parties involved towards the shared goal of containment. If this sounds like common sense,
that’s because it is. Nevertheless, organizations often fret over a myriad of possible bad outcomes rather than rolling
up their sleeves and getting the incident resolved.

Aside from each of the timespan metrics just discussed, the methods by which breaches are discovered account
for some of the most interesting and sobering figures we have in our report. The time to discovery is intimately
bound to the method of discovery, as some methods inherently take longer than others.

For internal discovery we further categorize methods as being active (those specifically designed/deployed for
detection) or passive (those in which awareness of the incident arises from non-security processes).

We attribute this to a couple of different factors, the foremost being the demographics of our dataset. Smaller
businesses typically lack both the knowledge and the sophisticated technology required to monitor such events.
Instead, they usually work along the lines of the well-known maxim, “If it ain’t broke, don’t fix it.” And if there ain’t nothing
to say it’s broke, it don’t get fixed. The second reason is to some degree a function of the previous factor and to some
extent is a by-product of the role law enforcement partners play in our dataset. We will discuss this in more detail in the
external discovery section below.
When we compare the breach discovery results of the general population with those of large organizations, a very
different picture emerges. Figure 44 illustrates that breaches discovered by external parties (49%) are substantially
lower, while internal methods of discovery are much higher (44%) compared to the overall dataset. We have said in the
past that larger organizations have the knowledge, capabilities, and resources to discover security breaches more
effectively (if they would only use them), and these results bear that out.

Another sobering thought with regard to CPP is that its effectiveness is
predicated upon the ability to correlate certain fraudulent activities and
patterns. Other types of valuable information, such as intellectual property or
other sensitive organizational data, can be harvested from numerous sources
but go completely unnoticed.

Notification by law enforcement was once again one of the main methods of breach discovery. In fact, it takes the top
spot this year over the usual champion, third-party fraud detection. We believe one of the reasons for this increase is
the larger role that law enforcement plays in the monitoring of criminal groups and their wayward activities (see
Appendix B.). In such circumstances, law enforcement has knowledge that an attack or breach has occurred and can
therefore notify the victim before fraud patterns emerge via common point of purchase (CPP) algorithms.
While we are on the subject of CPP, it should be noted that third-party fraud detection continues to be one the
strongest methods of breach discovery. The advantage of CPP lies with its ability to see a broader pattern emerge
across many organizations that are often dispersed over a large geographic area, thereby providing a more
comprehensive vantage point than any single organization could possibly provide. Despite this, CPP is still an
after-the-fact alert, and its success points to the hard-to-hear truth that fraudulent activity triggers detection
better than many of our tools designed to do so before data loss or fraud ever occurs. Another sobering thought
with regard to CPP is that its effectiveness is predicated upon the ability to correlate certain fraudulent activities
and patterns. Other types of valuable information, such as intellectual property or other sensitive organizational
data, can be harvested from numerous sources but go completely unnoticed. Thus, we believe the numbers
surrounding non-payment card breaches are far worse than reported since there is no CPP-like mechanism to
detect their loss.
External breach notification methods are much different for large organizations. While notification by law
enforcement was the second most seen, at 10%, it was still far lower than that of the overall dataset. In most cases
for large organizations notification occurred when the thief made the disclosure known. Perhaps we should create
new breach discovery classifications of “YouTube,” “Pastebin,” and “Twitter” for the 2013 DBIR? (Of course, we’re
joking (sort of), but it is quite important to understand the role social networking plays in breach discovery, but also
in how attacks are initiated using these tools. Perhaps we’ll follow up with a blog post another time.) An interesting
“what-if” scenario would be whether or not these organizations would have discovered these breaches through
some sort of internal breach discovery method. In many cases, there is little evidence suggesting they would.

Internal active discovery relates to IDS/IPS/HIPS, log monitoring, anti-virus, and other like technologies that
organizations typically use to prevent, detect, and respond to data breaches. Unfortunately, as referenced in
several places, many smaller organizations do not have the awareness, aptitude, funding, or technical support to
perform these tasks on par with the rapidity and/or sophistication of the threats they face.
Nevertheless, although large businesses are better at utilizing these
investments compared to the overall dataset, they still have a difficult
time arriving at any significant return. In 8% of breaches affecting large
organizations, it was basic log-review and analysis that topped the
internal active discovery list. Readers will probably already be aware
that this is one of the methods that we tout yearly, and believe to be more effective than nearly all other methods. How do we know this? Well,
when we conduct an investigation, that’s how we find the breach—
reading the logs (see Figure 46). We are also long-time proponents of
searching for haystacks instead of needles within logs.
We have seen other very useful compromise indicators
such as log file line count, log file line length
(particularly good with SQLi in the URI), spikes in
traffic types (e.g. ssh, FTP, DNS, or anything atypical to
the organizational norms), number of events, country
of origin of IP connection (or by protocol), bandwidth
usage, and e-mail messages sent/received. The list is
as long as the limits of the imagination. The really
interesting thing about this type of monitoring is that
it doesn’t take a ton of cash to implement an effective
solution. It can be done with a few commands on a
Linux or Windows system.

We often use what we refer to as the “Sesame Street”
method of discovering a data breach. If you have kids,
or have been a kid at some stage in your life, or, like
some we know—never stopped being a kid, you may
recall the Sesame Street song titled “One of These
Things is Not Like the Other.” In our role as
investigators, we often employ this method when
searching for evidence of a security incident. To work
smarter rather than harder, we stopped looking for the
needles, and began looking at the haystacks. If one of
the haystacks is “not like the others,” then that’s a good
place to start looking for needles.
When we present the data depicted below, we often ask
the audience, “Can anyone see something anomalous
with this chart?” After the chuckling subsides, we
illustrate that the chart could be made the size of a
postage stamp (hopefully we aren’t dating ourselves
too much here) and one could still immediately notice
“one of these things is not like the other.” The particular
example below shows log file size, but this type of
graph could represent any number of artifacts.

All of this is, of course, obvious with glorious 20/20
hindsight, but that’s a privilege of our job!

Internal passive discovery is best described as when someone or something that is not responsible for security
reports the signs of an incident. Typically, this involves a regular employee who, in the course of their daily
responsibilities, notices something strange (e.g., slower system performance). More important than what they
notice is that when they witness strange circumstances they make the conscious decision to ACT on it. Unfortunately,
our investigators commonly learn through interviews that strange activity was noticed on a system, but nothing was
said or done. Usually, the employee states that either they didn’t know what to do, or they didn’t feel it was important.
Organizations that properly train personnel to recognize the tell-tale signs of a security incident and how to respond
will reap the side benefits of having an army of roaming incident-detectors working for them. Last year, we compared
this to the wonderful gift of free beer; we’ll appeal to a wider audience this time and go with free wine.
Based on these results, it would appear that the wine flows more freely in larger organizations. 16% of their
incidents were witnessed and reported by employees. This makes sense, as larger organizations have more security
awareness campaigns due to the requirements of various compliance regimes, and training is often conducted
annually. On the other end of the spectrum, the typical store clerk will not usually notice strange and unusual
circumstances unless it hampers their Bejeweled game on Facebook. Although it may not keep employees from
clicking on links and opening attachments from unknown senders, it can help an organization learn about a potential
security incident faster if their employees recognize that something is amiss and react.

The obvious problem with identifying anti-forensics techniques is: if they are effective, how would we know?
Nonetheless, our investigators often do identify anti-forensics techniques in the field. And when they do, they
make a note of it.
We still believe this represents a very
conservative low-end estimate of the prevalence of antiforensics. Naturally, these techniques vary in nature and
purpose, but they’re all centered on erasing, hiding, or
corrupting evidence in some way or other. Specific
common examples include wiping or tampering with logs,
encrypting compromised data so that it cannot be
examined, and rearranging the crime scene to mask the
obvious fact that Mrs. Peacock killed Colonel Mustard in
the study with the candlestick (just seeing if you’re awake).

The obvious problem with identifying
anti-forensics techniques is: if they are
effective, how would we know? Naturally,
these techniques vary in nature and
purpose, but they’re all centered on
erasing, hiding, or corrupting evidence
in some way or other.
Somewhat surprisingly, the use of anti-forensics does not
seem to be markedly different for larger organizations.
Anecdotally, however, it seems that criminals don’t
discriminate when it comes to the application of antiforensics techniques. The obfuscation of evidence is just
as much at home in “APTs” as it is in scareware.
Timestomping, packing, encryption; these techniques are
the rule, not the exception.

Building a timeline of system events is one of the
primary tasks of any forensics investigator. Stomping
all over that timeline is one of the most common
anti-forensics techniques out there. We are all
familiar with the standard modified, accessed, and
created timestamps. However, in the NTFS file
system there are a number of other timestamps
associated with every file. By understanding how
these timestamps work we are better able to identify
when timestomping is occurring.
In NTFS a file is a collection of attributes indexed
by a Master File Table (MFT). The timestamps we
all know and love can be found in the $STANDARD_
INFORMATION attribute. In this same location is a
fourth timestamp: the MFT entry modified
timestamp. All four of these timestamps are
accessible via NtQueryInformationFile in the
Windows API. Code that is stomping on these
timestamps will likely be linking NtSetInformationFile.
Another attribute that can be found associated with
an NTFS file is the $FILE_NAME attribute. This
attribute is associated with particular hard links of a
file, and each comes with its own unique set of all four
timestamps. These timestamps are not accessible
(or directly modifiable) via the Windows API.
The crux of this discussion is that by accessing the
standard Windows API, an attacker with sufficient
privilege can modify all of the timestamps in the
file system to effectively render timeline analysis
pointless. In the physical world, it would be akin to
a hurricane blowing across your crime scene.
Timestomping is not game over for a skilled
investigator, but it is certainly a significant setback.

The Payment Card Industry Data Security Standard (PCI DSS) is a set of
control requirements created to help protect cardholder data. Every
year Verizon’s caseload contains a number of organizations that are
required to adhere to the PCI DSS. Because these are confirmed data
breach victims, obvious questions arise with respect to the compliance
status of these organizations. This section examines this important topic

Another interesting parsing of the dataset revealed that there was a noticeable difference in the way the PCI DSS
compliance picture looks if you were to carve the 96% into two size categories: a) all companies in the entire dataset,
and b) companies of 1,000 or more employees. The majority of Level 4’s discussed earlier fall well below the 1,000
employee mark. However, looking at the larger organizations you find that the number of “Not in Place” items using
the same measurement as above drops from eight to only three. What might we be able to infer from this?
Possibly that larger organizations are doing a better job at complying with the standard (or implementing security
fundamentals in general) and thus leaving themselves less exposed and subjected to being breached. This would
also tend to align with some of the data presented by the Payment Card Industry regarding the compliance rates of
the large-merchant population.
Another interesting area to examine is that of the four percent of victims found to be compliant but still suffered
a breach. We still hear the common mantra “How could I have been breached?—I’m compliant!” We cannot stress
enough that while compliance definitely helps drive security, compliance does not equal security. And we believe
the perpetrators of these breaches share a similar philosophy. In fact, in many cases where we have the opportunity
to be involved with the prosecution’s interviews we learn that the perpetrators rarely know who they are hacking.
In most cases it seems that they learn the identity of their victim after they have gained unauthorized access.
Due to the point-in-time nature of PCI assessments, it is possible that an organization deemed compliant at its last
audit may not still be compliant at the time of the breach. Furthermore, there is always some degree of variability
across individual assessors.


We still hear the common
mantra “How could I have
been breached?—I’m
compliant!” We cannot
stress enough that while
compliance definitely helps
drive security, compliance
does not equal security.

What does this all mean with regard to the effectiveness of the PCI DSS?
Overall, the standard attempts to set a bar of essential practices for
securing cardholder data. Nearly every case that we have seen thus far
has attributes of its breach that could have been prevented if the control
requirements had been properly implemented. Of course, there is no way
to be certain that new and different tactics could not have been used by
the perpetrators to circumvent a compliant entity’s controls. However, as

long as we continue to see organizations with the sizable gaps that we see here, we will likely continue to see the
perpetrators utilize such vulnerabilities as the path of least resistance to gain unauthorized entry. Organizations
should continue striving to achieve the best possible security posture. For most, PCI DSS continues to be a
reasonable (and required) yardsticks for measuring their progress.


Nary a year goes by without someone asking why the DBIR lacks information about the impact or consequences of
data breaches. As the well known axiom suggests, “You can never please all the people all the time.” Actually, we not
only understand their question, but we’re standing right next to them shouting for more data on the magnitude of
loss. We spend a great deal of time and text touting the virtues of the DBIR for risk management decisions, yet we
provide little to no information on one of the two major components of risk.

This omission is not for lack of effort; there are legitimate reasons that the report does not contain such information.
Foremost among them is that most organizations are focused primarily on fighting the fires caused by a breach and
want to get back to “business as usual” as quick as possible. The process of recording the variety of ways money
21	 Factor Analysis of Information Risk (FAIR) defines risk as “the probable frequency and probable magnitude of future losses.” See “An Introduction to Factor Analysis of Information Risk (FAIR)”,),” Risk
Management Insight LLC, November 2006. Stats in the DBIR almost entirely relate to the frequency of loss events.

 Even after all the fires are extinguished, for
reasons we haven’t exactly nailed down yet, organizations are not motivated to collect data on their losses.
Secondly, an investigation focuses on collecting evidence to prove or disprove compromise, assess exposure, and
contain the breach. Analyzing and quantifying financial losses to the victim organization is simply not what we’re
paid to do. Although we do occasionally come across pieces of such information, we do not have the opportunities
to gather near enough to complete the puzzle. Nor are we on the ground long enough after the breach to truly study
the long-term consequences.
While we might be able to publish the bits and pieces that we collect on
losses, we made a conscious decision at the very beginning not to do so.
One of the aspects of the DBIR that we (and we hope many others) like is
that, from cover to cover, it is filled with objective, credible, factual
information. Since we do not collect data of that caliber on losses during
an investigation, we have not felt it fits with the rest of the report.
That said, we’re just as eager to gather and report the data as some
readers are to see it. We realize that breach details describing the threat
agents, actions, and loss events along with a credible account of financial
losses are the Holy Grail of our field. Because of this, we’ve reconsidered
our aforementioned decision to not publish “bits and pieces,” and have
begun to make whatever impact-related observations we can both
during and after the investigation. Sometimes this yields almost nothing;
sometimes it can be very informative. We do not have enough data
compiled for 2011 to make this a statistics-driven section, but we can
relay some of these observations to you.
While a few 2011 breach victims estimated their losses to be in the
hundreds of millions of dollars, most did not get near to that amount. In
fact, the large majority of them emerged relatively unscathed from their
troubles. While they were inconvenienced and probably had a sleepless night or two, they returned to normal business operations before too long.
Hands down, the easiest data surrounding impact that we noted in 2011 was response and recovery losses, which
include forensics. Naturally, we’re biased here, since we don’t typically work for free. For larger organizations, such
fees don’t normally affect the bottom line, but they can be hard to cover for smaller businesses. Fraud-related
losses were also very common, which isn’t surprising given the large number of breaches to payments cards and
identifying information. These ranged from a few bucks on the low end to in excess of $100 million.
More rare, but also more substantial, were regulatory and legal fees that were noted (often through public domain
information) for several victims. Sometimes it’s difficult to discern whether these costs actually get paid out, but
class action lawsuits and similar filings can require a sizeable amount at least be set aside.
Brand damage, declines in market value, and loss of competitive advantage are always the top of mind “WIBeHI”
(Wouldn’t it be horrible if…) fears for executives with respect to data breaches. For most breaches—even ones
that seem rather bad—these fears are unfounded. Breaches don’t appear to typically have a major long-term
impact on stock value.

We know of at least four victims that are no
longer in business, either wholly or in large part because of
their breach. One organization was unable to recover from
collateral damages to their client-facing systems that
occurred during the incident. Another’s main line of
business depreciated to such a degree they closed up
shop. A smaller “click and mortar” retailer decided “clicks”
weren’t worth the additional risk exposure after a breach,
and ceased e-commerce operations. Other examples
include outraged customers, soured B2B relationships,
governmental scrutiny, and other consequences nobody
wants to experience. Fortunately, these occurrences are
rare, but we feel it incumbent upon us to mention that
periodically they do occur.
It is also important to remember that the impact is often
felt by organizations that are not the breach victim. In
payment card breaches, for instance, the card issuers (the
banks that provide payment cards to their customers) can
be severely affected by fraud on customer accounts.
These downstream effects are often neglected when
considering the overall consequences of data breaches.
As we close out this short section (with the hope that it
will grow in the future), we would like to state our belief
that assessing the impact of an incident is a worthwhile
exercise. We understand that most organizations just
want to “get back to good” following a breach rather than
dwell on the consequences, but such crises can also be an
opportunity for valuable learning experiences. Not only
can this help to better understand the incident in question,
but also the data derived can benefit future risk analyses
and decision-making.

What this means is that the typical
cybercriminal keeps reverting to the same
permutations of a handful of tactics: basic
hacking combined with malware. Sure,
there are some intricate pieces of malware

and some hacking techniques that stand above the rest, but by and large, we see attackers being only as intelligent
and adaptive as they need to be. A slight rewording might drive the underlying point home more effectively:
attackers are only as intelligent and adaptive as WE FORCE THEM TO BE. Clearly—as a community—we’re not
exactly forcing them to bring their A-game.
The threat-action pairs for larger organizations change in a couple of interesting ways (keep in mind the dataset is
considerably smaller for larger organizations). First, malware appears to be more prominent and varied; 40% of
the malware combinations (malware-malware) only appeared in larger organizations. Second, phishing attacks
(combined with malware) pop into the top 25 in three different pairs and 84% of the pairs involving social only
appeared in larger organizations. Finally, the list of pairs for larger organizations has much more variety, with only
36% duplicates (versus 93% overall). While this may be due to large organizations forcing the threat agents to
bring at least their C-game, it’s more likely a reflection on the automated, repetitious, and uniform attacks against
smaller organizations.
But wait—we’re not done; a few more tasty morsels can be pulled from this data. By comparing how often a specific
threat action occurred overall with how often it was paired with other actions, we can make a series of statements.
Anyone hoping to dazzle co-workers over drinks, impress a romantic prospect, or just have something to say during
an uncomfortably lengthy elevator ride with higher-ups is welcome to give these a shot.
When malware was used to exfiltrate data, 98% of the time it was paired with keylogging functionality. This
indicates that most keyloggers were set up to send out the information they collected. 251 incidents leveraged
this combination and out of those, the attacker also used stolen credentials in all but eight of those incidents (yes,
we ran this with groups of three threat actions as well).
98% of backdoors installed were paired with exploitation of the backdoor. This is not really surprising given that
this report only covers breaches with known data loss. But still, that’s a very high percentage, and shows backdoors
aren’t just scattered around haphazardly; they’re likely to get used (something to think about when you see them in
your network).
73.6% of people will believe a statement that includes a statistic, even if it is completely made up. Okay, we made
that statement up. But now we can say that we’ve made up one statistic in the DBIR.
91% of the 276 breaches leveraging stolen credentials also had keyloggers installed. We mentioned this after the
first statement, and this too may not be surprising since we’ve limited this data to only successful breaches. But
these are “shock and awe” statements; let’s take what we can from it.

When default or easily guessable credentials were used (in 379 incidents), 57% of the time a keylogger was
installed and a backdoor was present in 47%. Remember though, that 104 incidents had the single threat action of
leveraging default credentials.
Out of the 174 backdoors installed, 61% were also seen with a keylogger. Again, this isn’t earth-shattering, but it
is interesting to note that, more often than not, malware is multi-purpose.
To sum up, while a lot of this is just exploratory at this point, there are some important lessons for the present and
directions for future study. For one, we’ve often stressed the importance of raising the cost of attack to the
criminal. These results provide more than anecdotal evidence that their cost is not very high in the majority of
breaches. We need to change that. Another thing to consider is the utility of pattern recognition to incident
detection. While some monitoring technologies use various heuristics and pattern-matching, a great deal of our
detection capabilities depends on one-dimensional signatures. The more we know about attack sequencing,
behaviors, relationships, dependencies, etc, the better off we’ll be in terms of recognizing badness when it’s
happening in front of our eyes.
Keep an eye on our blog (verizon.com/enterprise/securityblog) for more work like this.

In April 2008, the U.S. Secret Service was contacted by a bank fraud investigator and advised that a number of the
bank’s debit cards had been compromised at a franchise restaurant in the New England area. All of the activity from
the compromised cards took place in Europe and utilized manually punched transactions and white plastic cards.
Secret Service agents responded to the franchise location and conducted interviews with the owners, managers,
and several employees in order to gain an understanding of the business process for accepting payment cards as
well as who had access to the processing system. Preliminary investigations revealed that the point of sale (POS)
server was connected to the Internet without a firewall in place or an antivirus program installed. Additionally,
franchise employees were allowed to use the system to access the Internet to check personal web-based e-mail.
Analysis of the POS server identified a keylogging program, which had been used to capture payment card data.
This data was subsequently exfiltrated to a server owned by a web hosting company in Florida. Unfortunately, the
web hosting company did not maintain appropriate log information to provide any assistance in this case.
The Secret Service was contacted again at a later date by a credit card provider and informed of additional
fraudulent payment card activity related to this investigation. This time the fraudulent activity took place in Turkey,
Bulgaria, Austria, and other European locations. The payment card data appeared to have been compromised at a
separate location of the same franchise. Analysis of the POS server at this location again identified a keylogger, as
well as Peer-to-Peer and password cracking tools. Further analysis of this POS server identified the login
credentials as belonging to the same large web-hosting company mentioned above.
Meanwhile, Secret Service agents working a similar POS intrusion into a private business had again identified the
same server and the same login credentials during their investigation. Based on this evidence, a Federal search
warrant for this server was obtained and the analysis identified additional franchise locations that had been
compromised throughout the United States.
Eventually, the Secret Service was contacted by the company that processed the payment cards for the entire
franchise. The processor had investigated a POS intrusion at a location in Florida that identified payment card data
that was exfiltrated to a server located at the same web hosting company identified by the Secret Service. Analysis
revealed an additional 25 franchise locations that had been compromised by the same attacker using similar login
credentials to those previously identified.
Secret Service agents worked together with the franchise headquarters to determine the origin and extent of the
compromise. Analysis determined that the corporate network had not been compromised. It appeared that the
attackers were compromising each location separately through remote administration services that were
accessible from the Internet. Once the attackers gained access, keylogging software programs were installed to
capture the payment card data.
Approximately 50 franchise locations had been compromised, and several FTP servers with similar login credentials
had been identified at the same web hosting company. Analysis of these FTP servers also began to identify POS
intrusions into other companies.
Additional analysis performed by CERT at Carnegie Mellon University identified artifacts originating from a
country in Eastern Europe. CERT also identified the retrieval of keylogger files from an IP address that resolved to
a private company’s server. The company provided consent for the Secret Service to monitor the activity on this
compromised server. This led to the identification of additional FTP servers at the same web hosting company that
had received similar traffic as previously identified FTP servers, along with similar login credentials as the others.
Analysis of the traffic on the new FTP servers and the compromised private company server identified numerous
connections resolving to the previously identified Eastern European country. Additional analysis of the FTP
servers identified an additional 50 merchants with no affiliation to the franchise that initiated this investigation.
The traffic collected from the compromised private company server identified that it was being used for more than
an intermediate collection point. Evidence collected from this server helped to identify the tools, tactics,
compromised data, and the individuals attacking POS systems throughout the United States. A Remote Desktop
Protocol (RDP) toolkit had been identified. The RDP tool appeared to be used to run automated scans of an IP range
searching for computers with an open RDP port. When an open RDP port was identified the attackers would test a
list of usernames and passwords or use password cracking tools to gain access to the POS system. In addition to
hacking tools, several ICQ numbers were identified as the attackers would log into the server via Remote Desktop
and start an ICQ chat client. They would log into web-based e-mail accounts, chat via ICQ, and retrieve files from the
FTP servers.
Secret Service agents were able to identify several e-mail accounts and ICQ chat conversations from the suspected
attackers. Agents served search warrants on these accounts and were able to start building a framework around
this group of attackers. The connections made by the attackers to the compromised private company server and to
web-based e-mail accounts were traced back to the previously identified country in Eastern Europe. Monitoring of
the FTP servers that received payment card data and the compromised payment cards identified through e-mail
search warrants continued to identify new victim businesses.
With the assistance of Secret Service offices located in Eastern Europe and their foreign law enforcement
counterparts, the locations and real world identities of the attackers were pieced together from the evidence
collected. Several of these individuals had been previously investigated in their country for “computer crimes” and
were known to law enforcement.
During the course of the investigation, over 112,000 payment cards had been compromised from 163 of the
franchise locations that initiated the case. Additionally, at least 800 other retail computer systems were
compromised in numerous hotels, movie theaters, medical facilities, residential servers, pizzerias, bakeries, cafes,
and various small businesses. Over $20 million in losses have currently been identified as a result of this investigation.
In May 2011, a Federal Grand Jury indicted four Eastern European individuals for conspiracy to commit computerrelated fraud, conspiracy to commit wire fraud, and conspiracy to commit fraud in connection with access devices.
In a collaborative effort with law enforcement in Eastern Europe, three of the suspects were arrested. Two suspects
were arrested by Secret Service agents while they were attempting to enter the United States. The third suspect
was arrested by foreign law enforcement in Eastern Europe pursuant to a provisional arrest warrant issued by the
Department of Justice and is awaiting extradition to the United States.

Cloud, Aurora, Mobility, Zeus, APT, Wikileaks, Stuxnet, Anonymous. If a word cloud were created using infosec headlines from
2010, these would certainly be rendered big and bold. It’s an interesting juxtaposition of themes. While the Cloud and mobile
devices increasingly allow us to do anything from anywhere with anyone at any time, Aurora, Zeus, Advanced Persistent
Threats (APTs), Wikileaks, and Stuxnet remind us of the difficulty of protecting our information assets in a usability-driven
world. Because our caseload (and that of the USSS and NHTCU) is a window into that world, one would expect to glimpse
aspects of it in this annual report on breach trends. And this year’s DBIR meets that expectation.

Apart from the word “Security,” “Cloud” was the next mostcommon word among presentation titles at the 2011 RSA
Conference. It’s definitely in our collective hearts and minds.
As such, we are often asked whether “the Cloud” factors into
many of the breaches we investigate. The question is both
easy and difficult to answer. The easy answer is “No—not really.”
We have yet to see a breach involving a successful exploit of a
hypervisor allowing an attacker to jump across virtual
machines (VMs), for instance. On the other hand, we constantly
see

breaches

involving

hosted

systems,

outsourced

management, rogue vendors, and even VMs (though the
attack vectors have nothing to do with it being a VM or not). In other words, it’s more about giving up control of our assets
and data (and not controlling the associated risk) than any technology specific to the Cloud.
While we’re on the topic of giving up control of our assets and data, we might as well touch on mobile devices. This is another
oft-asked topic during our breach-related presentations and discussions. The fact of the matter is that mobile computing
devices (tablets, smartphones, mobile phones, etc.) are rarely the source of data loss across our caseload. That has a lot to do
with the kind of cases we investigate (which tend to involve deliberate breach and compromise situations rather than
accidental loss of devices). Plus, this report includes only confirmed incidents of data compromise. The threats to mobile
devices are real and we fully expect them to increase and diversify along with the use, uses, and users of such devices. Just
consider the effect of the iPad since its debut one year ago; many CxOs who were once technology-indifferent now demand
to use their iPads at work. The convenience and functionality of these and other similar devices will drive widespread
corporate adoption and security will once again find itself rushing to catch up.
Zeus sprouted up within our 2009 caseload, but came to full bloom in 2010. Between the USSS and ourselves, Zeus and its
evil business partner, Automated Clearing House (ACH) fraud, were rampant among consumers and businesses alike. Both
Zeus and ACH fraud receive further treatment in this report, so we will simply mention it here and move on.
If Zeus shows us that criminals have their minds on our money, Aurora, APTs, Stuxnet, and Anonymous remind us that some
threat agents have more than money on their minds. These gave information risk a more sinister, targeted, and personal feel
for us all in 2010 (some might add hopeless). Whether these feelings are justified by a significant increase in risk is difficult to
discern. Perhaps these feelings are, in fact, justified. Perhaps they are justified only for a subset of us. Maybe risk did not
change at all, but our awareness of it changed dramatically. Maybe it’s a nugget of truth surrounded by multiple layers of fear,
uncertainty, and doubt. What we do know with certainty is that our 2010 caseload revealed certain characteristics that one
might associate with these events. For instance, numbers of public sector victims hit an all-time high. We studied more
incidents involving theft of classified information, intellectual property, and other sensitive organizational data than ever
before. Simply an artifact of a much larger and more diverse sample caseload rather than a real change? Maybe...or maybe not.
APTs deserve some special treatment here. Some will remember that we voiced concern in the 2010 DBIR and subsequent
blog posts over the APT hysteria sweeping the security community. We still believe that a “scope creep” exists in the definition
of APT. The term’s originators use it primarily in reference to state-sponsored attacks from the People’s Republic of China.
Others use it to describe any threat possessing above average skill and determination. The logical outcome of the former is
to seriously assess and seriously address security posture within government agencies and the defense industrial base
(which is right and good). The logical outcome of the latter is to conclude that “everyone is a target” of APT (which is an
oxymoron and leads to irrational fears about the boogeyman while common thieves clean you out of house and home). It is
simply not possible for everyone to be a target. It is undoubtedly true (based on investigative experience) that some are the
target of state-sponsored attacks (originating from China and/or elsewhere). It is also undoubtedly true (also based on
experience) that some who think they are victims of APTs are really the victims of organized criminals, hacktivists, glorified
script kiddies, and their own mistakes. Because “APTs” (any definition) are real, it’s time we get real about defining and
defending against them.
Outside the spotlight of these headlines, however, a very different story played out in 2010. The amount of compromised
data hit an all-time low across the combined Verizon and USSS caseload. DataLossDB, the Identity Theft Resource Center, and
other sources also show a marked decline in total records lost and exposed. What’s going on? The headlines seem more
hopeless than ever yet the numbers (some of them at least) seem almost hopeful. Why the contrast? What’s the “real” 2010?
We believe threads of truth exist in both stories. As discussed above, there is some truth behind the headlines. Similarly, data
loss figures point to a possible and real change in the motives and tactics used by criminals to steal information. We’ve done
our best to relay these stories and statistics within these pages and unpack their core messages and meaning. We hope this
effort will play some small part in leading us all to a happier ending in 2011 and beyond.

If Zeus shows us that criminals have their minds on our money, Aurora,
APTs, Stuxnet, and Anonymous remind us that some threat agents have
more than money on their minds. These gave information risk a more
sinister, targeted, and personal feel for us all in 2010

Over the past several years the Secret Service has successfully investigated some
of the largest cybercrime incidents in the United States and built a reputation
within the criminal underground as a force to be reckoned with.
Though 2010 did not see a major data breach such as the TJX and Heartland
Payments Systems incidents, that didn’t mean that criminals were taking the year
off. For the Financial institutionscal year 2010, the Secret Service arrested more than 1,200 suspects
for cybercrime violations. These investigations involved over $500 million in actual
fraud loss and prevented approximately $7 billion in additional losses.
A number of factors have lead to a shift in the way the cybercriminal underground
conducts business. In 2010, Albert Gonzalez received a 20 year prison sentence for
his role in the TJX and Heartland Payment System breaches. Prior to that, Maksym
Yastremskiy received a 30 year prison sentence in Turkey as the seller of payment
card data for Gonzalez and other cybercriminals. Also in 2010, Vladislov Horohorin,

aka BadB, was arrested in Nice, France on a Secret Service warrant and is currently being extradited to the United States. BadB
was an original founder of the CarderPlanet criminal forum and was the largest and well-known trafficker of stolen payment
card data for nearly a decade. In a joint investigation with the Dutch National High Tech Crime Unit, the Secret Service
provided investigative assistance that led to the take down of the Bredolab Botnet and the arrest of the Botherder nicknamed
“Atata” by Armenian Authorities. Furthermore, the Secret Service has focused attention on bulletproof hosters, which provide
web hosting services that allow their customers considerable leniency in the types of materials they may upload and
distribute. Seizures in excess of 200 TB of data belonging to bulletproof hosters have made the proliferation of malware more
challenging for cybercriminals and provided a substantial number of investigative leads.
With all these factors taken into account, it is not surprising that the number of compromised records significantly decreased
during 2010. After any major investigation and arrest, the cybercriminal underground evaluates what happened and evolves
from the lessons learned during the prosecution of their peers.
After the prosecution of Albert Gonzalez, cybercriminals learned that when they successfully compromised vast amounts of
payment card data, it is wiser to divide the dataset, mix it with others, and sell it in parts to different vendors over an extended
period of time. The vendors will do the same in an attempt to hide the origin of the stolen data. This makes it harder to
identify the location of a breach since there isn’t a flood of compromised data hitting the underground market at one time.
Cybercriminals have also learned that targeting a large processor or a large company will make them a target too.
It appears that cybercriminals are currently satisfied with compromising POS systems and performing account takeovers and
ACH fraud, as 2010 shows an increase in these areas. In relation to prior years, it appears that there were more data breaches
in 2010, but the amount of compromised data decreased due to the size of the victim companies’ data stores. This shows a
preference in the cybercriminal underground to target the smaller, easier targets that provide them with a lesser yet steady
stream of compromised data.

There has also been a noticeable increase in account takeovers. This can be directly related to the continued rise of the
Zeus Trojan and other malware variants created to capture login credentials to financial websites. These account
takeovers result in fraudulent ACH transfers from the victim’s account to an account under the control of the perpetrator.
The Secret Service and the financial services community are working together to combat this growing trend. The
Financial Services Information Sharing and Analysis Center (FS-ISAC) has teamed up with the Secret Service, U.S.
Treasury, Department of Justice, and many other agencies to create the Account Takeover Task Force (ATOTF), which
focuses on prevention, detection, and response to account takeovers.
As cyber criminals continue to adapt and evolve, so will the Secret Service. As seen in the arrests of Yastremskiy, Horohorin,
Atata and others, there is no safe haven for these criminals.

The process described above has value beyond just describing the incident
itself; it also helps identify what might have been done (or not done) to prevent it. The goal is straightforward: break the chain of
events and you stop the incident from proceeding. For instance, security awareness training and e-mail filtering could help keep E1
from occurring. If not, anti-virus and a least privilege implementation on the laptop might prevent E2. Stopping progression
between E2 and E3 may be accomplished through egress filtering or netflow analysis to detect and prevent backdoor access.
Training and change control procedures could help avoid the administrator’s misconfiguration described in the conditional event
and preclude the compromise of intellectual property in E4. These are just a few examples of potential controls for each event, but
the ability to visualize a layered approach to deterring, preventing, and detecting the incident should be apparent.


Threat agents refer to entities that cause or contribute to an incident. There can be
more than one agent involved in any incident and their involvement can be malicious
or non-malicious, intentional or accidental, direct or indirect. Critical to any forensic
investigation is to identify the source of the breach, not only for purposes of response
and containment, but also for implementing current and future defensive strategies.
Verizon recognizes three primary categories of threat agents—External, Internal,
and Partner.

Insider attacks, fortunately, are not so scalable. They can target their employer and perhaps some of its partners or customers,
but not typically in the manner or to the extent described above. We hypothesized in previous years that this may be due to
increased regulation, heightened awareness, more assessments, better technology, or combinations of these (maybe even
something else entirely). What has not declined are the number of incidents in which partners were “in the picture” for
circumstances surrounding the breach. By this we mean that the partner was not an active (or causal) threat agent, but they
were responsible for hosting, managing, securing, etc. the systems involved.
Should we conclude that organized criminals tried harder
(hit more victims) but were less successful (stole less data)?
This may or may not be true, and there’s a chance they might
actually be more successful overall if measures other than
record counts are considered.
When examining compromised records for organized criminals, we see a different picture than the one we have come to know
(and dis-love). Prior reports pinned upwards of 80-90+% of all stolen data on the activity of these groups; here they declined to
a little over 50%. Should we conclude that organized criminals tried harder (hit more victims) but were less successful (stole less
data)? This may or may not be true, and there’s a chance they might actually be more successful overall if measures other than
record counts are considered. For instance, many of the perpetrators of the largest known breaches are living behind bars now
(not considered by most to be a high quality of life). An approach using a standardized methodology to take a little data from a
lot of organizations may help achieve a decent (or indecent, rather) living while
avoiding incarceration (at least for a time).
First, a considerable
number of clients kept insufficient log information to successfully identify the attacker; it simply can’t be determined by forensics
alone. This is in part due to the demographics of the 2010 caseload; smaller organizations are less likely to have the resources or
the expertise to manage their IT infrastructure. The second reason is that many victim organizations do not wish to expand the
investigation to include this line of inquiry when the attack has already been successfully mitigated. Similar to “unaffiliated
person(s),” one wonders about their true agenda and alliegances.
Lastly, we wanted to mention another group of external agents that are sometimes lumped in with insiders—that is, former
employees. There is some grey area around exactly when an employee (internal agent) becomes a former employee (external
agent) and the classification depends on the individual’s employment status with the organization when the breach occurred
as opposed to when it was discovered or investigated. In our recent casework, we observed several examples involving
former employees stealing data from their ex-employer. One of them sold their shared administrative credentials on the
black market, which resulted in authorized access soon after. Since these credentials were still shared among active
employees, they weren’t disabled as this individual left. Another stole data while employed, nabbed more after leaving, and
then extorted their former organization. Yet another sold their knowledge about the inner workings of a system to a
competitor. Several others were nice enough to continue visiting the internal network occasionally to catch up on the latest
developments and gossip. Deprovisioning of user accounts, anyone?
Origin of external agents
Ascertaining the geographic origin of external agents generally suffers from problems with making this determination based
upon IP addresses. Even when the country of the source IP(s) can be accurately pinpointed, it is often not the country where
the actual attacker resides, but rather a host in a botnet or just another “hop” used by the real culprit. In some cases, however,
various types of additional information help
refine or corroborate IP-based geolocation. All
these issues aside, knowing the origin of
attacks (whether immediate or ultimate) is still
very useful for many reasons.

Insiders can cause or contribute to breaches in all sorts of ways. For classification
purposes, we group these into three major buckets. They either acted deliberately and
maliciously, inappropriately but not maliciously, or unintentionally without malice.
Much like last year, investigators determined that nearly all internal breaches (93%) were
the result of deliberate malicious activity. This may seem odd, but one should remember
that we’re specifically discussing data loss cases investigated by either a third party
forensics group (Verizon) or a law enforcement agency (USSS). Also keep in mind that if
the insider’s only involvement was related to a conditional event7, they are not
considered a primary threat agent and thus not depicted in the statistics above.
For the second year in a row, it is regular employees and end-users—not highly trusted

ones—who are behind the majority of data compromises. That ratio was roughly even in our first two reports, but since the
addition of the USSS cases, lesser-privileged insiders are increasingly dominant. Examples of regular employees represented by
the 88% shown in Table 7 spanned corporate end-users, bank
tellers, cashiers, waiters, and others among the rank and file. These
employees aren’t normally escalating their privileges in order to
steal data because they don’t need to. They simply take advantage
of whatever standard user privileges were granted to them by their
organizations. This is a good time to remember that users need not
be super users to make off with sensitive and/or valuable data. Case
findings suggest that regular employees typically seek “cashable”
forms of information like payment card data, bank account
numbers, and personal information.

For the second year in a row, it is regular
employees and end-users—not highly
trusted ones—who are behind the
majority of data compromises. This is
a good time to remember that users
need not be super users to make off with
sensitive and/or valuable data.

The proportion of internal breaches tied to more privileged and
trusted employees like executives, system administrators, and developers totals about 12% (less than half of what it was in
2009). When those in such positions are involved with breaches, case history shows they usually steal larger quantities and
more valuable forms of information. This makes a lot of sense in that highly privileged and senior employees have the “keys
to the kingdom” as they say. 2010, however, did not follow the pattern of history (maybe someone changed the locks?).
System and network administrators stole far less information than regular
employees. Executives, usually linked to the theft of IP and other sensitive
organizational information, did not take significantly more of such data
than other types of employees. Why? To be honest, we’re not sure.
It’s worth the time to make a quick point on the types of assets targeted by insiders. Our data shows that external agents target
servers and applications and end-user systems most of time. The assets targeted by insiders vary between all types of assets. We
believe this is one the (many) reasons that insider threat is difficult to control. They have access to a plethora of assets and know
where and how to obtain data from them.
Partner Agents (<1% of breaches, <1% of records)
In comparison to previous years, breaches stemming from business partners declined sharply (based on partners identified as a
primary threat agent). There were only three (yes, 3) of them in the entire combined 2010 caseload. How does one write a
section about three events? Answer: One doesn’t. Instead, we’ll simply mention what they were and then briefly clarify other
ways in which partners factored into breaches but did not cause them.
There were two instances of partner error and one of misuse resulting in data compromise in 2010. One acted deliberately and
maliciously (Misuse) and the other two acted unintentionally (these are touched on in the Error section).
Extending the conversation from partners as threat agents to partners that factor into or relate to the breach in other ways gives
us something more to talk about. First, partners can contribute to a conditional event within the broader incident scenario.
Conditional events create circumstances or conditions that—if/when acted upon by another agent—allow the primary chain
of threat events to progress. In this respect, they are more akin to vulnerability than threat (which is why partners involved in
them are not considered primary threat agents). In 2010, partners contributed to conditional events in a sizeable 22% of
incidents. A common example of this is in the retail and hospitality industries where a remote vendor responsible for managing
a POS system neglects to change the default credentials, leaving it vulnerable to attack.
Something else to consider, a good number of assets involved in 2010 breaches were either hosted or managed by a partner.
This fact may have had absolutely nothing at all to do with the incident, but it is a partner-related datapoint and worth tracking
and monitoring over time.
All in all, what was said last year remains true; organizations that outsource their IT infrastructure and support also outsource a
great deal of trust. A partner’s security practices—often outside the victim’s control or expertise—can factor into breaches in
various ways. Third party policies, contracts, controls, and assessments should account for this.
Figures relating to compromised records were comparatively more stable across all threat categories in 2010. Hacking and
Malware were still the dominant duo of data loss, though slightly less so than in previous years. The only other category
responsible for a significant amount of loss was Physical.
In the spirit of the 2009 Supplemental DBIR, Table 8 lists the top 15 most prevalent threat action types (not categories) in 2010
along with their frequency (percent of breaches) and impact (percent of records). The information recorded in Table 8 is also
represented in Figure 18 with the percentage of breaches (frequency) along the x-axis and percentage of compromised records
(impact) along the y-axis. We will leave you to mull over these at your convenience and move on to a more in-depth analysis of
each threat action category.

Malware is any software or code developed or used for the purpose of compromising or harming information assets without
the owner’s informed consent. Malware factored into about half of the 2010 caseload and nearly 80% of all data lost. A majority
of breaches involving malware were against organizations in the Hospitality industry, with Financial Services being the second
most affected group.
Upon identification of malware during a data breach investigation, the IR team conducts an independent analysis to classify and
ascertain the capabilities of the malware with regards to the compromise at hand. Investigators often collaborate with ICSA
Labs, an independent division of Verizon, and use the resultant analysis to better assist the victim with containment, removal,
and recovery. Malware can be classified in many ways but we utilize a two-dimensional approach that identifies the infection
vector and the functionality used to breach data. These two dimensions are directly relevant to identifying appropriate detective
and preventive measures for malware.

As always (at least in our caseload), the most common malware infection pathway is installation or injection by a remote
attacker. This covers scenarios where an attacker breaches a system and then deploys malware or injects code via SQL injection
or other web application input functionality. It also accounts for four-fifths of the malware infections in our 2010 caseload, up
from around half in last year’s study. It’s popularity as an infection vector stems from the attacker’s desire to “set up shop” after
gaining access to the system. Installing malware is simply part of the moving in process.

The web, while still the second most common infection vector, decreased from last year. Web-based malware is divided into two
subcategories, code that is auto-executed (aka drive-by downloads), and code that requires additional user interaction beyond
the page visit; fake AV scaring users to “click here to scan and clean your infected system” is a common example of this tactic. The
main reason for the “drop” in web-based malware (which wasn’t really a drop at all since the number of incidents involving them
is similar to before) is that the highly-automated and scalable attack scenarios described throughout this document do not use
this pathway. Improvements in browser security could also be contributing to this shift, but we haven’t seen any direct evidence
to support this finding.
E-mail based malware doesn’t show significant changes from previous studies, while other infection vectors decreased.
Occasionally, we still see infection vectors such as network propagation and portable media devices, but there appears to be a
consistent shift towards attackers “owning the box” to get specific malware on the system. The somewhat high percentage of
“unknown” is attributable to many different factors. Most often it is due to a lack of evidence (no log data, software removal, and
premature cleanup) on the system. In these cases, we know malware was present, but the infection vector cannot be
conclusively determined.
Equally important to the pathway of malware infection is the function it exhibits once it is within the victim’s environment.
However, we often find all sorts of
other unrelated malware during the course of our investigation. This serves as an additional indication of inadequately managed
systems. Although malware frequently utilizes several methods to harm a system, it still serves one or more of three basic
purposes in data breach scenarios: enable or prolong access, capture data, or further the attack in some other manner.

Sending data to an external entity, backdoor, and keylogger functionalities continue to be the three most common functions
found in breach-related malware and all increased this year. It is important to note that none of these are mutually exclusive and
it’s common for a single piece of malicious code to feature several components. Backdoors, which allow attackers unauthorized
access to infected devices, are again atop the list with a two-fold increase. Once they have gained that foothold they can install
additional malware, use the device as a launch point for further attacks, retrieve captured data, and so on. Over half of data loss
in cases featuring malicious code involved a backdoor component.
Keyloggers and form grabbers were seen in two-thirds of cases, nearly doubling from the previous year. Commercially available
keylogging software, such as Perfect Keylogger and Ardamax Keylogger, are freely available on the web with fully functioned
pirated commercial versions distributed on P2P networks and torrent sites. These utilities also allow the attacker to build a preconfigured remote installation package that will be deployed on a target system. They exhibit many types of anti-forensic
capabilities, such as hiding itself from a list or running processes, and manipulation of timestamps of its components and output
files. Attackers can customize the software to create output files with user-defined
filenames, which enable the use of legitimate Windows filenames. Other features,
such as encryption of output files and automated exfiltration methods via e-mail or
FTP also exist. Historically, criminals use these types of keyloggers because of these
features and ease of configuration.
Keyloggers are also common in Zeus family of malware used to target consumer or
merchant credentials to online banking applications. An interesting two-victim
dynamic develops where a customer victim (consumer or business) suffers the loss of
valid banking credentials, and a bank is victimized when the attacker uses the stolen
credentials to conduct a fraudulent transaction. Many times this entails a wire transfer
to an account outside of the United States where the funds disappear quickly into the
hands of money-mules.


In addition to keyloggers, the use of RAM scrapers in POS-directed attacks has also increased. RAM scrapers are designed to
capture payment card data from a system’s volatile memory, and the increase of its use is consistent with the decrease in packet
sniffers. Increased encryption of network traffic across both public and private networks has driven some of this transition. The
payment card data residing in RAM is not encrypted and is most likely “fresh” with a current expiration date. Another potential
factor in the reduction of packet sniffers may be that several of the groups tied to large cases involving packet sniffers are in jail
(e.g., Albert Gonzalez). That’s not at all to say sniffers are a lost art, but there does seem to be a connection.
Backdoors initiate outbound reverse connections from the infected system to circumvent firewalls and other security controls.
We’ve seen several types of backdoors throughout our investigations, some of which facilitate interactive remote access
employing SSH tunneling to forward RDP port 3389 to an IP address configured by the attacker, and others that communicate
to a “client” application accepting communication from the infected system. Attackers deploy the latter type of backdoor using
a “server” executable on a target system, which will communicate with a “client” application on the attacker’s system. These
backdoors are often configured to communicate on commonly used ports such as 80, 443, or 22 to conceal the suspicious traffic
from system administrators. Such backdoors are described in the hacker community as a Remote Administration Tool (RAT)
and are readily available on the web and across hacking forums. Generally, AV classifies RATs as remote access Trojans, however
commercial non-free versions of these tools exist and are advertised by the developers to circumvent AV. These standalone
“server” executables are usually configured and built using a GUI based “client” application with all attacker specified options
embedded within the executable. These types of backdoors commonly contain file transfer and keylogging functionality as
well as other anti-forensic techniques such as encrypting its traffic, password protection, and secure deletion capabilities. The
keylogging components of these backdoors allow criminals to capture authentication credentials and use them for subsequent
and/or expanded attacks against corporate networks. One particular organized crime group used the same backdoor/keylogger
on over 100 different organizations.
Network utilities, such as PSTools are commonly used to deploy malware on systems and to harvest the output. Though these
tools are not inherently malicious, criminals are deploying them and using them in a malicious manner. If such utilities were
added to a system by an attacker, we categorized them under malware.

While any amount
of data leaving the
owner’s possession
is never a good
thing, the act does
(or at least can)
provide evidence
of foul play. It’s a
matter of looking
for the right
indicators in the
correct places.

When malware captures sensitive information, it must then be exfiltrated from (taken out of ) the
victim’s environment. There are two basic ways this happens: either the malware sends it out of
the organization (found in nearly eight out of ten of incidents involving malware) or the attacker
re-enters the network to retrieve it (see backdoor). The general rule of thumb is that smaller
packets are sent out (i.e., credentials captured by keyloggers) while larger hauls of data are
retrieved (i.e., the contents of a network file share transmitted through a backdoor’s file transfer
capabilities). While any amount of data leaving the owner’s possession is never a good thing, the
act does (or at least can) provide evidence of foul play. It’s a matter of looking for the right
indicators in the correct places.
For this reason (and others) we advocate paying attention to what goes out of your network and
what changes take place within your systems. Don’t have any customers or partners in East Asia,
yet network and firewall logs show periodic bursts of traffic sent there from your networks? What
about those ZIP or RAR files with hidden and read-only attributes that showed up in your root
directory last week and have been growing steadily ever since? Maybe there’s a perfectly good
explanation for these things… but you will never know for certain unless you take steps to
identify and verify them. It highlights the importance of detecting and responding to malware
quickly. In some incidents the affected company missed an opportunity to lessen the aftermath

of infection by ignoring or not adequately investigating initial anti-virus alerts. Regrettably, those alerts sound less often these
days, and AV alone is not always enough.
Malware Customization
This year nearly two-thirds of malware investigated in the Verizon caseload was customized; the highest we have ever seen.
Additionally, most of the records stolen by malware were taken in breaches where customized forms were observed. The extent
of customization found in a piece of malware can range from a simple repack of existing malware to avoid AV detection to code
written from the ground up for a specific attack. In 2010 we have seen the majority of customized code shifting to a level of
effort that falls in between these two extremes.
Code modification to existing malware was present in a little less than half of Verizon cases involving malware. This is often
something like a “kit” in which you start with certain known base code that provides low-level functionality, but can add to it or
modify it to fit a specific purpose. Hackers can then collaborate on more advanced functionality to build a bigger and better
monster. Additionally, the modification and customization of such malware not only allows attackers to add or change
capabilities, but also hinders the detection of such malware. The infamous Zeus malware falls into this category. Attackers
commonly started off with a base version of Zeus, but a large community of individuals modified or recoded its elements to
enhance or change its functionality and detectability over time.
When code modification is present, over two-thirds would fall into this level of customization. Many of the freely available
backdoors and keyloggers also allow for low-difficulty customization and modification. For example, attackers no longer have
to modify code to alter the exfiltration strategy of a particular piece of malware, they can just type an IP address in a form, check
(or uncheck) some boxes, hit “Apply” and then “OK.”

This year nearly two-thirds of malware investigated in the Verizon caseload was
customized; the highest we have ever seen. The extent of customization found in a
piece of malware can range from a simple repack of existing malware to avoid AV
detection to code written from the ground up for a specific attack.
In a year that includes more breaches than ever, the increased proportion of customized is not a good sign. This is especially true
when mixed with other findings of this report. It means that even the majority of highly-automated and non-targeted attacks
against small organizations utlize customized malware. This, in turn, means that the cost and difficulty of customization is
relatively low. This commoditized customization is made ever more accessible to an ever-increasing pool of criminals by an
extensive “malware-as-a-service” market. We find it hard to foresee anything but trouble here for the good guys.
Hacking (50% of breaches, 89% of records)
The term “hacking,” although ambiguous (and ubiquitous), essentially categorizes all attempts to intentionally access or harm
information assets without (or in excess of ) authorization by thwarting logical security mechanisms. Hacking affords the criminal
many advantages over alternate modes of attack. Namely, it can be accomplished remotely and anonymously, it doesn’t require
direct interaction or physical proximity, and there are many tools available to automate and accelerate attacks. The use of
automated tools, typically written and developed by someone other than the attacker, lowers the learning curve and allows
even less-skilled threat agents to successfully pull off an intrusion. In this section, we examine the types of hacking observed by
Verizon and the USSS in 2010, the paths through which these attacks were conducted, and other details about this
important category.

The method utilized in the highest percentage of breaches and stolen records was exploitation of backdoors or command/
control functionality. This isn’t the backdoor itself (which is considered malware), but is inextricably linked to it. With a backdoor
installed, attackers can bypass security mechanisms to gain access without relying on legitimate channels. This offers the added
advantage of greater stealth and evasion of host-level logging. Legitimate remote access applications do not log an intruder’s
actions if he or she is not using them.

The next few techniques listed in Figure 22 are basically a blueprint for standardized and highly scalable attacks against soft
targets. That is to say, the perpetrator(s)—largely organized crime groups—set up automated systems to scan for certain open
ports and services (footprinting and fingerprinting), try a few well-known combinations of default credentials used on various
types of systems, and then—if still necessary (it’s often not)—run a brute-force attack to crack the system. These scans run at all
hours of the day and night, trying to gain access, and recording successes. The would-be assailant wakes up, has some coffee
(or tea, or maybe even vodka), and begins the workday with a nice compiled list of IPs for vulnerable devices along with the
exact usernames and passwords needed to access them. After that, put in a few hours cramming malware onto selected
systems, revisit last week’s victims to collect some captured data, and then head home early to the wife and kids. This continues
until they get caught, grow bored with it, die, or get hired by a security company (yes, the latter is a jibe, but, unfortunately, it’s
often true).

The would-be assailant wakes up, has some coffee (or tea, or maybe
even vodka), and begins the workday with a nice compiled list of IPs for
vulnerable devices along with the exact usernames and passwords needed
to access them. After that, put in a few hours cramming malware onto
selected systems, revisit last week’s victims to collect some captured data,
and then head home early to the wife and kids.
After the triad above was the use of stolen login credentials. This common technique is particularly vexing to victims because it
shrouds the attacker in a disguise of legitimacy. Rather than sounding alarms because an unrecognized or unauthorized user is
accessing sensitive assets (yes, we realize the data suggests that no alarm would be sounded anyway, but we’re trying to be
optimistic), it looks like Bob doing his job. Nothing out of the ordinary with that, right? Authenticated activity is much less likely
to trigger IDS alerts or be noticed by other detection mechanisms. It also makes it easier for the attacker to cover his tracks as he
makes off with the victim’s data.
As with last year, we found that credentials are stolen more often by malware than, say, phishing or snooping them off sticky
pads (though those things do happen). Bank credential stealing malware such as Zeus or Spyeye will grant an intruder possession
of legitimate access credentials that often drive the remainder of the data breach. This occurs when an end-user downloads a
piece of malware, either via drive-by-download or through user interaction with some e-mail or other message tailored to the
user. The credentials are then distributed through botnets, compiled, and organized for each institution. The attacker will then
use these credentials to either make fraudulent financial transactions from business accounts (ACH fraud), personal accounts
(consumer fraud), or steal some type of sensitive PII data for identity theft.
That the use of stolen login credentials fell in 2010 from its top position is rather misleading. The distinction of what is a single
incident vs. multiple incidents can be difficult to make with this technique. For instance, if a bank notices that 100 accounts
showed signs of unauthorized access, they would likely consider these to be 100 different “incidents.” However, if an
investigation was conducted and all of those were traced to a single perpetrator, it might be viewed as one large incident
affecting multiple accounts. It comes down to perspective and knowledge of the details behind the attack. We mention this
simply because such scenarios were quite common in both Verizon’s and the USSS’ caseloads. We treated them as single
incidents, which has an effect on the stats associated with stolen credentials.

After we wished it a happy 10th birthday last year, SQL injection has returned for another party, but with less fanfare this time.
From 25% of hacking-related breaches and 89% of all data stolen, those numbers declined in 2010 to 14% and 24% respectively.
Of course, there’s that whole caseload-scaling thing to consider, so it’s not as though SQL injection is disappearing. It simply
hasn’t been as widely incorporated into the kind of canned attacks described above for other techniques. Something interesting
to note about SQL injection is that it factored into a disproportionately higher percentage of breaches in Asia.

Nearly all exploit configuration weaknesses or inherent functionality of the system or
application. This trend continued in 2010 as only five vulnerabilities were exploited across the 381 breaches attributed to
hacking. These are as follows: CVE-2009-3547, CVE-2007-5156,
CVE-2009-2629, CVE-2010-0738, and CVE-2007-1036. Though
surprising, this makes sense if one considers the prevalence of
techniques discussed earlier in this section, few of which are
vulnerabilities in code that can be “patched.”
It’s difficult to tell if this trend (of few vulnerability exploits) exists
because hackers prefer other vectors or if they’ve been forced in
that direction because organizations are patching well. Most
likely, it’s a little of both. Patching is definitely a security practice

It’s difficult to tell if this trend (of few
vulnerability exploits) exists because
hackers prefer other vectors or if they’ve
been forced in that direction because
organizations are patching well. Most
likely, it’s a little of both.

that is well-known and receives a lot of attention (it’s often the core statistic of a security metrics program). For the most part,
organizations do seem to be keeping patch levels current, at least on Internet-facing systems. As you can see from those CVE
dates, most attacks exploit older vulnerabilities, ones that should have been eliminated by any reasonable patch deployment
cycle. Therefore, we continue to maintain that patching strategies should focus on coverage and consistency rather than raw
speed. The resources saved from doing that could then be put toward something more useful like code review and
configuration management.

Having lost ground to web applications over the last few years, remote access and desktop services are once again at the
number one spot in the list of attack pathways. A whopping 71% of all attacks in the Hacking category were conducted through
this vector. The intruders started by stealing legitimate credentials to the bank’s ACH wire transfer portal
belonging to three separate internal employees, who all received an e-mail from the “FDIC” on a Friday afternoon. The
employees noted that the attached PDF file wouldn’t open correctly. The following Monday, several million dollars were
wired out of the bank using the three employees’ access credentials.

The word “patchable” here is chosen carefully since we find that “vulnerability” does not have the same meaning for everyone within the security community. While
programming errors and misconfigurations are vulnerabilities in the broader sense, lousy code can’t always be fixed through patching and the careless administration patch
has yet to be released. Furthermore, many custom-developed or proprietary applications simply do not have routine patch creation or deployment schedules.
As soon as an intruder discovers a particular vendor’s authentication method and schema (be it for
TCP port 3389 for RDP; or TCP port 5631 and
UDP port 5632 for pcAnywhere), he will be able to exploit it across a multitude of that vendor’s partners and customers.
Oftentimes, in lieu of conducting a full port scan for these remote service applications, attackers will customize their scripts
to exclusively look for these ports and search a broad swath of the Internet. This speeds up their capability of searching for
and finding services unprotected by router/firewall ACLs and allows them to quickly check for default credentials as well. This
of course relies on remote access authentication schema being uniform across all of that particular vendor’s customers—but
hey, who are we kidding? They always are.

Just because web applications dropped as an overall percentage of attacks,
don’t believe for an instant that they are any less critical a vector than they were
a year ago. If you remove hospitality and retail victims from this dataset, web
applications are right back on top and are more numerous than ever.
The installation and exploitation of backdoors has already been covered in this report. They do, however, warrant another
mention here as we discuss common paths of attack. Along the typical chain of events, the backdoor is often placed on a victim
system after gaining access via default or stolen credentials. The agent then has control of or can access the system at will
without leaving traces in logs (if they exist in the victim environment).

As in years past, backdoors are frequently utilized
Just because web applications dropped as an overall percentage of
attacks, don’t believe for an instant that they are any less critical a
vector than they were a year ago. If you remove hospitality and retail
victims from this dataset, web applications are right back on top and
are more numerous than ever. Please don’t let the bad guys catch your
development and application assessment teams napping.

Social tactics employ deception, manipulation, intimidation, etc. to exploit the human element, or users, of information assets.
Typically, these actions are used in concert with various other threat categories and can be conducted through both technical
and non-technical means.

Solicitation and bribery remains the most common type of social tactic in 2010, but by a much wider margin than
before. This frequently entails collusion between an external agent and an insider, though other combinations occur as well.
Whoever is involved, one party uses petitions, promises, and payments to get another to participate in the crime, usually
because it would have been difficult or impossible without their aid. Widespread solicitation scenarios target waiting staff and
cashiers to skim payment cards and bank employees to perform all manner of illicit activities. Less common examples involve
recruiting system administrators and other privileged parties to steal data, open holes, disable security systems, etc.
Pretexting numbers are also quite high, and have more than doubled from the previous year. There are a myriad of ways in
which imaginative and resourceful criminals can utilize pretexting in an attack scenario. We observed convincingly-attired
repairmen walk brazenly into victim locations to steal, tamper with, and replace devices. We saw organized foreign criminals use
elaborate yarns to weasel their way into positions of influence in numerous organizations (or gain the trust of those that did). We
studied records of human resources staff hoodwinked into providing (and changing) personal and employment information to
would-be fraudsters. We witnessed Jedi masters convince Stormtroopers that these were not the droids they were looking for...
oh wait...no; that was Star Wars. Nevermind.

While counterfeiting and forgery can involve everything from websites to documents (and more), the use of fake credentials
(drivers’ licenses, birth certificates, etc.) was 2010’s most prevalent example. Many of these had to do with identify theft and
account takeover schemes targeting financial institutions.

In last year’s report, e-mail was the path du jour in most cases. Over the last
year, however, criminals increasingly relied on the personal touch with a
whopping 78% of cases involving in-person contact.
Phishing is not new by any means, but it does seem to be finding some renewed attention in the criminal community. Rather
than the typical e-mail lure to change your bank password, external sources along with our own caseload hint that phishing
is being used more often to gain a toehold in the victim’s environment through attached malware. This tactic, of course, is
not new either; it simply seems to be hitting a (who know’s how temporary) growth spurt.
The vectors through which social tactics were conducted changed significantly in 2010. In last year’s report, e-mail was the path
du jour in most cases. Over the last year, however, criminals increasingly relied on the personal touch with a whopping 78% of
cases involving in-person contact. This was the clear vector of choice for
solicitation and pretexting—and understandably so. Even in our high-tech
business world, many deals won’t get done without an in-person “meet and
greet.”
We define Misuse as using entrusted organizational resources or privileges for any purpose or in a manner contrary to that
which was intended. These actions can be malicious or non-malicious in nature. The category is exclusive to parties that enjoy a
degree of trust from the organization such as insiders and partners. In 2009, Misuse was the most common of all threat actions,
but dropped substantially in 2010. In no way does this mean Misuse is rare; 17% corresponds to almost 130 breaches that
involved some form of Misuse.
The three most common types of Misuse observed in 2010 are a repeat of those identified in 2009, with embezzlement,
skimming, and related fraud once again on top. Several large internal fraud cases worked by the USSS helped make this type
of misuse even more predominant over the past year. In one, certain members of a Nigerian fraud ring were indicted for their
involvement in a long-running and extensive identity theft operation within some of America’s largest banks. The fraudsters
gained key positions within these institutions which allowed them to steal personally identifiable information, access and/or
create bank accounts, apply for fraudulent loans, sell information on the black market, and other nefarious activities.
Other, less complex, instances of embezzlement, skimming, and related fraud were seen as well. These were commonly
perpetrated by bank tellers, restaurant waiters, retail clerks, or others in similar positions in which the “simple” handling of financial
transactions is inherent to the job. Oftentimes these employees used handheld skimmers and other devices to facilitate the
theft, which is why “use of unapproved hardware/devices” is rather high in Figure 26. While such activity may seem out of sorts
with some of the more technical attacks described in this report, it is nevertheless a real (and common) method of stealing
data—especially payment cards. As discussed in the section describing Social tactics, these scenarios very often involve an
external party that solicits and/or bribes the insider to commit the crime and provides them with the requisite devices to pull
it off.
Abuse of system access/privileges, at the #2 spot in Figure 26, is similar in nature to
embezzlement, but specifically involves the misuse of logical access to information
systems. As suspected, many breaches involve both non-technical forms of
embezzlement along with abuse of system access (and any other type of Misuse listed
in Figure 26, for that matter). The actions leading to the court martial of U.S. Army Private
Manning provide a now infamous real-world example of this type of Misuse. He abused
his (overly) privileged access to SIPRNET to browse and copy classified State Department
cables without authorization to an external hard-drive (unapproved device). While this
event stole the spotlight in 2010, it is by no means the only or most spectacular example
of system abuse from 2010. The combined Verizon-USSS dataset contains scores of
them, but the worst aspect of such cases is that countless others will likely never
be discovered.
As evidenced by the examples above, privileged users typically need a means of moving
or exfiltrating data once they have misappropriated it. Figure 26 is essentially a laundry
list of how this can be accomplished. Some use corporate or personal e-mail to send it
to external parties or accounts. Some smuggle it out on various types of personal
devices or media. Others use approved devices, but for unapproved purposes or in an
unsanctioned manner. We continue to find that the success of a breach does not hinge
on the perpetrator being able to use a certain portable device (i.e., plugging up USB
slots doesn’t eliminate the problem). Unfortunately, users have a plethora of choices
when it comes to media and devices fit for secreting data and removing it from their
employer. For this reason, it is generally easier to control data at the source than it is to block a virtually limitless array of potential destinations. Certain technologies, however,
like DLP and behavioral monitoring may add some additional levels of protection between those end points.
This doesn’t mean that everyone who veers slightly from the straight and narrow will inevitably careen headlong
into a life of crime, but it does mean that questionable behavior should be seen for what it is—a potential warning sign—and
treated appropriately. Another lesson reinforced during this round of analysis is the importance of quickly deprovisioning user
access and privileges when they are no longer needed. Year after year we investigate breaches involving former employees or
business partners. A simple yet good rule of thumb is that if you no longer want them on your payroll, then don’t leave them in
your systems.

This category encompasses human-driven threats that employ physical actions and/or require physical proximity. In previous
years physical attacks were consistently one of the least prevalent threat actions in terms of both percentage of breaches and
percentage of records lost. This was partially due to the nature of common physical actions; for instance, it is unlikely that a stolen
mobile device will precipitate a full-blown forensics investigation and would therefore not be in our caseload. Another factor is
that many of the action types we classify as Physical are less likely to be associated with confirmed data loss. Moreover, if physical
access to a device is available as part of normal job duties for insiders, then that local access is not classified as a physical action
but rather falls under the
category of misuse.
Incidents involving ATM and gas pump credit card skimmers represent the
majority of physical actions. These cases would not typically be pursued by Verizon investigators, but certainly fall under the
jurisdiction of USSS. ATM and gas pump skimming is conducted largely by organized criminal groups and one “spree” can target 50
to 100 different business locations. These attacks have been occurring for years, but are on rise in many areas according to both
public reports and the caseload of the USSS.
The second change from last year is that we have witnessed a discernible increase in the proportion of record loss
associated with physical actions from prior years. Again, this is attributable to the increase in physical skimmer cases.
Record loss for these cases is an aggregate of the credit card numbers and/or PINs compromised and is therefore much
different than cases of theft that may involve a single document or device. Ten percent of all compromised records were
linked to cases involving a physical action in 2010. By way of comparison, physical actions were only associated with 1%
of data loss in 2009’s combined caseload.
The third change in physical is represented by the increase in tampering (98%), and surveillance (17%), and the decrease of
theft (2%) as physical action types from previous years. Yet again, this was directly influenced by the amount of ATM and gas
pump skimming cases in our data set. According to USSS data, ATM skimming is increasing and is becoming more organized.
Skimmers can vary greatly in sophistication both in inconspicuousness and feature sets.
A standard ATM skimmer is a reader device designed to fit on top of a legitimate card slot. Both readers are able to read the
data on the magnetic strip, and the credit card number is stored on the skimmer device to be retrieved at a later date. Hidden
cameras are often used in conjunction with the capture device to collect PINs upon user entry. These cameras are affixed
above the keypad and are concealed by the use of incredibly clever camouflage. In many instances, they fit almost perfectly
over the existing ATM shell, and are disguised by means of using the same material and color as the original. In other cases,
this is achieved by disguising the camera as a sign on the ATM that features the banks logo, or the logos of the cards accepted
by the ATM. Fake PIN pad covers are another method of PIN capture, and have the advantage of not relying on a line of sight
to the key pad. However, these are potentially riskier for the criminals as they are larger, more expensive, and because they
are touched by customers, potentially more vulnerable to discovery. These fraudulent devices are attached by junior
members of organizations using strong adhesives, and are completed in a matter of seconds.
As stated above, ATM skimmers are found with varied levels in sophistication. This type of crime is carried out by gangs
which possess a considerable amount of organization. The techniques used to reduce the chances of discovering the
fact that they have tampered with the machines begins with molds and overlays that mimic the existing card reader in
shape, and perhaps more importantly, color and material. Even the most basic skimmers are not generic, but designed
for specific ATM models in the same manner that mobile phone cases are manufactured for specific models. Better fit
equates to less deviation from a non-altered device, and, therefore, less potential for scrutiny.
The technology behind the skimmer is also becoming increasingly sophisticated. The more basic devices feature a
built-in storage component for the magnetic stripe and PIN data. The payment card data resides on the skimmers until
retrieved by a second visit from the criminal to detach the skimmer device. Advances in data exfiltration techniques
have included the use of Bluetooth technology within the skimmer to allow for wireless retrieval within a finite
proximity. This, of course, reduces the risk of apprehension when attempting to retrieve the device, which may occur if
the skimmer is discovered. Additionally, it allows the possibility of collecting data at various intervals, so if a device is
removed by a bank employee or law enforcement not all of the captured data is lost. The latest evolution in data
retrieval is the use of technology, again embedded in the skimmer, that utilizes GSM standards and will text captured data in real-time to the criminal’s cell phone. The correlation
between data capture and criminal possession is streamlined from
a one-time retrieval, to scheduled collections at the criminal’s
convenience, to an instantaneous event. The required proximity of
the criminal collecting captured data increases exponentially from
required local physical access, to close proximity, to virtually
anywhere.
The cases involving payment card capture at “Pay at the Pump”
terminals have featured different attributes than ATM skimming.
Access to the inside of the gas pump and the card reader hardware
is achieved by using a master key to unlock the front of the device.
Devices are placed inline between the card reader and the

remaining hardware. The data is not captured by a magnetic strip read, but from the communication of payment card
data from the reader to the embedded POS terminal. There is no trace of tampering from the outside of the gas pump
and Bluetooth transmission is typically utilized for retrieval of data. Gas pump skimming was more common in our
caseload than cases involving ATM assets; however the number of records lost is considerably lower.
Our caseload shows that ATMs and gas pumps are the most common assets targeted in skimming attacks, but they are
not the only ones. The USSS has investigated cases in which card readers, designed as physical access control
mechanisms to enclosed ATM locations (typically attached to banks and utilized for after-hours customers), have been
tampered with for the same intent as the ATM card readers. Point-of-Sale (POS) terminals have been targeted in
sophisticated tampering cases in which the devices are replaced with “new” devices redesigned to capture and store
payment card data as it is passed from the swipe reader to the terminal for legitimate processing. The capture and
exfiltration methods are similar to the gas pump skimmers, completely hidden inside the PED device and remote data
collection. Criminals have even incorporated social engineering methods, such as dressing in uniforms and identifying
themselves as technicians employed by the POS manufacturer. Upon arrival at the location, they inform staff that they
are replacing devices for scheduled maintenance and switch the legitimate devices for devices they control. The
majority of physical actions took place at the victim location in an outdoor area where, as one would expect, all gas
pumps and most ATMs are located.

The risk of mobile computing is a topic that Verizon’s RISK team continues to receive questions about. Both smartphones and
tablets have experienced phenomenal growth and equally phenomenal mind share in the past few years, and our clients
frequently ask us for recommendations around policies, processes, and controls for this class of assets. While we acknowledge
the growth of mobile computing and the increasing attractiveness of the platform to potential threats, we also must
acknowledge that again this year we have no representation of smartphones or tablets as the source of a data breach.

While we acknowledge the growth of mobile computing and the
increasing attractiveness of the platform to potential threats, we also
must acknowledge that again this year we have no representation of
smartphones or tablets as the source of a data breach.

It might be tempting to focus on the fact that 85% of breached
assets run Microsoft Windows, but it is important to note that the attacks used against these systems have little to do with
OS vulnerabilities; it’s not exactly rocket science to breach a system using default or easily guessable credentials.

One of the most frequent requests we’ve heard over the past few years is for data on
the operating systems of compromised assets. We’ve included that information this
year, trusting our readers will refrain from using it in “OS holy wars.”

Given the industry’s hyper-focus on cloud computing, we do our best to track relevant details during breach investigations and
subsequent analysis. As stated earlier in this report, we have yet to see a breach involving a successful attack against the
hypervisor. On the other hand, we constantly see breaches involving hosted systems, outsourced management, rogue vendors,
and even VMs (though the attack vectors have nothing to do with it being a VM or not). In other words, it’s more about giving
up control of our assets and data (and not controlling the associated risk) than any technology specific to The Cloud.
In other words, it’s more about giving up control of our
assets and data (and not controlling the associated risk)
than any technology specific to The Cloud.

Security attributes are exactly what they sound like they are: attributes that describe or pertain to the security of an
information asset. A security incident negatively affects one or more of these attributes.

News flash: 100% of all data breaches compromise the confidentiality of information assets. The fact
that integrity is involved in 90% of breaches may come as a surprise to some, but consider how many events occur during
a breach that can introduce unauthorized changes to a system. The installation of malware alone explains over half of that
number (all malware changes the original state of the system), and we haven’t even scratched the surface of what
intruders typically do once they own a system. Values drop way off after integrity. Examples of events affecting the
authenticity of assets are swapping a legit device for a phony one and initiating fraudulent transactions. The latter could
drive this attribute much higher, but our focus in the investigation is on how data was compromised rather than, for
instance, what criminals did with it afterwards (which often involves fraud of various kinds). Availability isn’t the main goal
for attackers interested in breaching data, but it is occasionally a casualty of war. For example, malware can bog down a
system even if doing so wasn’t its primary function. We did investigate an incident affecting the utility of information in
2010, but it was not a breach and thus not represented here (a terminated admin encrypted some data and tried to extort
his former employer). Possession losses aren’t represented because if we could not confirm actual compromise of data,
the case would not be included in this report.

The year also lacked
(as far as we know, at least) the headline-grabbing mega breaches that tend to drive up data loss so quickly. These external data
points suggest that something other than sheer caseload bias is at work.
It is worth mentioning that 3.8 million is actually a low-end estimate; we were unable to quantify data losses in almost a quarter
of all cases and other times could confirm only a portion of the total amount10. Still, increasing 3.8 million by 25% doesn’t change
matters in the least. It is possible that one of those unknown quantities was actually a mega breach, but we think not. None
exhibited the typical signs that accompany large data compromises we have worked in the past.

Our leading hypothesis is that the successful identification, prosecution,
and incarceration of the perpetrators of many of the largest breaches in
recent history is having a positive effect.
Cynics might argue that cybercriminals were just as active and successful in 2010, yet the breaches were never discovered. This
isn’t a stretch if you are familiar with the poor discovery-related findings we typically share in this report. However, this would
stipulate that criminals are either not using the stolen data or have found a means of bypassing Common Point of Purchase
(CPP) and other fraud detection mechanisms. CPP, however, remains the most frequent discovery method.
An optimist may interpret these results as a sign that we are WINNING! Sorry, Charlie, but we’re going to have to burst your
bubble. While we’d really like that to be the case, one year just isn’t enough time for such a wholesale improvement in security
practices necessary cut data loss so drastically. Plus, keep in mind that the number of incidents increased substantially (both in
our caseload and those publicly reported).

There are many reasons why ascertaining the full and exact amount of data stolen can be difficult. Some victims lack sufficient logs. Some destroy this information in trying to
respond to or contain the breach. Many attackers disguise, encrypt, erase, or otherwise make it difficult to access data in order to “count records.”
Now let’s turn to some explanations that do seem plausible. Our leading hypothesis is that the successful identification,
prosecution, and incarceration of the perpetrators of many of the largest breaches in recent history is having a positive effect. If
you consider that a rather small number individuals were tied to a disproportionately large number of breaches and/or breached
records, then you begin to get the sense that taking a few of them out could make a huge difference.
A corollary of the above is that the “second tier” of the criminal community has effectively been deterred from engaging in highprofile activity. Pulling off a huge heist might achieve fame and fortune, but it also attracts a lot of unwanted attention. Those
that wish to stay out of jail may have changed their goals and tactics to stay under the radar. This could be one of the chief
reasons behind the rash of “mini breaches” involving smaller organizations.
It is also possible that the talent pool is shallower than expected. Knocking off the kingpins could have precipitated a brain drain

of sorts in certain skillsets. We have circumstantial evidence of this, but
nothing concrete. For instance, a drop in certain techniques used by
certain criminals correlates with their arrest. But correlation, of course, is
not causation. It is also interesting that we consistently have a significant
portion of our caseload that ties back to the same individuals or groups.
If the attacker population were enormous, we wouldn’t expect to see
that in our sample year after year.
In addition to arrests, law enforcement has been busy infiltrating black
markets and other dark corners of the Internet where criminals
congregate, cogitate, and negotiate. Their presence is known and
stresses the tentative trust among thieves that exists in such communities.
This could disrupt the underground economy and account for some of
what we’re seeing.

The focus may continue to shift
in the future from payment card
numbers to other data types, such
as bank account data, personal
information, and even intellectual
property (more on this below).
These are not as flashy in the sheer
number of records lost, but can still
be lucrative to the criminal.

In the 2009 DBIR, we speculated that the flooding of the black market with millions and millions of stolen data records could
drive the price so low that releasing more would be disadvantageous. Criminals might opt to let the markets clear before
stealing more in bulk or selling what they already had. We could be in such a holding pattern now.
Furthermore, we have seen the scenario of large breaches and subsequent selling of card data on black markets replaced with
smaller captures and the direct use of the information for profit (recoding cards and making fraudulent ATM withdrawals). In
other words, the people behind the breaches are no longer becoming wholesalers after they capture the credit card information.
The focus may continue to shift in the future from payment card numbers to other data types, such as bank account data,
personal information, and even intellectual property (more on this below). These are not as flashy in the sheer number of records
lost, but can still be lucrative to the criminal. A single business’s bank account information, for instance, can result in a sizable loss
of money to the victim in the form of fraudulent Automated Clearing House (ACH) transfer.
Types of data compromised
When reviewing Table 15 for details regarding types of data compromised during breaches in the past year, results show that
payment card data maintains its predominance across the combined caseload. The 24% increase from 2009 is directly attributable
the large multi-victim cases worked by the USSS, which all had payment cards as the primary target (POS, gas pumps, ATMs, etc.).
Payment cards are desirable
to certain types of financially-motivated criminals because there are numerous established options for converting them to cash.

Other data types associated with fraud-for profit activities are personal information and
bank account data. Only one or two breaches
involved a substantial amount of records for either of these. For various reasons, quantifying an exact number was difficult in many instances, contributing to the lower
percentage of data loss. Not captured in the chart are the hundreds of millions of dollars lost through
fraudulent access to compromised bank accounts, identity theft, and other downstream crimes committed with this data.
Sensitive organizational data, intellectual property, and classified information still comprise a small proportion of compromised
data when compared more cashable forms of data. However, that the ratios remained similar to previous years even in the
face of huge gains in the number of smaller payment card breaches implies significant growth among these data types as
well. At a glance, this appears to concur with recent speculation that payment cards are passé and that IP is the new goal of
cybercriminals. This may well be true, but it’s a little too early to dub it a trend based on case evidence alone. Then again, it is noteworthy that the number of breaches involving such data has
never been higher in our caseload. It also should be noted that
the real rate of theft for IP and classified information is likely higher
than any sources (including ours) show. Since fraud detection
(e.g., CPP) is the most effective means of discovering a breach and
since IP isn’t used for financial fraud, then it stands to reason that
thieves could pilfer IP freely without being discovered. This is not
a comforting thought, but we’ll leave you with it anyway.

Authentication credentials were nabbed
in 45% of incidents in 2010, boosting it
to the second most compromised data
type. Stolen credentials are most often a
means to an end but are increasingly an
end in and of themselves.
They cannot, however, breach all organizations. Therefore, unless the perceived benefit is inordinately high, it is not
optimal for him to expend his limited resources on a difficult target while an easier
one is available. It also provides better understanding
of the criminals that are responsible for these crimes and what defensive measures
organizations should take to protect themselves.

Over the past few years, the percentage of highly difficult attacks has hovered somewhere in the mid-teens. This is an interesting finding and poses
some interpretive difficulties. This seems to be a
genuine shift (albeit not a dramatic one) away from highly difficult attacks in the past year.
An important observation is that this shift is more from “High” to “Moderate” than from “High or Moderate” to “Low or None”. The
sum of the top two difficulty levels in 2010 (57%) is basically the same as 2009 (59%) and higher than 2008 (48%) and years prior
(45%). Therefore, we cannot conclude that organizations are increasingly falling prey to simple attacks.
Another point to consider is that investigators noticed a higher proportion of automation with respect to attack methods in
2010. Those that once required some human finesse and guidance became a little more “fire and forget” and thus more
accessible to lesser-skilled assailants. Such attacks still have the same degree of effectiveness, but are not as difficult to pull off
(which is not a trend we want to see continue).
As has been true in the past, the more difficult parts of the attack sequence typically pertain to malware rather than the method
of intrusion (Hacking). Thus, our recommendation for prevention is still to focus on the front end. 90% of attacks are not highly
sophisticated, and the method of intrusion is relatively straightforward in most cases. Implement, double, and triple-check the
basics so that attackers are not granted a foothold from which to exploit your systems.
The exact manner by
which this flaw was identified is immaterial; the point is that the victim became a
target primarily because of an opportunity.
Targeted Attacks: The victim was first chosen as the target and then the attacker(s) determined a way to exploit them. This
doesn’t necessarily mean that a weakness or vulnerability wasn’t exploited to accomplish this; it simply means that opportunity
is not the primary reason for the attack.
Based on data collected by Verizon’s IR team in 2010, the ratio of targeted to opportunistic attacks remained similar to previous
years. The percentage of targeted attacks hovered in the high 20% range for 2008 and 2009 whereas it inched down a few
notches to 17% in 2010 (not a significant statistical change). The financial industry continued to experience a higher rate of
targeted attacks. The hospitality sectors (followed closely by the retail industry) were the highest victims of opportunistic attacks.
This was largely due to widespread knowledge in the criminal community about default credentials used for various types of
POS systems. Interestingly, more than half of all opportunistic attacks involved malware infections or hacking, some of which
included installation of RAM scrapers, keyloggers and/or backdoors on POS terminals and servers.
One finding that did constitute a significant change in 2010 was a sharp drop in the percentage of total records compromised
from targeted attacks. They accounted for 10% of records compromised compared to 89% and 90% for 2009 and 2008,
respectively. As with attack difficulty, this is mainly due to an absence of any mega-breaches in 2010, almost all of which have
been targeted in nature. Instead, we saw more targeted attacks at specific types of data that aren’t typically stolen in bulk, like
various types of sensitive organizational data and intellectual property. While this aspect may be in line with much of this year’s
media buzz around Aurora, APT, Stuxnet, and other highly targeted attacks, the general rule of thumb remains the same: Some
organizations will be a target regardless of what they do, but most become a target because of what they do (or don’t do).

The general rule of thumb remains the same: Some organizations will be a target
regardless of what they do, but most become a target because of what they do
(or don’t do).
Thus, our previous recommendation remains unchanged in that one of the fundamental self-assessments every organization
should undertake is to determine whether they are a Target of Opportunity or a Target of Choice. Those in the former category
should consistently seek to identify and remove opportunities to avoid needlessly attracting foes. Those in the latter category
should expect sophisticated attacks directed from skilled and determined adversaries. They should also expect the cost of
control to be much higher. However, remember that even Targets of Choice can fall to opportunistic attacks. Seasoned criminals
are not usually dumb and rarely work harder than necessary. Defend against dragons if you must, but don’t watch the skies so
much that common rogues slip inside the castle walls from below.

Instead of hunting for a user account with sufficient privileges, why not just use a keylogger or form-grabber to steal credentials
for one that you know will suit your needs? Such methods become evermore commonplace and we believe this results in
unknown unknowns being less prevalent across our caseload.
Not only were unknowns less prevalent, but the
amount of data compromised during breaches in
which they were a factor also realized a sizable
decline. In 2009, unknowns were found in just under
half of all cases, but those cases comprised over 90%
of all data stolen. A recurring theme of the 2010
caseload is the lack of “the big one”—a single breach
resulting in multi-millions of records lost. In the past,
most of these mega-breaches involved one or more
unknowns at some point in the event chain. That fact, combined with a lower amount of unknowns
observed in 2010, resulted in only 26% of data loss attributed to a case with one or more unknown conditions. The sharpest drop
occurred in “unknown privileges” and “unknown data,” both of which fell from around 90% of records lost to 26% and 21%
respectively in 2010. While such actions
taken by organizations may provide a partial
explanation for what we’re seeing, our gut tells us
that a shift in criminal tactics is the key factor in
this decline.
While we are glad to see the drop in the prevalence and impact of unknown unknowns, we doubt that the underlying problem
that allows them to exist has truly been addressed. Organizations should continue to strive to improve asset management, user
account management, dataflow analysis, and other practices that improve visibility across information assets. These efforts are
essential to a risk management strategy and will almost certainly pay dividends in the long run.

The timeline of an attack must be one of the least understood aspects of a data breach—and yet a good understanding of the
breach timeline can be of great importance for properly aligning defense and response mechanisms. We will again describe the
timeline of breach scenarios using three major phases. One could distinguish many more if desired, but we think this distinction
provides a clear overview and maps well to how incident response processes are typically organized.

The first phase depicts the time between the first entry into the victim’s environment to the moment when the data is located
and compromised. To use a more physical-world analogy, this is the time between the moment when the attacker first has his
foot in the door and the moment when he’s walking out the door with your belongings. In a substantial number of cases, the
desired data is not stored on the system that is the first point of entry. In fact, multiple steps are often required to conduct
reconnaissance on the network, locate the correct systems, and setup mechanisms to exfiltrate the data from the network.
Roughly one-third of breaches in 2010 reveal a timespan of mere minutes between entry and compromise (about the same
as 2009). To build upon the analogy above, these are cases in which the loot is lying just beyond the front door—i.e., on the
same system that was the initial target of the entry.
Similar to previous years, we continue to observe that in over half of cases, an attacker needs a minimum of “days” to
successfully complete this stage of the attack. Within that range, however, timeframes shifted noticeably away from “weeks/
months/years” end of the spectrum and into the “days” category. This shift was mainly a byproduct of the higher proportion
of automated attacks within 2010 caseload.

The former cases do not
require weeks of preparation, in fact, the attacker wants to install the skimmer as discretely and quickly as possible. Also, the
latter involves attacks that can be automated in order to share the same successful approach (or password) across a multitude
of victims.

Similar to previous years, we continue to observe that in over half of cases, an attacker
needs a minimum of “days” to successfully complete this stage of the attack.
As stated last year, a couple of days might not sound like a tremendously long time frame, but we’d like to counter this
argument. When someone attacks your network for several days, it allows for a greater opportunity for detection before
brains beat boxes and significant data loss occurs. We can and should take better advantage of that reprieve than we are now.
Compromise to discovery
In past years, our reports have shown that victims generally allow a breach to exist for weeks, months, and even years before realizing
they’ve been had. 2010 in this regard looks similar, though there was some minor movement among the timeframes. “Weeks”
appears to be the gainer, taking share away slightly from the leftmost categories. We’d much rather see a mass migration from the
right, which would indicate an improvement in discovery capabilities.

Regardless of the timespan involved, once an organization realizes that they have been the victim of a breach, quick and
effective remediation should be their first objective. We should mention that containment is not defined as the phase in
which everything is back to normal, but rather when the data outflow has been stopped. To return to our now somewhat
wearied analogy, the door or window is closed and a temporary lock has been installed. However, it’s still a long way from a
restored operating environment.
Here, the combined dataset again shows a tendency to shift towards the center as compared to last year, with more breaches
taking weeks to contain. The higher proportion of smaller organizations that generally don’t have any incident response policy
or staff in place is an important contributor to this result. For these victims, the level of effort required to ultimately contain the
data breach is low, but the process from initial breach discovery to uncovering the breach methods and taking the necessary
steps to contain it is often beyond their capabilities. We have also noticed that a tendency exists for displacement of responsibilitiy
when small businesses are the victims of a data breach. Because they usually assume little responsibility for their IT functions,
they believe that the vendor who sold them the POS software or terminals holds the responsibility to take action. This may or
may not be the case, but the resultant confusion and ambiguity reinforces the fact that organizations of all sizes must have some
level of preparation around incident handling and response.

To quote last year’s report: Proper Planning Prevents Poor Performance. This
mantra can expedite the containment of incidents, while ensuring that actions
taken preserve evidence for investigative needs.
To quote last year’s report: Proper Planning Prevents Poor Performance. This mantra can expedite the containment of incidents,
while ensuring that actions taken preserve evidence for investigative needs. This does not mean that organizations have to
practice complicated technical forensic procedures, but rather that they should think about responsibilities and chain of
command, define a “freeze point” at which they need to engage external consulting, and ensure practical matters like network
diagrams and contact details are up to date and available. Moreover, after the incident is contained, reviewing lessons learned
and applying those to future planning is essential.

Investigating as many breach cases as we do, we encounter a myriad of different situations upon our arrival on scene. Some of
these are quite unique, but most are all too familiar variations on a common theme, a theme in which unpreparedness, panic
and the blame game play a major role. While these repetitious occurrences can be frustrating to investigators in the field,
they do serve to provide us with opportunities to illustrate to our readers things to avoid during a breach event. We hope
these ‘ghost stories’ will provide the reader with a bit of insight into common problems encountered For instance, we often
see the shade of “DIY” in victim organizations. The scenario plays out like this: a breach has been discovered, the IT and
security staff try to solve the problem but lack the required training and procedures to do so. The weekend is fast approaching,
and management begins to panic. It’s often at this precise moment, typically late Friday afternoon, that we get the call.: “We
think we have a problem and we have worked on it for the past couple of days—but can you please come and help us out?” Of
course, by now precious time is lost and the well-intended actions of the in-house group have complicated the investigation or
even spoiled the evidence. While it isn’t crucial that every part of an incident response is outsourced, it is vital that the
limitations of the internal group’s knowledge and skillset be known, and a proper escalation path be in place.
Once we do finally arrive onsite, one of the first things we ask for is a network diagram of the involved systems. Typically, this
elicits a response such as: “Well, we have one, but it’s a little bit outdated. We have decommissioned a few systems, and
added a few new environments. Oh, and I meant to include the merger we did last year.” You get the idea. In these situations,
we have found that the fastest and most reliable method is to use the “consensus network diagram”. This involves getting
everyone with knowledge about the involved systems in a room, giving them a whiteboard and a marker, and asking them to
start drawing. It takes a little while, but after everyone provides input there is generally a reasonably usable diagram on the
board. This sounds like a simple or even a pleasant exercise, but when you remember that meanwhile valuable data is still
leaking from the company, and the frantic CEO is demanding updates. In hindsight, it might have been preferable to have
done some of this work beforehand.
Another specter that frequently rears its ugly head is that of the disappearing backup. Theoretically, backups are great for
investigative purposes. Who wouldn’t want to be able to go back in time to see what happened on a system? Unfortunately,
many backup systems are built and managed with business continuity solely in mind and, therefore, are only capable of
restoring full backups. In such cases, the victim organization often needs to arrange a complete server to restore the backup
to. Not impossible to do, but, again, something that takes valuable time which could have otherwise been saved. We
recommend our readers avoid this situation by the simple expedients of either changing the backup software used or
pro-actively ensuring that a spare server is available.
The challenges that arise during a breach are not always of a technical nature. Perhaps the most feared fiend of all is that of
the third party contract. The typical Service Level Agreement, of course, has a fast response time for those problems deemed
most urgent. Unfortunately, “most urgent” is often defined as “an important or critical system being down.” Strange as it may
sound, data leakage often doesn’t fall into a category that warrants the highest priority and fastest response. Luckily, some
outsourcing companies have the correct mindset about such matters, and tend to try to give priority to those situations that
are clearly urgent. However, some stick to the contract and respond with “We will provide the requested log file within 24
hours, according to the SLA for a medium priority incident.” In one case, the victim took more than three weeks before
delivering firewall logs to the investigative team. This was because the outsourcing company that managed the system could
not locate the physical system, to which they had to attach the external hard drive, within their own datacenter. When
outsourcing data, the wise professional will make certain that the protocols for accessing said data during a crisis are fully
understood and are acceptable to the organization.

One of the many benefits to studying breaches en masse rather than the myopic view of a single investigator’s caseload or even
one team’s caseload is that it allows one to spot things that would otherwise go unseen or appear unremarkable. A bird’s eye
view of the ways and means behind breach discovery is one of those areas where this is especially useful. And as veteran readers
know, the view hasn’t been very pretty.
The Verizon RISK Team uses three main categories to describe discovery methods: External, Internal Passive, and Internal Active.
External discovery is fairly self-explanatory; the breach was discovered by an external source and then reported to the victim. For
internal discovery we classify incidents as being discovered by Active methods (those that arise from processes specifically
designed for detection) or Passive methods (those in which the evidence of breach arises from non-security processes).
Over the past few years, we have been closely monitoring this
data, since one might argue that the method of breach discovery could act as a sort of “canary in the coalmine” for the ability of our
victims set to detect and respond to security incidents. Data
around how victims discover the breach would be an indicator of how well they know and monitor their own environment.

The most common third party detection method is Common Point of Purchase analysis, or CPP. At a very basic level, CPP
identifies probable breach victims based on the purchase histories of stolen payment cards. Banks use it to limit their financial
losses due to fraudulent transactions, and it works quite well for that purpose. Unfortunately, for CPP to work, the thief must
begin committing fraud with the stolen cards. Notification by law enforcement can happen any number of ways. Very often—
especially in this particular caseload—law enforcement personnel learn of and alert numerous victims as they identify, research,
and monitor suspects. Sometimes confidential informants provide information on the activities and victims of other criminals.
Other third party external methods include notification by customers/business partners and in some small number of cases,
braggadocio on the part of the threat agent.

Internal active discovery relates to IDS/IPS/HIPS, log monitoring and other like technologies that security departments typically use to prevent, detect, and
respond to data breaches. Unfortunately, as referenced above, many smaller
organizations do not have the awareness, aptitude, funding, or technical support to perform these tasks on par with the sophistication of the threats they face. That said, past history has shown that even large businesses seem to have a difficult time utilizing their investments for significant return.

We often joke (though it’s really not funny) that criminals seem to have better
ownership, insight, and control over the environment than the organization
paying the bills. Again this year we see a small representation of Internal Active
methods; only ~6% of the time did an organization’s designed security efforts detect the breach. In the experience of the investigation team, many of these
technology controls are either misconfigured, in the wrong place, or—as is far too often the case—not being utilized at all. For
example, one breach victim had recently purchased a SIEM system, but then let the admin go to save cost. We showed up to
find it brimming over with alerts pointing to the breach, which was of great use to us, but not so much for them. Again there
doesn’t appear to be a club big enough for this dead horse; it might be a great idea to leverage existing technology investments
to help detect and respond to data breaches.
If there is one positive note that we can squeeze out of these statistics around active measures, it’s that discovery through log
analysis and review has dwindled down to 0%. So the good news is that things are only looking up from here. Yeah, that’s
squeezing pretty hard, but what else can we do? Our casework continues to show that good evidence of the breach usually
exists in the victim’s log files waiting to be used. See the “On logs, needles and haystacks” sidebar in the 2010 DBIR for a few tips
on smart and cost effective ways to analyze logs.
Internal Passive Discovery
Internal Passive is best described as when someone who is not responsible for security reports the signs of an incident. Having
people aware of the signs of a security incident and knowing what to do when the tell tale signs of a compromise appear is a
wonderful thing, a bit like free beer. The depressing alternative is when our investigators hear stories from users about how they
noticed strange things on a system they were using but did not report it because they did not know how to report it, or did not
feel it could be important. To take advantage of this “free beer” we recommend that every organization should have a good
security awareness campaign, and that they test their people frequently to make sure they understand what the signs of
compromise might be for their system, and what to do if they see them. As we said last year, evidence of compromise is not
always in the form of subtle indicators that appear in log and event histories admins might be encountering, but rather in
obvious, noticeable change that should have been investigated.

With all the industry buzz around new and advanced threats, you might have anticipated a radical increase in the use of antiforensics. After all, if you want to be truly persistent, it will likely require repeated access to the victim’s environment and data—
each time with the possibility of leaving behind a digital footprint or two. Then again, if you happen to have budgets and
resources that most of us only dream about (perhaps the backing of a nation-state?), then wouldn’t you take advantage of antiforensics? And if you did, would there be any trace of your doing so?

The fact of the matter is that for the entire period that we have been studying breaches, we have seen consistent signs of antiforensics. Based on the most recent evidence, anti-forensics was used in approximately one-third of 2010 breaches worked by
Verizon. That represents neither a significant increase nor decrease over the prior year. The important thing to note here is that
these numbers are based on evidence. That is, hard facts collected during an investigation. Since the whole purpose of antiforensics is to remove such evidence, pessimists among us might view that third of breaches as the error rate for anti-forensics
rather than the usage rate. A different kind of pessimist might accept one-third as the usage rate and chalk the remaining gap
up to non-existent logging and self-inflicted anti-forensics performed by the victim. Either way, we can only report what we see.
While the overall use of anti-forensics has remained relatively flat, the techniques deployed have an ebb and flow to them.
Previously, the most common form of anti-forensics observed in the field was Data Wiping, leading well ahead of all others. The
prior pervasiveness of Data Wiping, which includes removal and deletion of evidence, came as no surprise. However, in the last
year we have seen Data Hiding (~40%) pull up as a much closer second place to Data Wiping (~57%). With respect to Data
Hiding, the use of steganography has remained relatively rare and flat year-over-year. The use of encryption for the purposes of
Data Hiding has again contributed most significantly to the rise in Data Hiding. It could be opined that this is potentially a
response to the wider usage of DLP or FIM solutions that might otherwise detect clear-text repositories of soon-to-be-exfiltrated
data. Where Data Corruption (~4%) was observed, it continued to be mostly manifested as log tampering.

The fact of the matter is that for the entire period that we have been studying
breaches, we have seen consistent signs of anti-forensics. Based on the most
recent evidence, anti-forensics was used in approximately one-third of 2010
breaches worked by Verizon. That represents neither a significant increase nor
decrease over the prior year.
It is also interesting to consider these AF numbers in connection with the total quantity of breaches (up) and the total quantity
of records compromised (down) that are covered in this study. The steady anti-forensics usage in the face of a much smaller
records-per-breach ratio would tend to support the notion that anti-forensics is a tool for the masses and not limited to the elite
criminals or highest-value targets. In many cases, the anti-forensic tools being used are found to be common across multiple
cases. This likely ties into the increasing underground marketplace for “malware-as-a-service.”
This continues to be a trend of interest to our investigative team as the use of anti-forensics plays a significant role in daily
activities. We will continue to monitor and report on the evolution of anti-forensics.
You Down With CPP? CPP is a method that banks employ to limit their financial losses due to fraudulent transactions. Let’s
say 200 cardholders all experienced fraudulent purchases on their credit cards. CPP analysis would look at the purchasing
history of these cardholders and try to find a common point of sale (e.g., stores) which they all shared. This is essentially
crunching data in such a way that the algorithm determines that all cards in question were used at StoreX in a given period
of time. Timeframing, history, geographic location, and many other data points are then used to determine if a particular
common point of purchase could be considered to have a high probability of incident.
CPP has the advantage of seeing through the fog within an organization by highlighting the glaringly obvious issues from
without. A scary thought about CPP is that this detection method is so successful because there is a mechanism (fraud) for
correlating the data together. Other types of valuable data such as personal information, health records, e-mail addresses, and
authentication credentials can often be harvested from many places, but they do not have the same protective mechanisms as
payment cards to detect the data breach. Thus, we believe the numbers around non-payment card breaches are far worse than
reported since there is no CPP like mechanism to detect their loss.

The Payment Card Industry Data Security Standard (PCI DSS) is a set of control requirements created to help protect cardholder
information. Every year Verizon’s caseload contains a number of organizations that are required to adhere to the PCI DSS. Because
these are confirmed data breach victims, obvious questions arise with respect to the compliance status of these organizations.
This includes network and data defense technology basics (firewalls, anti-virus, identity and access
management), as well as the non-technical aspects of security and risk management (policy and process development).
While the above refers to the victim’s status based upon their last official validation, another important line of inquiry relates to
their state when the incident occurred. When our investigators work a case in which the victim organization processes payment
cards, a review is conducted of which PCI DSS requirements were and were not in place at the time of the breach. The results of
this assessment are recorded, appended to the case report, and then conveyed to the relevant payment card brands. This work
is not an official PCI DSS audit, nor does it either uphold or overrule the victim’s compliance status. That said, it does provide
insight into the condition of the security program of the victim organization at the time.
In the incident report delivered to the card brands, investigators break down compliance by PCI DSS requirement. If the DSS
represents the basics of an information security program, then we are able to get a high-level understanding of the state of
the security program at the time of investigation. In Table 16 we present the results of these assessments over time.

Despite the rather poor showing, let’s see what we can learn. Firewalls, Anti-Virus, changing default credentials, and related concepts could all be found in “best
practice” documents for information security from 15 years ago (or more). So, either the “security message” isn’t reaching smaller
businesses or we, as an industry, are not explaining the benefits well enough for them to make the extra effort, or they aren’t
willing or compelled to do so for various other reasons.
In addition, low marks in other technical aspects of the PCI DSS (Requirements 3, 8, 10, 11) are similar to the areas that our QSA
clients struggled to meet. The association here is too strong to ignore; PCI compliance is not easy, and security is not something
to be addressed once every twelve months. Installing and maintaining a firewall configuration to protect data, developing and
maintaining secure systems and applications, restricting access to data by business need-to-know, tracking and monitoring all
access to network resources and cardholder data, and maintaining a policy that addresses information security (Requirements
1, 6, 7, 10, 12) are all aspects of the DSS that need an investment in continuous processes and upkeep to be effective.
What does appear to be working are areas where the security-conscious aspects of our industry can “bake security in.”
Requirement 4, “Encrypt transmission of cardholder data and sensitive information across public networks,” is one that has been
increasingly addressed by hardware and software vendors, as well as the vendor management programs of banks and card
processing vendors. We see Requirement 4 holding steady at around 90% compliance in victim environments over the past
two years.
We’ll end this year’s PCI section on a pragmatic note. One of the lingering questions from our discussions around PCI in this report
is always that of relevancy. It’s all well and good to validate compliance with the PCI DSS, but does it actually help reduce risk? Insofar
as that translates to a sincere security program—one that seeks to maintain validation on an ongoing basis—the data strongly
suggests the answer is “yes.” Let’s examine some of the results in Table 16 in light of threat actions discussed earlier in this report.

One of the lingering questions from our discussions around PCI in this report
is always that of relevancy. It’s all well and good to validate compliance with
the PCI DSS, but does it actually help reduce risk? Insofar as that translates
to a sincere security program—one that seeks to maintain validation on an
ongoing basis—the data strongly suggests the answer is “yes.”
The first and perhaps most noteworthy example of this would be found in Requirement 2 (Do not use vendor-supplied defaults
for system passwords and other security parameters). In our previous section on Hacking, we find that “exploitation of default or
guessable credentials” is represented in two-thirds of all intrusions and accounts for nearly one-third of all records compromised.
Similarly, “exploitation of insufficient authentication” is found in 10% of all intrusions and ascribed to 21% of all records breached.
Requirement 5 (Use and regularly update anti-virus software) can be directly mapped to the high frequency of malware used to
compromise systems and data. Sure, over 60% of malware is customized and not likely to be detected by AV, but that means
about 40% stands a decent chance of being recognized. Who doesn’t want a 40% reduction in risk?
When malware isn’t recognized by AV and is installed on the system, all is not lost. Requirement 1 (install and maintain firewall
configuration) and Requirement 10 (track and monitor all network access) are a critical second line of defense against backdoors
and other common types of malware and intrusion methods.
Let’s do one more (though we could go on for some time). Requirement 6 (Develop and maintain secure systems and applications)
and Requirement 11 (Regularly test security systems and processes) are both important processes that relate to the broader
category of Hacking (50% of breaches/89% of records). Because Hacking is often used in order to install malware, secure
development and testing can be considered to reduce the risk of that threat action as well (page 24, 49% of breaches/79%
of records).
Every year that we study threat actions leading to data breaches, the story is the same; most victims aren’t overpowered by
unknowable and unstoppable attacks. For the most part, we know them well enough and we also know how to stop them.
Mapping common threat actions from 1700+ confirmed breaches to PCI DSS requirements simply does not reveal many gaping
holes or grossly inadequate coverage. Does that mean the DSS is perfect? Not at all; few things are. Fortunately, perfection is not
a precondition for significant risk reduction benefits.
Your security
woes are not caused by the lack of something new. They almost surely have more to
do with not using, under using, or misusing something old.
The argument levied against that notion is that our adversaries are clever
rascals and will adapt in order to our thwart our “old” defenses. That is true
(and we’ve seen and discussed evidence of such adaptation), but let’s be real,
shall we? As a whole, do you really think we’re making them scramble to
adapt? Year after year our data seems to suggest that we are not, and that is
something that needs to change. If they adapt, then they adapt. C’est la vie.
But let’s quit allowing them to find success in stagnation.
To that end, we’ve found some old recipes for achieving newfound success.

We examined top attacks from 2010 and identified recommendations from
our previous reports most applicable to them. They are categorized and
listed below and we hope they help you at the planning and budget
negotiations table.

Achieve essential, and then worry about excellent: We find that many organizations achieve very high levels of security in
numerous areas but neglect others. Criminals will almost always prefer the easier route. Identifying a set of essential controls and
ensuring their implementation across the organization without exception, and then moving on to more advanced controls
where needed is a superior strategy against real-world attacks.

The argument levied against that notion is that our adversaries are clever rascals
and will adapt in order to our thwart our “old” defenses. That is true (and we’ve seen
and discussed evidence of such adaptation), but let’s be real, shall we? As a whole,
do you really think we’re making them scramble to adapt?
Access Control
Change default credentials: Simple and sweet, when system/network admins stand up a new system, change the password. If
you outsource this to a third party, check that they’ve changed the password. Don’t assume that your staff or your partners
consistently follows through on all policies and procedures. Along with changing default credentials, organizations should
ensure that passwords are unique and not shared among users or used on different systems. This was especially problematic for
assets managed by a third party.
Every year that we study threat actions leading to data breaches, the story is the
same; most victims aren’t overpowered by unknowable and unstoppable attacks. For
the most part, we know them well enough and we also know how to stop them.
User account review: Prior year’s data breach reports and years of experience lead us to believe in the value of reviewing user
accounts on a regular basis. The review should consist of a formal process to confirm that active accounts are valid, necessary,
properly configured, and given appropriate (preferably least) privileges.
Restrict and monitor privileged users: Trust but verify. Use pre-employment screening to eliminate the problem before it starts.
Don’t give users more privileges than they need (this is a biggie) and use separation of duties. Make sure they have direction
(they know policies and expectations) and supervision (to make sure they adhere to them). Privileged use should be logged and
generate messages to management. Unplanned privileged use should generate alarms and be investigated.
Secure remote access services: In many instances, remote access services have been enabled and are Internet-facing. We
recommend tying these services down where only specific IP addresses or networks can access them. Additionally, it’s important
to limit access to sensitive systems within the network. Many organizations will allow any device on the network to connect and
remotely access any other device; we highly recommend not managing your devices this way. Tie down remote access services
to specific management networks via access control lists.
Monitor and filter egress network traffic: At some point during the sequence of events in many breaches, something (data,
communications, connections) goes out that, if prevented, could break the chain and stop the breach. By monitoring,
understanding, and controlling outbound traffic, an organization will greatly increase its chances of mitigating malicious activity.

Application testing and code review: SQL injection attacks, cross-site scripting, authentication bypass, and exploitation of
session variables contributed to nearly half of breaches attributed to hacking or network intrusion. It is no secret that attackers
are moving up the stack and targeting the application layer. Why don’t our defenses follow suit? As with everything else, put out
the fires first: even lightweight web application scanning and testing would have found many of the problems that led to major
breaches in the past year. Next, include regular reviews of architecture, privileges, and source code. Incorporating a Security
Development Life-Cycle (SDLC) approach for application development is recommended as well. Finally, help your developers
learn to appreciate and write more secure code.

Enable application and network witness logs and monitor them: All too often, evidence of events leading to breaches was
available to the victim but this information was neither noticed nor acted upon. Processes that provide sensible, efficient, and
effective monitoring and response are critical to protecting data.
However, don’t just focus your logging efforts on network, operating system, IDS, and firewall logs and neglect remote access
services, web applications, databases, and other critical applications. These can be a rich data set for detecting, preventing, and
investigating breaches.
Define “suspicious” and “anomalous” (then look for whatever “it” is): This is admittedly vague, but—in truth— generalizing
what this entails in order to prescribe something for everyone would counteract the point. Discover what is critical, identify what
constitutes normal behavior, and then set focused mechanisms in place to look for and alert upon deviations from normality.

Change your approach to event monitoring and log analysis: Based on the data we collect in the Time of Breach events, we
believe that organizations would be better served to focus less on the “real-time” methods of detection, and more on the “thisweek” methods. If we can shift Compromise to Discovery time frame from Weeks and Months to Days, it will significantly reduce
the damage done to your organization. Focus on the obvious things rather than the minutia. This need not be expensive; a
simple script to count log lines/length and send an alert if out of tolerance can be quite effective. We are confident that this
approach will reap benefits and save time, effort, and money.

Increase awareness of social engineering: Educate employees about different methods of social engineering and the vectors
from which these attacks could come from. In many of our cases, we see where users click on links they shouldn’t and open
attachments received from identified persons. Reward users for reporting suspicious e-mail and sites and create the incentives
necessary for vigilance.
Train Employees and Customers look for signs tampering and fraud: Such awareness campaigns have been around in certain
areas for some time, but ATM and Pay-at-the-Pump tampering/fraud seem to be increasing in number and scope. Organizations
operating such devices should consider conducting regular examinations of them. Additionally, empower customers to help
protect themselves as well as aiding the organization in spotting potential issues.

Create an Incident Response Plan: If and when a breach is suspected to have occurred, the victim organization must be ready
to respond. An effective Incident Response Plan helps reduce the scale of a breach and ensures that evidence is collected in the
proper manner.
Engage in mock incident testing: I mean listen, we’re sitting here talking about practice; not an incident, not an incident, not
an incident—but we’re talking about practice (sports fans among you might get that reference). Yes, we are talking about
practice, because practice makes perfect. In order to operate efficiently, organizations should undergo routine IR training that
covers response strategies, threat identification, threat classification, process definition, proper evidence handling, and
mock scenarios.

One of the most critical and persistent challenges plaguing efforts to manage information risk is a lack of data. As community
decision-makers and practitioners, we have little data because we do not share and while there are many reasons for this, doubts
that it can be done in a practical, private, and mutually beneficial manner are chief among them. We would like to think that this
report is an example that sensitive and useful data can be shared responsibly to the benefit of many. In the past two years,
several other investigative firms have begun to share their results and we commend those efforts.

Well here we are again, and it is time to take the annual journey into our
collection of real-world data breaches and information security incidents from
the prior year.
No locale, industry or organization is bulletproof when it comes to the
compromise of data. Some are notably more represented than others and this
is not an indictment that the public sector is any less secure than any other
industry. As with prior years, the numbers that follow are heavily influenced
by US agency reporting requirements, which open up the fire hose of minor
security incidents.

Playing a part on the blue team in information security can, to a very small
degree, be compared to the lot of a hapless soldier. The soldier is told to guard
a certain hill and to keep it at all costs. However, he is not told who his enemy
may be, what they look like, where they are coming from, or when (or how) they
are likely to strike. To ride this analogous horse a bit further, the soldier is given
a hand-me-down rifle with only a few rounds of ammunition to fulfill his task. It
seems a bit unfair really—even the American Revolution got Paul Revere.

Be prepared:
forewarned is
forearmed.

With that in mind, we hope that this section and the facts and figures contained
in it will go some way toward making you better prepared than our friend
mentioned above. After all, “forewarned is forearmed.”

There was never any real danger of the financial motive losing its prominence,
as even at its peak, espionage remained a far distant second.
Many of the attacks discussed in this report have what we call a
‘secondary motive’, which we define as when the motive of the incident
is to ‘aid in a different attack’. We filter these out of the report because
it would overshadow everything else if we didn’t. One example is where
the bad guy compromises a web server to repurpose it to his own uses
(e.g., hosting malicious files or using it in a spam or DoS botnet). Even
criminals need infrastructure. “It is a far, far better thing” that someone
else manages it for free, rather than having to pay for it yourself. We
had thousands of these incidents, as well as poorly configured NTP
and DNS servers, leveraged to launch reflective DoS attacks.

Pistols at dawn, or knives at noon?
Now that we know at least a very little bit more about who’s coming after us,
the next logical question is: how are they armed? We also see the calling card of Point-of-Sale
(POS) attacks. No need to go get in the weeds on this here, as these topics will
reappear quite a bit in the pages to follow.

Nevertheless, at this point, we think both Phishing and Point-of-Sale could
safely say, in their best Ron Burgundy voice, “You might have heard of me,
I’m kind of a big deal.” Due to this rock-star status, we’re going to dig a little
deeper into POS attacks later in the Patterns section and also in the PostCompromise Fraud write-up. Likewise, we discuss phishing in greater detail
in the Phishing section and Cyber-espionage pattern. We even have a section
on credentials this year. Credentials have made numerous cameo appearances
in this report for years, but never before have they had a speaking part.
Always a bridesmaid, never a bride.

Guess what? When the bad guys’ actions are centered around phishing and
POS devices, the asset varieties displayed in Figure 6 reflect this. That lovely
“Person” line trending up is due to the human asset falling victim to phishing
attacks. The “User device” line upward trend is based on desktops being
infected with malware, as well as POS terminals getting popped.

Mick was wrong—time is not on our side.
Rome wasn’t built in a day, but data breaches frequently were. The
large spikes, however, are driven by very specific threats. The compromise
time of minutes, while depressing to look at, is actually another reflection of
the ubiquitous ‘Dridex’ breaches in this year’s dataset. As previously alluded
to, these cases begin with a phish, featuring an attachment whose mission in
its malware life is to steal credentials. If you have legit creds, it doesn’t take
a very long time to unlock the door, walk in and help yourself to what’s in the
fridge. Conversely, the exfiltration time being so weighted in the ‘days’ category
is heavily representative of attacks against POS devices where malware is
dropped to capture, package and execute scheduled exports.
Bad news travels fast, with one exception.
We continue to add incidents and breaches to prior
calendar years post-report to enrich our data. Also, some breaches will occur
late in the year and are discovered the next year.
To add another ray to this sunbeam, attackers are getting even quicker at
compromising their victims. When you review the leading threat actions again,
this really won’t come as a surprise. The phishing scenario is going to work
quickly, with the dropping of malware via malicious attachments occurring
within seconds. Physical compromises of ATMs and gas pumps also happen
in seconds. In the majority of confirmed data breaches, the modus operandi of
nation-states as well as financially motivated attackers is to establish control
via malware and, when successful, it is lightning fast. As this figure is for
confirmed breaches only, it makes sense that the time to compromise is almost
always days or less (if not minutes or less). If—and some have called “if” the
biggest word in the language—there’s any good news, it’s that the number of
breaches staying open months or more continues to decline slightly.

The time to
compromise is
almost always
days or less, if not
minutes or less.

When it comes to external 5 breach discovery, fraud detection and law
enforcement notification are battling it out like the Celtics and Lakers in
the ‘80s. All in all, external notification is up. And when
you have to wait on external detection to tell you you’re popped, it’s
probably too late to keep the horses in the barn.


One last thing before we get to the patterns. There are a couple of topics that
are omnipresent in many of the patterns that we use to classify incidents. While
they will receive credit where credit is due, in the pattern sections, we feel that
we also need to put the spotlight on them here.
We have numerous breaches where we can infer that some Common
Vulnerabilities and Exposures (CVE) were used in order for the attack to
advance. Hey, we’re looking at you, drive-by downloads! Unfortunately, we don’t
have a tremendous amount of CVE data in our corpus, either because it was
not measured or was unable to be identified. This lack of detail makes us an
embarrassment of sad pandas. (Yes, we wanted to say “sleuth”, but apparently
we can’t. Look it up.) Luckily we have contributors in the vulnerability space that
can lighten our mood.

We don’t have a
tremendous amount
of CVE data because
it wasn’t measured
or was unable to be
identified.

Phishing has continued to trend upward (like spawning salmon?) and is found
in the most opportunistic attacks as well as the sophisticated nation state
tomfoolery. We feature a section where we dive into the human element a bit
deeper, with some data on our innate need to click stuff.
Lastly, we strike a deceased equine a bit more with a section on
credentials (of the static variety). Don’t get us wrong—passwords are
great, kind of like salt. Wonderful as an addition to something else,
but you wouldn’t consume it on its own.

A look into software vulnerabilities, whether we are
making any progress in addressing them and ways
to improve.

Older vulnerabilities are still heavily targeted;
a methodical patch approach that emphasizes
consistency and coverage is more important
than expedient patching.

New vulnerabilities
come out every day.


The visualizations and statements regarding rates of exploitation in this section
are underpinned by vulnerability exploitation data provided by Kenna Security.
This dataset spans millions of successful real-world exploitations, and is
derived from hunting down exploitation signatures in security information and
event management (SIEM) logs and correlating those with vulnerability scan
data to find pairings that would be indicative of a successful exploitation.

Vulnerability management has been a Sisyphean endeavor for decades. Attacks
come in millions, exploits are automated and every enterprise is subject to the
wrath of the quick-to-catch-on hacker. What’s worse, new vulnerabilities come
out every day. Since the first DBIR, we’ve been advocating the turtle’s approach
to vulnerability management (slow and steady wins the race).
This year we revisit this data to see whether the trends hold, but in typical DBIR
fashion, we dig a little deeper, to look at not just how attackers are interacting
with vulnerabilities (exploitation), but also how well and how fast enterprises are
executing remediation. If we can measure both of these routinely, then we can
provide much-needed answers about how the tortoise won the race—and so
learn how to close the gap between attackers and enterprises.

This year we take a different approach to measuring the time from publication
to exploitation. Figure 10 is a box plot, which plots the time between publication
and the first observed successful exploit by vendors. 6 We can see that Adobe
vulnerabilities are exploited quickly, while Mozilla vulnerabilities take much
longer to exploit after disclosure. Half of all exploitations happen between
10 and 100 days after the vulnerability is published, with the median around
30 days. This provides us with some general guidelines on which software
vulnerabilities to prioritize along with some guidance on time-to-patch targets.

When it’s below
zero, remediation efforts are driving down vulnerability counts faster than new
vulns are entering the enterprise.
Basically, we confirmed across multiple datasets that we are treading
water—we aren’t sinking in new vulnerabilities, but we’re also not swimming
to the land of instantaneous remediation and vuln-free assets. However, all
that patching is for naught if we’re not patching the right things. If we’re
going to tread, let’s tread wisely.

All that patching
is for naught if
we’re not patching
the right things.

What should we mitigate? Hacker economics.
So what are the right things?

Hackers use what works and what
works doesn’t seem to change all that often. Secondly, attackers automate
certain weaponized vulnerabilities and spray and pray them across the internet,
sometimes yielding incredible success. The distribution is very similar to last
year, with the top 10 vulnerabilities accounting for 85% of successful exploit
traffic. While being aware of and fixing these mega-vulns is a solid first
step, don’t forget that the other 15% consists of over 900 CVEs, which
are also being actively exploited in the wild.

This gets at a core and often ignored vulnerability
management constraint—sometimes you just can’t fix a vulnerability—be it
because of a business process, a lack of a patch, or incompatibilities. At
that point, for whatever reason, you may have to live with those residual
vulnerabilities. It’s important to realize that mitigation is often just as useful as
remediation—and sometimes it’s your only option.

Mitigation is often
just as useful as
remediation—and
sometimes your
only option.

Knowledge is power.
Establish a process for vulnerability remediation that targets vulnerabilities
which attackers are exploiting in the wild, followed by vulnerabilities with known
exploits or proof-of-concept code.
Have a Plan B.
If you have a system that cannot be patched or receive the latest-and-greatest
software update, identify it, and apply other risk mitigations in the form of
configuration changes or isolation. Discuss a plan on how the device(s) could
be replaced without causing severe business disruption.

Vulnerability scanning is also useful in identifying new devices and new
services. Review scan-to-scan changes as another control to identify unknown
devices and deviations from standard configurations.
Verizon 2016 Data Breach Investigations Report

The majority of
phishing cases
feature phishing as
a means to install
persistent malware.

You can’t fool all the people all the time. Or can you?
Social engineering in its basic form is simply to dupe or trick someone into
doing something they would not otherwise do (not unlike some online dating).
Social tactics can take many forms such as pretexting, elicitation (the subtle
art of extracting information from a subject via conversation), baiting (planting
infected media in victim areas), and a myriad of other lowdown and dirty tricks.
However, by far its most successful variety is phishing, which as the name
implies is malicious correspondence trying to get the recipient to take the
bait in the form of an attachment or embedded link. It is important to note that
‘pretexting’ via email (a back-and-forth dialogue leveraging an invented scenario
to gain a certain end) and a phishing email are similar, but not the same. In the
case of a pretexting email, the criminal is primarily purporting to be someone
they are not, usually within the victim organization (e.g., the CFO who instructs
the victim to approve a fraudulent Automated Clearing House (ACH) transfer).
The basic structure of phishing attacks remains the same—user clicks, malware drops, foothold is gained. There are still cases where the phishing email leads users to phony
sites, which are used to capture user input, but the majority of phishing
cases in our data feature phishing as a means to install persistent malware.
The victim opens the email, sees the attachment that contains the malware du
jour and says “That file looks good, I’ll have that”. What happens next is dictated
by the end goal of the phisher.
“What we have here is a failure to communicate.”
Apparently, the communication between the criminal and the victim is much
more effective than the communication between employees and security staff.
We combined over eight million results of sanctioned phishing tests in 2015
from multiple security awareness vendors aiming to fix just that. Figure 14 is
jam-packed with information. In this year’s dataset, 30% of phishing messages
were opened by the target across all campaigns.10 “But wait, there’s more!” (in
our best infomercial voice) About 12% went on to click the malicious attachment
or link and thus enabled the attack to succeed. That indicates a significant
rise from last year’s report in the number of folks who opened the email (23%
in the 2014 dataset) and a minimal increase in the number who clicked on the
attachment (11% in the 2014 dataset). The median time for the first user of a
phishing campaign to open the malicious email is 1 minute, 40 seconds. The
median time to the first click on the attachment was 3 minutes, 45 seconds, thus
proving that most people are clearly more on top of their email than I am.

The main
perpetrators for
phishing attacks
are organized
crime syndicates
and state-affiliated
actors.

However, before we drag these individuals outside and collectively stone
them, keep in mind that the main perpetrators for these types of attacks are
organized crime syndicates (89%) and state-affiliated Actors (9%) who can
put some thought into the ruse they use (yeah, I know). In roughly 636,000
sanctioned phishing emails, we captured whether the email was reported.
Approximately 3% of targeted individuals alerted management of a possible
phishing email. We did not verify by what means the email was reported, or
whether it was because they were savvy enough to avoid the trap or because
they only realized it once they had fallen in themselves.
10	Granted this could be affected by preview pane opening of emails or people not loading images in emails.
As an aside, the smaller proportion of nation-state Actors in this year’s data is
due to a large contribution from a particular contributor who saw a great deal of
‘Dridex’ campaigns which skewed the data toward organized crime. We should
not conclude from this that certain groups from East Asia have had a crisis of
conscience and mended their wicked ways.

What do the attackers ultimately steal? A heck of a lot of credentials (mostly
due to the large amount of opportunistic banking Trojans—beware of Greeks
bearing gifts), but also trade secrets.

Filter it! Filter it real good!
“An ounce of prevention is worth a pound of cure.” It was good advice when
Ben said it and so it remains. The first opportunity to defend against emailborne threats is (thankfully) before a human can interact with it. Email filtering
is your buddy in this fight and you need to have an understanding of your
current solution, and test its implementation.
Talk amongst yourselves (I’m verklempt)!
Provide employees with awareness training and information so they can tell
if there is something ‘phishy’ (couldn’t resist) going on. Also, provide them with
a means for reporting these events. We recommend a button on their taskbar,
but whatever works for you.

Protect the rest
of your network
from compromised
desktops and
laptops by
segmenting the
network and
implementing strong
authentication.

One click does not a catastrophe make.
So, it snuck past your email filters and someone went clicky-clicky. There is
still ample opportunity to limit the impact. Assuming the organization’s “seekrit
stuff” isn’t resident on the initial foothold, make it hard to pivot from the user
device to other assets in the organization. Protect the rest of your network
from compromised desktops and laptops by segmenting the network and
implementing strong authentication between the user networks and anything of
importance. Static passwords are adorable, but sophisticated attackers don’t
just bypass them, they utilize them to advance their attack.
Keep your eye on the ball.
You increase your chances of catching signs that you have fallen victim to a
phishing attack if you monitor outbound traffic for suspicious connections and
potential exfiltration of data to remote hosts.

Use of stolen credentials and other hacking and
malware actions targeting traditional username and
password authentication are prevalent across
numerous patterns.

Static credentials continue to be targeted by several
of the top hacking action varieties and malware
functionalities.

63% of confirmed
data breaches
involved weak,
default or stolen
passwords.

We’re not mad, just disappointed.
The use of stolen, weak or default credentials in breaches is not new, is not
bleeding edge, is not glamorous, but boy howdy it works. Static authentication
mechanisms have been attacked for as long as we can remember. Password
guessing from an InfoSec perspective has been around at least as long as
the Morris worm, and has evolved to prominent malware families like Dyre and
Zeus that are designed to (among other bad things) capture keystrokes from
an infected device. All those efforts to get users to use special characters,
upper/lower case numbers and minimum lengths are nullified by this ubiquitous
malware functionality.
The capture and/or reuse of credentials is used in numerous incident
classification patterns. It is used in highly targeted attacks as well as in
opportunistic malware infections. It is in the standard toolkit of organized
criminal groups and state-affiliated attackers alike. Even fraud committed with
stolen payment card data often relies on the static Card Verification Value
(CVV) information on the magnetic stripe.11
We are realists here, we know that implementation of multi-factor
authentication is not easy. We know that a standard username and password
combo may very well be enough to protect your fantasy football league. We
also know that implementation of stronger authentication mechanisms is a bar
raise, not a panacea. Even with all of that, 63% 12 of confirmed data breaches
involved leveraging weak/default/stolen passwords. The obvious action of the use of stolen credentials is numero uno,
but we see some other common actions used in conjunction, including C2
malware, exporting of data, phishing and keyloggers.

The nine
classification
patterns were
born of recurring
combinations of
the who, what,
how and why.

What began with a muttered complaint of “ugh, another one of these” during
data conversion a couple of years ago grew into a shift in how we present our
core results and analysis. The nine incident classification patterns were born
of recurring combinations of the who (Actors), what (assets), how (actions) and
why (motive) among other incident characteristics.
In the 2014 report, we found that over 90% of breaches fell into one of
the nine buckets and this year’s dataset is no different. We hope that by
discussing security incidents, both for this year and historically, and using
these clusters as the foundation, we can allow security folks to gain the
most from the entire (huge) dataset. Understanding that you don’t have to
necessarily worry about 2,260 different breach possibilities, but only a
select number of nine patterns (depending on your industry) makes the
life of a CISO less of a daily Kobayashi Maru.

Much to the chagrin of Jerry Lee Lewis, there was not a whole lot of moving
and shaking going on in the pattern rankings compared to last year and looking
at all incidents, only one pattern moved in the pecking order. Crimeware was
the third most common pattern last year and has moved to sixth. The reason is
the filter on the secondary motive we discussed in the Breach Trends section.
Thousands of incidents where we know a device was participating in a denialof-service (DoS) bot (but nothing else) were not sent to /dev/null per se, but
you won’t find them here.

OK, in lieu of worrying about how patterns rank overall compared to each other, let’s
get to the good stuff. The best way to use the patterns is to understand the applicability
of each of them to your organization. Of course if you are an E Corp-like
conglomerate, you can have business units that fall into several industry categories.

From an incident standpoint, Denial-of-Service stands out like “a zoot suit
at a Quaker funeral”. This is partly due to the fact that DoS attacks are in
fact, happening all the time—remember all those popped boxes in the DoS
botnets we filtered out? Another reality is that the other patterns that are more
commonly classified as incidents as opposed to confirmed data breaches
(Crimeware, Insider and Privilege Misuse, and Physical Theft and Loss) are
mostly provided by the public sector and healthcare. Those are the top three
incident patterns and we are confident that in the real world they are taking
some of that market share from DoS in other industries.

The most interesting discovery in the breach patterns to industry matrix was
the rise of Web App Attacks across the board, but especially for financial
services organizations (up from 31% in the 2015 DBIR). The next item that
raised an eyebrow or two (or perhaps a unibrow) was the decline (down
from 36% last year) in Crimeware, also in Finance. Is there anything to this?
Actually, yes. This year, again thanks to the organizations involved in the Dridex
takedown, we have even more data involving the reuse of stolen credentials.
This caused the spike in the Web App Attack pattern and if we removed these
breaches, the numbers would be more in line with 2014. On the flip side, in 2014
we received more data on malware infections within organizations, leading to
breaches that landed in our Crimeware bucket. Is Crimeware not playing as big
a role in breaches? The perspective of the reporting contributor has a lot to do
with the pattern breakdowns as well. This includes exploits of code-level
vulnerabilities in the application as well as thwarting
authentication mechanisms.

The breaches within this pattern are heavily
influenced by information gathered by contributors
involved in the Dridex botnet takedown. Hundreds of
breaches involving social attacks on customers,
followed by the Dridex malware and subsequent use
of credentials captured by keyloggers, dominate the
actions. Defacements are still commonplace and
CMS plugins are also a fruitful attack point.

The great complexity
of the infrastructure
makes web
application servers a
target for attackers.

Websites aren’t what they used to be, with a background of a tiled cloud
image, the company name proudly displayed center top in Comic Sans and
with identical animated gifs on either side. Combined with a healthy dose of
ALL CAPS, <blink> tags and, of course, a site counter at the bottom with
numbers that had just the right touch of drop shadow. 1997 was a simpler time.
Now organizations have less ugly (typically), less static and more businesscritical websites promoting their operations, conducting ecommerce and
hooking into backend databases. Users are not merely reading a homepage
and clicking on a couple of links to basic information about store hours, but are
increasingly more interactive and issue various types of inputs to be read and
acted upon by the web infrastructure. The greater complexity, including the web
application code and underlying business logic, and their potential as a vector15
to sensitive data in storage, or in process, makes web application servers an
obvious target for attackers.

For starters, not all website compromises are targeted affairs. We had almost
20,000 incidents of websites that were popped used to either host malware,
participate in distributed denial-of-service (DDoS) attacks or repurposed as a
phishing site. We have no idea as to the method of compromise, nor the victim
demographics and thus these instances of secondary motivation have been
culled from the information that follows. About half of the incidents that remain
were website defacements and the data we have on those was not enough to
establish whether the motive was ideology, a personal grudge, or just for fun. Typically the hacking actions used to
compromise were not known either, but in case you thought defacements, like
the blink element, were an obnoxious thing of the past, think again.

When we filter down into confirmed data disclosure, the financial
motive flexes its muscle with 95% of breaches associated with
criminals all about the cheddar.

A pattern within the pattern
smacks us in the face with a glove and demands satisfaction. The top six
actions narrate the story of the Dridex campaign better than Morgan Freeman
combined with Sir David Attenborough ever could. These breaches, uncovered
through the forensic analysis performed on several C2 servers tell the tale of
phish customer > C2 > Drop Keylogger > Export captured data > Use stolen
credentials. Even with a particular spree inflating these numbers, the top six
looked very similar to last year, albeit in a different order, and with phishing
making an appearance in the top actions this year.

95% of confirmed
web app breaches
were financially
motivated.

There are other stories beyond the botnet though. We wanted to know
what other data points the use of stolen credentials was associated with
when that spree was removed from the data. Phishing still showed a strong
association in the pattern, but also mail servers. While masked at first in our
data by the botnet, social engineering to acquire web-based email credentials

We have seen content management systems (CMS) as the vector for
installation of web shells, which are also classified as a backdoor in our
framework. Either exploiting a remote file inclusion (RFI) vulnerability, or
abusing an insecure upload functionality, the web shells are injected and used
as the gateway to additional mayhem. In financially motivated attacks against
ecommerce servers, web shells are used to access the payment application
code, which is then modified with a new feature that will capture the user
input (think payment card number and CVV) for future pickup. As with prior
years, this is backed up by other studies. And it wouldn’t be a proper DBIR
if we didn’t raise a glass to one of the elder statesmen of web application
hacking, SQL injection (SQLi). It, like other vulnerabilities associated with web
applications, stems from a lack of input validation allowing Actors to pass
SQL commands via the web application and to the database.

In attacks against
ecommerce servers,
web shells are
used to access the
payment application
code and capture
user input.

Like that song you can’t get out of your head. Here is another shot across
the bow of single-factor, password-based authentication for anything of
criticality. If you are securing a web application, don’t base the integrity of
authentication on the assumption that your customers won’t get owned
with keylogging malware. They do and will.
I value your input, I just don’t trust it.
Validate inputs, whether it is ensuring that the image upload functionality makes
sure that it is actually an image and not a web shell, or that users can’t pass
commands to the database via the customer name field.
Worrying about OS and core application code is hard enough, but thirdparty plugins are also gray-hair-inducing. Establish a patch process for CMS
platforms and third-party plugins.

Remote attacks against the environments where
card-present retail transactions are conducted.
POS terminals and POS controllers are the targeted
assets. Physical tampering of PED 24 pads or
swapping out devices is covered in the Payment
Card Skimmers section.

Point-of-sale
devices continue
to be a reliable
source for
stolen payment
card data.

Headline-grabbing remote payment card breaches
have shifted from large retailers in 2014 to hotel
chains in 2015. Use of stolen credentials to access
POS environments is significant. Command and
control functionalities are being reported at a much
higher rate than in years past, although this may be
in part due to an underrepresentation of C2
functionalities as opposed to a 2015 trend.

RAM scraping continues to be omnipresent in 2015,
but keylogging malware has a significant role in
many POS attacks, being a common method of
capturing valid credentials to be used against POS
assets. Continuing the trend of the last several
years, the sprees (single threat Actor, many victims)
represented in this data are a byproduct of
successful attacks against POS vendors and cannot
be attributed to automated attacks targeting poorly
configured, internet-facing POS devices.

There are still folks out there seeking to get paid and looking
to stolen payment card data as the means to meet their greedy objectives.

Point-of-sale devices continue to be a reliable source for this data, notably
the POS terminals that directly consume magnetic stripe information
from customers, or POS controllers that typically act as an aggregator of
transactional data from the terminals in a server-to-client relationship.
In small businesses, the POS environment may have a population of one, with
a lone computer processing payments and communicating out to the payment
processor. This device might also (unfortunately) be used for checking personal
email, social media breaks and other interwebby activities that introduce more
risk to the POS application which is all alone, with no anti-virus or host-based
firewall to talk to.
Four or five years ago, our findings were dominated by POS breaches—
simplistic and automated in nature and making full use of known default vendor
credentials. We lovingly called these POS Smash and Grabs, and this attack
method was one that we saw over and over again and helped drive us to the
development of incident classification patterns. The gist of these, if this is your
first DBIR rodeo, is: 1) POS server is visible to the entire internet, 2) POS has
default login, 3) Bad guy leverages 1) and 2) to install malware and 4) Malware
grabs the payment card data as it is processed. This scenario was, and still is,
a small business problem. It did, however, offer some insight into what was to
come for larger organizations.
The 2015 DBIR detailed the rise of larger organizations suffering POS breaches
and their representation in this pattern. While 1) and 2) were not present in
these breaches, raising that fruit a little higher from the ground, there are some
definite similarities. Both the smash and grabs and large organization breaches
took advantage of static, single-factor authentication. Attackers have had to up
their game a bit, having to do some work to compromise valid and assumedto-be non-default, credentials to access the environments. Moreover, they
have issued the stolen credentials from a foothold on the network as
opposed to directly from the internet.

Attackers have
had to up their game
to compromise
valid credentials
and access the
environments.

The vector associated with the hacking actions tells an interesting story as
well. Ninety-seven percent of breaches featuring use of stolen credentials also
had a vector of Partner. This is selected when the Actor uses legitimate partner
access in the hacking action. This year continued the trend of the criminal
sprees in our data being associated with attacks against POS vendors followed
by using their access into their customer base. Bill Gates once said “Your
most unhappy customers are your greatest source of learning.” With all of their
customers equally unhappy, the amount of learning some POS vendors have
acquired must have been like Neo’s martial arts training.

The other similarity of large and small organizations is that malware is the
workhorse of POS breaches. We have seen the evolution from “off-the-shelf” keyloggers,
to memory scraping malware (RAM scrapers), to POS-specific RAM scrapers
with names like BlackPOS and PoSeidon (in case you weren’t sure what they
were designed to attack). Exfiltration has evolved from static code within the
malware to FTP data to a single destination, to utilization of a C2 infrastructure
to ship the captured data out.

Both C2 and Backdoor are more prevalent this year than in years past. The
reality is that POS malware families are typically multifunctional and some
of the most notorious (Dexter, vSkimmer, Alina, Backoff, JackPOS) have
command and control/backdoor capabilities. In many cases, it is easier to
prove the use of one functionality (the one that stole the data) than others (C2
beaconing). Many of the POS Intrusion incidents did not have the evidentiary
logs needed to validate outbound communications. Long story short, the spike
in C2 and Backdoor may very well be a product of better windows into the
entire behavior of the malware.

“I know kung fu.”

Static single authentication is a weakness that is used in spades by the
attackers. If possible, improve this with a second factor such as a hardware
token or mobile app, and monitor login activity with an eye out for unusual
patterns. Have a conversation with your vendors and ensure that they are using
strong authentication to access your POS environment.
Who can it be, knocking at my door?
Find out what monitoring options are available for your POS environment and
validate their implementation. Track remote logins and verify any and all that are
against the norm.

Separate the POS environment from the corporate LAN and ensure that it is not
visible to the entire internet.

All incidents tagged with the action category of
Misuse—any unapproved or malicious use of
organizational resources—fall within this pattern.
This is mainly insider-only misuse, but outsiders
(due to collusion) and partners (because they are
granted privileges) show up as well.

They’re behind your firewall, getting all up in your
data. They are often end users and they are
comfortable exfiltrating data out in the open on the
corporate LAN. Insider incidents are the hardest
(and take the longest) to detect. Of all the incidents,
these insider misuse cases are the most likely to
take months or years to discover.

The Privilege
Misuse pattern is
one of the few that
includes collusion
between internal and
external Actors.

The disgruntled insider—we all have an idea in our minds of what this
person looks like. Perhaps it is the software developer who is frustrated
with management; maybe it is the healthcare worker who has been
recruited by organized crime; or maybe it is that guy in the basement
grieving the loss of his red stapler. Regardless of what they look like,
the fact is they are inside our carefully constructed defenses and they
are wreaking havoc with our data.
The Insider and Privilege Misuse pattern is one of the few that sees collusion
between internal and external (or even partner) Actors. Figure 27 shows the
percentage of these breaches where multiple Actors are present.
These are most frequently an external/internal pairing, but ruling out partners
as potential colluders is a mistake. The break from the norm that we saw was
the rise in misuse breaches tied to external Actors only. This was normally
solely associated with TGYFBFTDHRA, 25 but this year we had cases where
25 That guy you fired but forgot to disable his remote access.

The butler did it.
Back to the insiders—who are they? When their roles were classified in the
incident, almost one third were found to be end users who have access to
sensitive data as a requirement to do their jobs. Only a small percentage
(14%) are in leadership roles (executive or other management), or in roles with
elevated access privilege jobs such as system administrators or developers
(14%). The moral of this story is to worry less about job titles and more about
the level of access that every Joe or Jane has (and your ability to monitor
them). At the end of the day, keep up a healthy level of suspicion toward all
employees. While we would like to think they will never give you up, let you
down, run around or desert you, we simply can’t (tell a lie, and hurt you).

It is interesting to see the potential convergence
of the financial motivation and the espionage motivation. While this also
reflects the change in the dataset as we progress over time, the rise of the
espionage-motivated insider should give organizations reason to consider
implementing processes to detect when exiting employees may have taken
valuable data with them.

When the nature of their actions is known, the general privilege
abuse is always at the top of the list. This is merely using access to gain
information for alternative and unsanctioned uses. Data mishandling follows
and typically involves mailing sensitive information or loading to a sharing
service. Many times this is not done with malicious intent, but for a convenience
factor. Use of unapproved hardware and software are the third and fourth most
common varieties of misuse. The unapproved hardware is usually either a USB
drive (used to store information to be used later, like, when employed at another
company kind of later) or a hand-held skimmer that we have seen food servers
use to capture diners’ payment card data.


The shift from days to months led
us to look at what was different. We found that there were more cases where
bank employees provided info that was used for fraud—and was discovered
quicker—in years prior. For organizations that will not have fraud detection in
their arsenal, the shift is likely more representative of their world.

So love your employees, bond at the company retreat, bring in bagels on Friday,
but monitor the heck out of their authorized daily activity, especially ones
with access to monetizable data (financial account information, personally
identifiable information (PII), payment cards, medical records).

Our dataset included numerous instances of audits being performed after an
employee had left, which uncovered evidence of a USB drive used to transfer
data prior to their departure. It makes sense to take measures to identify use of
these portable drives sooner rather than later.
Keep one eye on your data and the other on your employees!
You cannot effectively protect your data if you do not know where it resides.
Likewise, it does you little good to know where it is but then pay no attention to
who has access to it. Make sure that you are aware of exactly where your data
is and be careful who you give privileges to and to what degree. It makes sense
to give the valet attendant your keys to park your car, but not to hand over your
credit cards as well.

Almost without exception, every international fraud and business crime
case that Mishcon de Reya LLP has advised on in the past 12 months
involved the use of computer equipment and electronic data. For a
company that falls victim to cybercrime, there are immediate financial
ramifications from loss of revenue while systems are down, the unlawful
exploitation of valuable data that has been stolen, or possible claims
faced from the queue of litigants seeking compensation. Additionally,
there can be a broader impact on customer trust and confidence
following an incident that can lead to reputational damage that is more
difficult to quantify.
Yet, there is huge inconsistency and discrepancy in the way that
governments are tackling this problem. Many believe that the legislation
is out of date with technology and too weak to combat the problem with
any meaningful sanction. There is widespread confusion and enhanced
regulatory risk as businesses are forced to comply with radically
different laws as their data passes from one country to the next.
In the US, there are a multitude of privacy and data security laws
but no specific and comprehensive federal law, and no official
national authority responsible for enforcing it. As a member of the
European Union, the UK implemented the European Union’s 1995 Data
Protection Directive 95/46/EC with the Data Protection Act 1998.
The Information Commissioner’s Office is responsible for enforcing it
and upholding information rights, but the ICO is championing tougher
sanctions, including prison sentences rather than fines, to deter theft
and trading of personal data. At the moment, there is no mandatory
reporting obligation in the UK under the data protection legislation
and the toughest penalty that the ICO can impose is a £500,000 fine
(about $700,000) for the most serious of data breaches. As such,
the legislation lacks the necessary teeth to properly deter misuse of
personal data.
While there is other criminal legislation law enforcement can use
to combat cybercrime more broadly, the authorities in the UK
and elsewhere face difficult and expensive jurisdiction hurdles as
offences routinely cross borders, requiring authorities to cooperate
internationally to investigate acts, then extradite and prosecute
criminals. With huge volumes of encrypted data, proxy servers masking
true IP addresses, secure VPNs and anonymous currency exchanges
used by criminals, many authorities are falling at the first hurdle in
terms of finding the necessary evidence to support a prosecution.
Unfortunately, there is still a long way to go before the scale and rate of
cyberattacks is brought under control by effective legislation.

Misdelivery of information both in paper and digital
form remains the most prevalent variety of error.

The most common
error of losing stuff
is so common, it was
deemed worthy of its
own pattern.

People aren’t perfect.
With all of the hubris and bravado in the InfoSec world, one proclamation we
usually don’t hear is “Our employees NEVER make mistakes.” Well, because
they do. Everyone does and this is the section where we talk about breaches
caused by the people saying “Oops, my bad”. If you got hacked due
to the lack of any patch process or validation, then that is not an error. The
action or inaction was not a direct cause of the data loss (the bad guy still had
to get his hack on). To ensure that every incident we come across isn’t rubberstamped as an error due to less-than-perfect security practices, we limit its
use to only when the action is the direct cause of attribute loss. And because
the most common error of losing stuff is so common, it was deemed worthy of
its own pattern along with stolen assets on page 43. As in prior reports, due to
the influx of thousands upon thousands of misdelivery incidents from the public
sector26 that tried to steal the show, we have removed them in the interest of
finding actionable tidbits of information that would never have a voice otherwise.
Data errors reduce productivity (DERP).

Publishing information where an unintended audience (e.g., the entire internet)
is able to view it remains in the top five. As does misconfiguration—mistyping a
firewall rule allowing access to a sensitive file server from all internal networks
instead of a specific pool of hosts would be a fine example.
Rounding out the top five is disposal errors. These are primarily documents,
which is concerning, since that data is in human-readable format—look Ma, no
controls! While not as common in our dataset this year, proper wiping of hard
drives on decommissioned devices must also be standard operating procedure
for organizations.


There is perhaps an element of absurdity in recommending controls for the
Error section. One can’t really say “don’t screw up again”, or “pay attention to
what you are doing for Pete’s sake”. Nevertheless, there are some common
sense practices that can be implemented to help keep errors to a minimum.
After all, with all the crooks trying to ruin us, the least we can do is try not to
help them.
Learn from your mistakes!
Keeping a record of common errors that have plagued your organization can
be used for something other than to mock fellow employees at the company
Christmas party. Collecting this information can be used to implement new
training materials for security awareness. Did Jim in accounting cc: everyone
in to his latest rant again? Talk about it. Just don’t mention Jim by name.
Incorporate frequent “Oops moments” into security training.
“I’m the map, I’m the map, I’m the map, I’m the map, I’m the map!”
Now that you are keeping a record of wrongs (love may not do it, but wise IT
departments do), use that data to map the most common errors to effective
controls that can help to minimize the frequency with which they occur, and
mitigate the damage they do when they do take place.

Ensure that all
assets go through
a rigorous check by
the IT department
before they can be
decommissioned or
disposed of.

Stop trash talking!
When assets are ready for disposal, make sure that there is a documented
procedure for wiping all assets before they are trashed or resold. Ensure that
any and all assets go through a rigorous process of check and recheck by
the IT department before they can be decommissioned and disposed of. Our
dataset is rife with examples of assets being sold to a third party while chockfull of PII and other sensitive data.

Pretty much what it sounds like—any incident where
an information asset went missing, whether through
misplacement or malice.

When we look at all incidents, laptops are the top
asset affected by this pattern. However, for
confirmed breaches, it is the documents, with their
lack of controls, which result in the most confirmed
disclosures. Lost assets were over 100 times more
prevalent than theft.

For non-encrypted
devices, the
determination of a
breach can be tough,
given that you no
longer have custody.

Humans, what are you gonna do?
If you have young children, the next time you are in their school take a gander
at the horror show known as Lost and Found. You will see what appears to be
at least 2.5 articles of clothing per student shoved in a bin and left there long
enough to form a single brick of coats, hats, gloves and unidentifiable pieces
of fabric that entered—but like Charlie on the MTA—never returned home.
People lose things all the time—this is not new or particularly newsworthy. It
is, however, a real-world pain in the neck for organizations that are at best
replacing Scooter’s laptop, or at worse scrambling around to figure out if there
was PII on the device and whether encryption had been implemented.

Employees lose things.
Bad guys also steal your stuff.
Full disk encryption.
Same old story, same old song and dance.

Determination of a breach can be tough, given that you no longer have custody
of the computer in question. Is the data on that system at risk? Certainly, since
it is trivial to bypass the sole control—the password. Still, we cannot by our
definition, in most cases of lost computing devices, label them as a confirmed
data breach. This discrepancy between the number of confirmed breaches and
the number of incidents in this pattern shows that there is quite a bit more data
in the at-risk category than the number of confirmed breaches implies.
Based on all the incidents in this pattern, laptops are the most common target.
However, when we narrowed our research to confirmed breaches, documents
are in the lead due to the ability to infer that the finder or thief can read the
language in which the information is written.
Physical theft is a problem that we have seen time and again, and these
incidents most commonly occur in the victim’s own work area (39%) or
from the personal vehicle of the employee (33.9%). That said, these items
are being lost far more often than they are being stolen. In this year’s data,
an asset is lost over 100 times more frequently than it is stolen. At the
end of the day, the impact is the same—the laptop is gone and likely
wasn’t turned into Lost and Found.

In this year’s
data, an asset is
lost over 100 times
more frequently
than it is stolen.

Just do it.
Full disk encryption on all mobile devices and removable media—make it part of
the standard build.
Keep hope alive that security and situational awareness will become ingrained
in your users. Include physical security of corporate assets as part of their
orientation and ongoing training. Reiterate that cars are not an appropriate
place to leave laptops. Cars have windows which thieves have proven that they
can not only see through, but also break to get what they want.

Rein in the paper as much as feasible given your business. Establish data
classification and make it a policy violation, with potential consequences, to
print and transport sensitive data. Consider tokenizing to replace sensitive
information with an alternate unique identifier when printed copies are required.

Any incident involving malware that did not fit into a
more specific pattern. The majority of the incidents
that comprise this pattern are opportunistic in
nature and have a financial motivation behind them.
This pattern frequently affects consumers and is
where “typical” malware infections will land.

The Crimeware pattern continues to be driven
by external organized criminal groups that are
financially motivated. Establishment of control
over a device using C2 malware followed by
ransomware, then the targeting of credentials or
enrollment into a botnet accounts for the majority
of the incidents.

Typically, these are
high-frequency, low impact annoyances
that will not receive
a full forensics
investigation.

Since the expansion of our data contributors and the advent of the patterns,
Crimeware has historically been generous in the number of cases, but not so
rich in detail. The majority of the incidents found in this neck of the woods come
(in bulk) from CERT/CSIRT organizations, who receive them from a wide variety
of organizations. These are typically high-frequency, low-impact annoyances
that will not receive a full forensics investigation and/or be documented and
categorized. We focus on the smaller subset of incidents where the fidelity is
higher and use those as predictors into the nature of the rest. This year we also
will be delving into malware data received from our security vendor contributors
(many thanks to Cylance, Fortinet, ICSA Labs, Palo Alto Networks and Tenable)
to shed some light on certain areas.
When the functionality of the malware was known, C2, ransomware, spyware/
keylogger, and backdoor and export data were the top five functionalities
(see Figure 33).

6,800 instances of identified devices launching traffic at unknown victims
would have dominated the numbers in such a way that it would deter from the
usability of the data.
Ransomware, in the number two spot, realized the biggest jump in our data
and this will continue to be an element that we track. In case you missed it,
ransomware is malware that encrypts files resident on the infected device
and, in worst cases, attached file shares. Extortion demands follow, leveraging
the need for availability of the data. This is cut from the same cloth as denialof-service extortion, but typically is opportunistic in nature and affects
organizations and consumers alike.


The rest of the top five draw out a very familiar pattern involving banking
Trojans. The criminal groups behind these families of malware know that you
need to control your infected minions (C2/backdoor), and you need to capture
(keylogger) and send (export data) the banking credential information—so these
are the tools of the trade. These functionalities are top-heavy this year, but are
by no means new or indicative of an upward trend.
Generally speaking, there are three major avenues for crimeware installation,
either via emails with malicious attachments, websites serving up drive-by
downloads with each visit, or a hybrid of the two—emails with links to pages
with, you guessed it, drive-by code installs.

Do you want ransomware? Because that’s how you get ransomware!
We stated earlier that because run-of-the-mill malware does not
always merit incident responders rappelling in through skylights and
cloning drives, it is a bit light on details. We did however receive a
group of ransomware cases where the vector was known (hooray!)
and what was specifically exploited (Flash). Even better was that we
had the version of Flash exploited and the current Flash version. We
thought, “This could be interesting—how bad can people be at updating
Flash?” The answer is, very bad. This is a small sample size, but the
results were still eye-opening. We aren’t putting this here to ring the
shame bell at anyone, but Figure 35 shows that over one half of these
browsers were rocking Flash versions that were over a year older than
the current revision. The speed of Lewis Hamilton was not required
for the majority of these drive-by downloads; the pace of a horsedrawn carriage would have done just fine. It should be noted that some
organizations with more togetherness in their act also fell victim, with
one having a version that was current and another only two weeks
older than the latest iteration.

We look to non-incident data for the rest of this section to provide some more
malware information. We first wanted to reaffirm what we found last year
regarding the uniqueness of hashes.
To hash or not to hash? Let’s not.
Last year we burst many a bubble by calling out that a unique hash does not
mean you have been targeted by an ultra-sophisticated group of nation state
malware ninjas.
This year, we compared hashes out of a total of 40 million records of malware
from several contributors and noticed that again there was little overlap across
organizations. When investigating for commonalities, we saw that about 20,000
MD5 hashes existed across multiple organizations out of almost 3.8 million
unique hashes.

We then looked at
how long hashes
were used for.
Drumroll please…
not long.

And poof, he’s gone.
We then looked at how long hashes were used for. Drumroll please … not long.
When looking at the difference between when a hash was first seen versus
when it was last seen, we saw that the count of hashes over this time difference
was very much long-tailed (see Figure 36 below). The vast majority were used
for a very short period of time and then dropped off the face of the network.

Analysis of one of our larger datasets showed that 99% of malware hashes are
seen for only 58 seconds or less. In fact, most malware was seen only once.
This reflects how quickly hackers are modifying their code to avoid detection.

Where be me eye patch, matey?
We know that malware droppers, in many cases, succeed by exploiting known
vulnerabilities, so utilize those patches that your vendors release for your OS,
applications (cough, browsers, cough) and security tools.
Exes, stop calling!
Defending against malicious executables ranges from not allowing programs
to run scripts/macros (e.g., document-based programs) to having your email
server strip/remove executables or other file extensions as attachments in
emails. Less is more in this scenario, as you will be reducing the attack surface.

The lifespan of
malware hashes
is short and not
so sweet.

Don’t monkey around.
Don’t be like the three wise monkeys here. See, listen and discuss. As
suggested in last year’s report, capture malware analysis data in your own
environment; actually look into the different families of malware in your own
organization and, if at all possible, the entry point.

All incidents in which a skimming device was
physically implanted (tampering) on an asset that
reads magnetic stripe data from a payment card
(e.g., ATMs, gas pumps, POS terminals, etc.).

There continues to be little variation in this pattern.
Actors from Eastern Europe favor this attack type,
with ATMs the target of choice and the discovery
method remains largely external.

70% of Payment
card skimming
incidents in our
dataset can be
blamed on criminal
organizations.

“Third verse, same as the first”
In a world full of chaos and change, it is a comfort to know that you can
rely on certain things to stay relatively constant. For instance, your bread
will always fall buttered-side down, your distance from a bathroom will remain
in direct proportion to the urgency of your need for one and skimming won’t
really change much from year to year. That is probably because the crooks
were raised in the “If it ain’t broke, don’t fix it” school. Payment card skimming
remains one of the most lucrative and easy to pull off crimes, both for
organized criminals and the occasional independent pilferer (he’s just
a poor boy, from a poor family).
Due to the fact that these incidents come mainly from US-based law
enforcement, our data is almost entirely US-centric with regard to victim
location.
Also reflecting past trends, the vast majority of breaches in this category were
related to ATMs (94%), with gas pump terminals coming in second (5%) and PIN
entry devices (PEDs) barely making an appearance (1%). The physical action of
‘surveillance’ was selected in over 90% of cases—this is due to the installation
of pinhole cameras designed to capture PIN codes on the devices in question.
As in prior years, the skimmers can be, and often are, constructed with extreme
precision and great detail and are difficult, if not impossible, to detect with the
naked eye (or for that matter, even with eyes that are fully clothed in contacts
or spectacles).

“And finally... some bad news”
With regard to discovery timelines, we discussed last year that detection times
were getting better, and were leaning heavily toward the ‘days’ category rather
than ‘weeks’ or ‘months’. This year, we do not see that shift continuing. On the
contrary, discovery times are firmly entrenched in the ‘weeks’ this year.
There is a dramatic decline in internal discovery and a corresponding increase
in discovery by fraud detection in our dataset this year. It is not clear whether
the employees of victim organizations all need a better prescription vision
plan, or whether it is simply that those victims who discover the tampering
themselves quickly remove the devices without reporting it to law enforcement
(or not to the agencies that partner in this research). Naturally, it is quicker
to discover skimming-related theft when you see it with your own eyes
than it is to wait for signs of CPP to appear, so the relative change
in each category would make sense.

There is a dramatic
decline in internal
discovery and a
corresponding
increase in discovery
by fraud detection.

Incidents in this pattern include unauthorized
network or system access linked to state-affiliated
Actors and/or exhibiting the motive of espionage.


Espionage begins with the same threat actions as
many other patterns to gain access, but will deviate
as needed once the initial compromise occurs.

The Actors are
predominantly state affiliated groups.
Competitors and
nation states are
also mixing it up.

Unlike Bond movies, Cyber-espionage has a glaring lack of machine-gun
umbrellas, henchmen with razor-rimmed hats and tear-gas-laden briefcases.
It does, however, have a diverse victim demographic, and while the villains may
not be exfiltrating data to an underground fortress disguised as a volcano, they
are certainly more skilled and patient than your script kiddies. If you want to dig
into some dossiers, see the research studies by some DBIR contributors and
others wearing the white hats in the Cyber-espionage Research sidebar.
First, let’s define the pattern for you. Cyber-espionage features external threat
Actors infiltrating victim networks seeking sensitive internal data and trade
secrets. Incidents where an employee steals the customer database and sets
up his own lemonade stand will fall into the Privilege Misuse pattern. The Actors
are predominantly state-affiliated groups, although organized criminal groups, competitors and nation states are also mixing it up. Beyond the
top four, we have a smattering of other industries that show that <obvious> if
you have something someone can use to their advantage, you are a potential
target of Cyber-espionage</obvious>.

We will admit here and now that our view into the specific tactics of
these adversaries is front-loaded and focuses on the tactics used to
gain the foothold. Many of these breaches begin with the tried and true
mirepoix of phishing, dropping some backdoor and/or C2 malware, and
then using that malware for the entry point. Phishing, as a leading action,
provides a number of advantages over many other exploit approaches.
The time to compromise can be extremely quick and it provides a
mechanism for attackers to target specific people in an organization. And
by using a service that is necessary for business communication to the
internet, it allows an attacker to bypass many security devices and gain
a foothold on an endpoint in the organization from a remote attack.
When phishing isn’t the vector for the persistent malware installation, the
browser is. Drive-by downloads leveraging browser or common plug-in
vulnerabilities are utilized to accomplish the same mission—compromise a
desktop on the corporate LAN and go from there. While targeting specific
individuals may not be as feasible, the targeting of specific sites that are
likely to be visited by certain sectors is. Strategic web compromises allow the
adversary to leverage a vector more associated with opportunistic Crimeware
to begin their assault.

Phishing, as a
leading action of
Cyber-espionage,
provides a number
of advantages—the
time to compromise
can be extremely
quick and attackers
can target specific
people.

After the initial access is established, what happens next is dependent on
the location of the data and the obstacles that the adversary must overcome
to reach the finish line. It goes without saying that the obstacles in your internal
environment should resemble a Warrior Dash more than a kid’s potato sack
race, but more on that later. Looking at Figure 40, we can infer a bit more of
the storyline via the combination of footprinting of the network and utilizing
stolen credentials for advancing the attack. While we don’t have the specifics
on what methods were used to acquire credentials, there are a lot of breaches
with unspecified malware and if we were to bet on it, keyloggers and password
dumpers would be our educated guesses on the tools selected for that
stage of the game.

That’s my ex, Phil.
Trade secrets, aka proprietary information, are the most common data variety
captured in Cyber-espionage breaches, present in over 90% of cases. Also
represented are data types that help map out a path (configuration information
gleaned from footprinting and fingerprinting the environment) and provide a
means to move around in the network (credentials).

Cyber-espionage Actors put on their pants the same way we all do. It’s just that
after their pants are on, they persistently and patiently compromise terabytes
of data. In the DBIR, we’ve seen that the threat Actors will start with simpler
tools and techniques before moving on to more sophisticated attacks. For
this reason, basic protections are still critical to guard against these types of
threats, in addition to specialized protection.

90% of Cyberespionage
breaches capture
trade secrets
or proprietary
information.

Malicious software was involved in 90% of our Cyber-espionage incidents this
year. Whether it’s delivered via email, a web drive-by, or direct/remote installation,
protecting the endpoint is critical.

Any attack intended to compromise the availability
of networks and systems. Includes both network
and application attacks designed to overwhelm
systems, resulting in performance degradation or
interruption of service.

Attacks are either large in magnitude or they are
long in duration but they are typically not both, and
many are neither.

This isn’t a forever thing, but we are using a hybrid of the naming conventions
utilized by our data sharing contributors and the high-level NAICS categories.
We are doing this, not out of laziness, but because when we looked to do the
mapping from our data sharing contributors naming conventions to NAICS, we
were worried about losing fidelity in the data. Many of the affected companies
are gambling sites, as an example. We would lose a lot of the industry
demographic information if we classified them as an internet entertainment or
game site, or likewise as a casino. No framework is perfect 27 and we felt that
blending the two classifications for this particular section made sense.

Patch it. As the attackers’ botnets popped their steroids for a beefier blow,
the attackers began to realize their creativity and scope should not be so
limited. This epiphany has resulted in script injections into browser sessions,
distributed reflective DoS attacks, as well as the infancy of temporal lensing 28
(which sends packets via different paths with a focus on time so that they
arrive simultaneously in order to overwhelm the target system). Not only are
these attacks increasing in scope, but also in number. We received the gory
details of DDoS attacks (e.g. bytes and packets per second, duration) from
Akamai Networks, Arbor Networks, and Verizon DoS Defense. We will get into
magnitude and duration in a little bit but first, let’s examine density.
As provided in the last two reports, Figure 41 shows two density plots of
bandwidth and packets in DoS attacks, respectively. In this year’s dataset,
we see that the means of bytes per second versus packets per second were
5.51Gbps and 1.89Mpps respectively.

Try this on for size.

DoS attacks
are either large in
magnitude or they
are long in duration,
but typically
not both.

With density, magnitude and duration out of the way, let’s finally look at
enumeration of packets per second (pps) by industry and a caveat that
comes with it. We compared the max and median number of pps per industry
and as expected, they varied quite a bit. For example, although one of our
large datasets showed that Media had the highest number (222 million pps)
throughout this year’s data, it doesn’t necessarily mean (no pun intended) that
it is the industry you’d expect to run out the door with their pants on fire every
time. To see this, just look at Figure 43 that reflects the median number of pps
for Media (approximately 600,000). Another such case includes High Tech
Consulting, where the max pps was around 214 million, yet the median was
around 540,000. In general, we don’t always want to look at the max as it may
only point to a single event, not all events throughout the entire year, hence we
need to consider the median.

To sum up, “They start wanting me to care more, and I just don’t” works for
good ol’ Han, but unfortunately we cannot live by his motivational motto when it
comes to DoS. Not only is it one of the most popular attack types out there, but
the rise to dominance of DoS is forcing attackers to join the dark side in droves;
it may be time for Han, and the rest of us, to have an abrupt paradigm shift.

Fear not the lone wolf.
Isolate key assets to help prevent your devices from being used to launch
attacks. For instance, enforce the principle of least privilege, close any ports
that are not necessary and—bottom line—if you don’t need it, turn it off. Also,
prepare your den for potential attacks. Patch your servers/services, use your
IDS/IPS to identify and block bad traffic, use your firewalls to help filter, and
have a response plan ready.

It makes sense as the peak size, complexity and frequency of DoS attacks
continue to evolve and rise, that cloud service providers must have solutions in
place in order to protect the availability of their services and infrastructure.
Understand the capabilities of your defenses.
Have a solid understanding of your DDoS mitigation service-level agreements.
Make sure that your own DoS response procedures are built around existing
denial of service protections and your operations teams are trained on how to
best engage and leverage these services if and when they become more than
just a ‘piece of mind’ control.

As DoS attacks
continue to evolve,
cloud service
providers must have
solutions in place
to protect their
infrastructure.

Social actions are extremely prevalent, mostly
phishing incidents without the necessary
corroborating details to cluster them into a more
specific pattern. Pretexting for financial gain is
trending upward from last year.

By far, the biggest
source of incidents
in this pattern is
phishing attacks
where not much else
is known.

If the other patterns are the hip bars in the Gulch, Everything Else is more like
the local hangout off of Belcourt. Just like in 2014, the Everything Else pattern
isn’t a subset of unique, never-seen-before events, but some select groups that
like hanging out away from the main drag.

There are two reasons why an incident would not be on the guest list, thus
causing the bouncers, in the form of clustering analysis, to keep them behind
the velvet rope and outside of the nine clubs. The first is that there simply
was not enough information provided about the incident to associate it with a
pattern. By far the biggest source of incidents in the Everything Else pattern is
phishing attacks where not much else is known. A large number of them come
from a pair of Computer Security Incident Response Team (CSIRTS), but ten
additional different data contributors reported phishing attacks that fell into this
pattern. We won’t dwell on phishing in general since there’s already a section
for that, but it is interesting to note why these end up here and are not bounced
via the complexity filter we discuss in Appendix E: Methodology and VERIS
Resources. Merely knowing phishing was involved gives us a fair amount of
details—we know a human asset is targeted, we know a threat action, we know
the vector is email, and we know or infer an integrity loss due to the altering
of human behavior. So there is a lot we know, but it’s what we don’t know
that lands it here.

The second reason that incidents hang their hat here, is that they are actually
different from the norm. One scenario we are seeing more of is financial
pretexting, sometimes called ‘CEO Fraud’. This involves old-fashioned social
engineering of employees with the authorization to move money. Emails
purportedly from the CEO or other head honcho provide instruction to transfer
funds to an entity, with a seemingly valid reason provided. These may also be
blended with other forms of communication, but you get the gist of it. ‘Twas not
the CEO behind that email and somebody who believed they were following
legitimate instructions is not having a very good day. As our dataset continues
to get a better view into this corner of cybercrime it may be time for this to
move out of the indie scene and become more mainstream.

We encourage
organizations to
collect as many
details as possible
for data breaches
and many of these
breaches will get
"on the list."

You know we like Everything Else, so let’s talk about everything else in
Everything Else.
Outside of the aforementioned social actions, and focusing on confirmed
breaches, we have a significant number of hacking events, but without
knowledge of the specific varieties used by the adversary.

As we stated earlier, it is the missing pieces of the puzzle that are the cause
of these “hacks” ending up on the back pages of patterns sections. As always,
we encourage organizations to collect as many details as possible for data
breaches and hopefully incident reporting detail will improve and many of these
breaches will get “on the list”.

Actions taken by
the adversary are
not exclusive to a
single pattern.

The focus on credentials and phishing in particular, show that actions taken by
the adversary are not exclusive to a single pattern—anything but.

And while we tend to stray from focusing on particular trees in the data breach
forest, the scenario is interesting to walk though as it
features many of the most common threat actions, vectors and assets from
our corpus. What you are looking at is a progression of a breach involving
the targeting of a POS vendor and subsequent collection of sensitive data
used against a second group of victims. The birth and rebirth of a breach is
established above.
The attack begins with a targeted phishing campaign against the vendor.
The person on the other end interacts with the email (clicks) and malware
installation on the user device occurs. While the end of this story is stolen
payment cards, those who aren’t flipping their collective wigs trying to comply
with PCI should still pay close attention. Up to this point we could be talking
about the beginnings of a state-affiliated Cyber-espionage breach, or even
a totally opportunistic Crimeware attack. Once the initial access has
been established the attacker’s motivation influences which street
they choose to drive down.
In the above case the foothold is used to harvest credentials to be used against
B2B customers. We can even infer some likely suspects as far as malware
varieties here, notably some level of control and access (backdoor/C2) and a
means to establish the first confirmed data disclosure (keylogger).
So for the adversary, great success. User duped, device compromised, data
captured—time to yell “Yabba dabba doo” and slide down the dinosaur tail to
signify the end of another productive work day? Not quite.
The breach is reborn as an attack on the customer using the stolen credentials
against a static authentication factor. With the second network compromised,
malware is installed directly (after system access). Malware functionalities
of scraping RAM and exporting data, as well as establishment of control and
persistence, make their appearance. They combine to capture, package and
exfiltrate payment card data, thus completing the breach.

Having an
understanding
of how patterns
complement each
other can help direct
your efforts as to
what to prioritize
your limited
resources against.

Having an understanding of how patterns can complement each other and
share portions of event chains can help direct your efforts as to what to
prioritize your limited resources against. That is, knowing the processes used
by the Actors, the tools (Actions) to accomplish their goals and how many of
these patterns begin with the same or similar bag of tricks.

Last year we analyzed impact data associated with cyber insurance
claims leading to two main conclusions. First, record loss is not a simple
linear relationship; the first few records breached cost significantly more per
record than the 100,000th. Second, there’s a lot we don’t understand about
the cost of breaches. In fact, half of why one breach costs one amount and
another costs another amount is not known. (The other half is due to the
number of records breached.) A year later and we are still looking for the
meaning of life and a better predictor of bottom line impact to
organizations that suffer a security incident.

PCI breaches
had a much
higher median
of documented
record loss
than PHI or PII.

We decided against attempting to build a better mousetrap this year. With
limited tangible, hard data available on the cost of breaches, that exercise was
not going to be a dragon we attempted to slay. Instead we dug into actual cyber
insurance payout data again contributed by NetDiligence and looked into other
characteristics that could be interesting and actionable. We poked around
with the data varieties involved in the dataset and found that PCI breaches
had a much higher median of documented record loss than personal health
information (PHI) or PII.

Legal guidance
during the crisis
management phase
and forensics
investigations is
where the majority
of the cash is going.

Forensics (like freedom) isn’t free.
The short
explanation is that it shows that the majority of the insurance payouts go
toward costs within the phase of breach recovery associated with determining
just which creek you are up and your current paddle supply. Legal guidance
during the crisis management phase and forensics investigations are where
the majority of the cash is going. These cost categories are followed by breach
notification and credit monitoring, because sending flowers to your customer
base just isn’t going to cut it.

If you look at all the different cost categories, they are ordered from first
to last. The first phase includes up-front costs which are incurred when you
think you have suffered a loss, and are receiving third-party guidance and
investigative services to determine what happened and establishing how bad
it was. This is followed by reluctant acceptance and trying to save as much
face as possible with the customers affected. Then come the long-term costs
involving legal representation, settlements and fines, which would occur after
the story of your breach is coming to the epilogue. It should be noted that while
our glimpse into the cyber insurance world is enlightening, it also requires
some additional context. It’s important to understand what might not be
covered by insurance. Many cyber insurance policies do not include coverage
for remediation costs or judgments to pay punitive damages – each being
potentially expensive on their own. In many jurisdictions, punitive damages
are not even legally insurable. And these costs are not nearly as common, in
comparison with the more upfront costs.
Attorneys and investigators don’t charge by the record breached, but typically
on an hourly basis whether for a fixed number established by a pre-existing
retainer, or on demand. Develop relationships before their services are
required and align your ducks, so in case these services are required, you
have processes in place to quickly provide the level of access and information
needed to kick things off properly. You want to try to ensure hours aren’t spent
looking for a network diagram or SLAs while suits are in a conference room
looking at their mobile phones.

There are seemingly
endless types of
stolen data available
for sale from an
equally endless
variety of sources.

There are seemingly endless types of stolen data available for sale from an
equally endless variety of sources. However, this document is not “War and
Peace,” so we will attempt to shorten and simplify our analysis by limiting the
scope to the data types that are easily understood and where a significant
volume of stolen data is available through reasonably well-understood
marketplaces.

Property or access to enterprise systems
can also be stolen and monetized, and often are. However, while we commonly
see services related to the theft of a variety of data, transactional details are
not commonly seen on the open market and it is therefore difficult to quantify
its market value. Some data may be more valuable to keep rather than re-sell
on the markets. It is probable that those who steal IP are actually using it
themselves to create a better widget without the laborious and costly R & D
otherwise required. So, we will focus on the areas where we do have sufficient
visibility—the categories mentioned above.

There are multiple methods by which stolen cards are obtained and
cashed out. Furthermore, there are several factors that influence how
compromised payment card data will be used for financial gain once
it is purloined.
The initial decision made by the threat Actor is whether to sell the data they
have acquired, or to engage in the post-compromise fraud themselves. In large
breaches with record losses in the millions, it may be advantageous to act as
a wholesaler and sell in bulk to intermediaries who will ultimately initiate the
fraudulent transactions. The “This little piggy went to market” section below
digs deeper into the black market for stolen data.
Methods available to monetize stolen payment card information (like the
Wonder Twins) can take many forms. We can, however, begin with a simplistic
breakout of possible fraud mechanisms into two distinct and commonly used
categorizations, card-present and card not-present fraud.

We will start with card not-present (CNP) fraud. Obviously, this fraud
is associated with purchases made either online or over the phone. At first
thought, it seems like this would be a desirable fraud action to take. It can
be done remotely with no need to physically travel to a store and show your
face. But there is a catch. Namely, the lack of the 3 or 4 digit number on
the physical cards, known as the Card Verification Value (CVV2). The CVV2
code is a required field on the vast majority of ecommerce sites. In a blatant
demonstration of pure pigheaded obstinacy, the issuing banks do not place
the CVV2 code on the magnetic stripe of the card, thereby forcing criminals
to actually work for their money. Therefore, the necessary piece of
information to perpetrate CNP transactions is typically gathered
in attacks against legitimate CNP transactions. The two main
patterns associated with capturing CNP data are:

Profiting from stolen
card not-present
(CNP) transactional
data is similar to old
school fencing of
stolen goods.

Crimeware installed on consumer devices with spyware or form
grabber functionalities to capture (client-side) the PAN+Expiration+
CVV2 combo which are needed in addition to billing information to
“prove” possession of the physical card.
Web App Attacks leading to compromise of the payment application and
subsequent code modification to collect and exfiltrate the same information.
Profiting from stolen CNP transactional data is similar to old school fencing of
stolen goods. Think of goodfellas handing out cartons of cigarettes off the back
of a truck at a “discounted” price. CNP orders for goods or services are placed
online and then delivered through a network of intermediaries to obfuscate the
true recipient of the shipment. At the end of the shipping chain the goods are
delivered to warehouses where the goods are then sold through local websites.
Present and accounted for!
POS Intrusions and Payment Card Skimmers: Two great tastes that go great
together—91% of payment card breaches fall into these two patterns. Both
patterns feature specific assets that are targeted due to their role in processing
payment card information and both involve card-present transactional data.
And the data captured in a card-present transaction is highly likely to be reused
in card-present fraud. Some of you at this point are noticing a lack of Chip and
PIN mentions, and we will get to that in a bit, we promise.
Both of these attacks, if successful—and let’s be real, they frequently
are—result in the compromise of magnetic stripe information and are detailed
more thoroughly in their respective sections. Let’s focus on the stripes, shall
we? That bold black stripe on the back of your card holds some key pieces
of information: the PAN, expiration date and discretionary data (most
notably the CVV) that was designed to help establish "proof" that the
physical card is legitimate.

The CVV protects against cloning the payment cards of the people that take
pictures of their debit cards and post them on Twitter, so I guess that’s a win. 30
But since the common attacks are grabbing all the static magnetic stripe data,
the utility of CVV (not CVV2 which is used in CNP transactions) is lessened.
This is where the Europay, MasterCard and Visa (EMV) standard—via Chip
and PIN—comes into play, using a one-time security code to establish the
authenticity of the physical card instead of the static CVV.
ATM skimming operations also target the users’ PINs. Combining this key piece
of information with the mag stripe allows for quick cash-outs in areas where
Chip and PIN protection has not been fully implemented such as in the USA,
South America and Asia.
To recap: CNP fraud most often leverages peeking in on legitimate CNP
transactions. Card-present fraud stems from stealing info from card-present
transactions. The CVV and CVV2 numbers help to prevent the cross-pollination
of fraud, but neither are a powerful force field against stealing payment info
and getting paid.

As consumers began to access financial information online, cybercriminals
targeted the theft of both login credentials and ultimately the money in the
accounts. Financial account login credentials can be used to exfiltrate money
through transfers via online banking applications. Phishing and malware can
team up to capture account and routing numbers to commit ACH Fraud. The
Crimeware pattern makes another appearance in the form of banking Trojans
(e.g., Zeus, Dyre and Dridex) that have evolved to efficiently target static and
thus reusable banking information. Privilege Misuse by banking employees is
another pattern that leads to banking data loss. Simply put, employees have
access to this data, and often use it for their own gain solely or in collusion
with external criminal groups.

In cases of Privilege
Misuse, employees
have access to
data and use it for
their own gain or
in collusion with
criminals.


Personal data, aka PII, is the other data type that is often associated with
financial fraud. The term “identity theft” is no longer an alien concept to most
people and there are numerous ways for adversaries to use PII. Opening up
new lines of credit and filing fake tax returns are common fraud methods.
PII can also be used to craft better pretexts to be used in a variety of social
engineering attacks. Many disclosures of PII fall into the Miscellaneous Error
pattern, as well as Insider and Privilege Misuse and Physical Theft and Loss.
This little piggy went to market.

Like any market, the market for stolen payment cards is subject to supply and
demand. Large-haul payment card breaches were non-existent in the 2011
DBIR and we were concerned over the small record count (approximately 4
million records, down from 144 million the prior year) in our 2011 DBIR data. We
confirmed the lack of known high record count breaches for that year. And the
market data above points to a low supply, raising the cost, which supports that
finding. Following the retail mega-breaches in 2014, we saw that there was an
overabundance of cardholder data that influenced a drop of about 50% from
prices just three years earlier. As we fast forward into 2016, we continue to see
a steady yearly decline. With supply through the roof, sellers of stolen cards
began differentiating based on other criteria to prop up prices. We discovered
that the criminals were selling by geography (e.g. city) and by validity rate,
immediately following large breaches. Clearly, knowing the location where
cards can be used without suspicion and the likelihood that the cards are valid,
provide significant value to buyers. Today buyers can specify certain countries
or card types for extra cost (we have seen an $8 upcharge for this). Costs are
significantly higher with additional cardholder information (PII) such as billing
address and social security number. Overall, however, the trend over the past
four years has been a general decline in the prices charged.

Sellers of stolen
cards began
differentiating,
basing their prices
on geography or
the validity rate
of the cards.

There is not much data to establish price trend information for stolen financial
account credentials. However, we have found some current pricing information.
For $250, a buyer can acquire access to an account (from a number of major
banks) with a balance of $5,000. There is a volume discount here, where $400
provides access to an account with $10,000. This reflects an account balance
of between x20–x25 the purchase price.

PayPal accounts are also a common target for those who wish to steal financial
account login credentials. We have seen markets with even greater discounting,
where 60 bucks will get you $4,000 in PayPal credit (x67 the purchase price).

The value of something is what someone is willing to pay for it, and if
there is a demand for something there will always be someone willing
to supply it in order to obtain a profit. The rules of the market can
be perfectly applied to the cybercrime marketplaces. Through the
operations coordinated through Europol, we have seen how all kinds
of illegal goods are traded through black market digital sites, some on
the dark net, taking advantage of the anonymization possibilities given
by the technology, and many of them on the open net. There is a clear
demand for stolen data and, therefore, there will always be criminals
ready to supply and satisfy this demand, especially if we take into
account the disproportion between the risk-cost-profit, as data can be
easily stolen and transmitted.
The whole internet community, from citizens to companies or
governments, is a target for cybercriminals looking for protected
data. Private users are victims of phishing/spam campaigns aiming
at stealing online banking credentials or sensitive documents. Small,
medium and large companies, for which data is one of the most
important assets (information on its customers, their market strategy
or industrial information) are constantly targeted through sophisticated
technical attacks or basic social engineering techniques. As stated in
Europol’s iOCTA (Internet Organised Crime Threat Assessment) 2015,
the media commonly referred to 2014 as the “Year of the data breach.”
With record numbers of network attacks recorded, this is a constant
trend and the future scenario doesn’t look any better.
The law enforcement community is constantly fighting against
these criminal markets, its administrators and the criminals trading
the stolen data. However, only through a coordinated effort involving
all the parties involved; law enforcement, private sector, financial
institutions, internet security industry, we will be in position to
properly tackle this threat.

Organized criminal
activity increased
due to high levels
of botnet activities
and stable levels
of POS intrusions.

External Actors reported a slight growth in percentage of breaches from last
year but not outside of historic norms. Internal Actors realized a similar decline
in percentage and count from 2014. Collusion between internal and external
Actors is still sluggish since its above average 2012 mark. Diversification of data
and less breaches involving solicitation of banking workers has contributed to
its decline. Partner Actors have remained flat.
Organized criminal activity reports an overall increase benefiting from high
levels of reported botnet activities and stable levels of POS intrusions in 2015.
Shifts in data contributions were cited as a cause of a slight decline in stateaffiliated Actor prevalence last year.
Activist group activity review showed that breach levels were down and noted a
continued moderate shift in focus from SQLi to denial-of-service campaigns.
Threat action trends
Hacking and Malware activity was characterized as growing rapidly and was
similar to 2011 numbers. A botnet takedown contributed to this growth as well
as an upward trend in the social threat action category. Phishing had a stronger
association to known Crimeware breaches in 2015.
Physical actions cited the significant increase of non-law-enforcement
data contributors as the principle reason for their decline from 2013 levels.
Skimming operations have realized flat to slightly declining activity from 2014.
Conditions for use of stolen credentials and use of backdoor or C2 have
continued to show growth in 2015. A partnership of the two varieties in a
banking Trojan campaign was cited as a reason for increased activity. Brute
force activity continued to be subdued as stolen credentials continued to
establish growth in the POS Intrusion market.

The continued use of Web App Attacks has allowed SQLi and RFI to report
stable activity in 2015. Contacts indicated that spikes in Crimeware breaches
have resulted in significant gains in C2 and keylogging data malware
functionalities. Data exports via malware also have a positive outlook.
RAM scrapers continue to show significant usage overall, but are showing
signs of decreasing activity. The victim population in associated scaled remote
attacks on guessable POS credentials is showing signs of overall decline.
Penetration into several incident classification patterns in 2015 is credited for
the growth of phishing in the breach dataset. Social threat actions are showing
stable growth. Pretexting activity has increased and was seen at a higher
percentage than solicitation/bribery—this is a significant change from 2014 and
was last seen in 2011. A positive growth in the use of pretexting in financially
motivated breaches was reported in 2015 contributing to the rise in activity.
This gain was offset by a sluggish performance by the Misuse variety of use
of unapproved hardware. Reports suggest that the majority of these breaches
involve use of USB drives to steal data and are related to espionage motives.
Financially motivated uses of hand-held skimmers have realized a slowdown
from 2014, which was stable when compared to 2013.

The majority of
use of unapproved
hardware in
breaches involve
use of USB drives
to steal data and
are motivated by
espionage.

Financial, Information and Online Retail industries showed growth in their
representation in the report. Accommodation showed moderate activity slightly
up from 2014. Public, Retail (not online), Healthcare and Professional Services’
presence softened in 2015. This is likely due to changes in the contributing
organizations and several breach sprees that influenced numerous 4A (see
Breach Trends section for definition) aspects in 2015.
No breaches have been attributed to vermin or any other environmental action,
remaining flat.


There are a couple
of highways the
attackers like to
use. Blocking those
slows them down.
Attempting to block
all possible paths is
a fool’s game.

In the end, it’s the math that does the work. If you’d rather not math that hard,
just try out our handy, dandy web app. Just choose your threat (an industry or
pattern), choose what you’d like to protect (confidentiality, integrity, availability,
or everything), and the type of analysis you want to do (all potential attackers or
just the most likely) and let it do the hard work for you.
In closing, if you are not addressing, to an appropriate level, your entire attack
surface, you may be adding locks to a door while a window is left open.

Based on feedback, one of the things readers value most about this report
is the level of rigor and integrity we employ when collecting, analyzing
and presenting data. Knowing our readership cares about such things and
consumes this information with a keen eye helps keep us honest. Detailing our
methods is an important part of that honesty.
Our overall methodology remains intact and largely unchanged from previous
years. All incidents included in this report were reviewed and converted
(if necessary) into the VERIS framework to create a common, anonymous
aggregate dataset. But the collection method and conversion techniques
differed between contributors.

We performed
analysis using
reproducible
research
methodologies.
Multiple team
members validated
all results.

We were seeking
actionable intelligence from the mega-data breach at Sony Pictures
Entertainment (SPE) in November 2014. Online wire-transfer provider Xoom
was probably the year’s first victim of a Business Email Compromise (BEC) to
the tune of $31 million. Palo Alto Networks reported Dridex banking Trojans
“began 2015 with a bang.” Chick-fil-A and OneStopParking were the victims
of payment card breaches which hit the headlines. Sadly, headlines on sites
like AOL and Huffington Post also led to the year’s first major malvertisement
campaign with an exploit kit (EK) attacking browsers with unpatched Adobe
Flash Player. Later in January, Adobe released a new version of Flash Player to
mitigate a zero-day vulnerability being exploited in three advertising networks.
On February 4, Blue Cross health insurance member-company Anthem
announced they were the victims of a data breach along with almost 80 million
people. And on February 27, ThreatConnect reported Chinese threat Actor
“Deep Panda” was probably Anthem’s attacker. Invincea and iSight partners
each released intelligence on a Chinese cyber-espionage campaign that
occurred in November 2014. Dyre, Vawtrak and Carbanak joined the list of
active banking Trojans. Symantec and Microsoft announced the first major
malware takedown of 2015 after the seizure of the infrastructure for the Ramnit
botnet. With no arrests reported in the takedown, it came as no surprise Dr.
Web reported signs of a Ramnit comeback about a month later.
In March, Premera, another Blue Cross member, announced a data breach
affecting 11 million people. ThreatConnect’s intelligence attributed the Premera
breach to Deep Panda. The Mandarin Hotel Group reported a payment card
data breach. POS vendor NEXTEP also reported a breach. March’s takedown
of the “Evolution” deep web marketplace included arrests and it stayed down.
A day after the Canadian Security Intelligence Service (CSIS) reported Vawtrak
was targeting Canadian banks, AVG reported a Vawtrak campaign collecting
banking credentials globally.
Early April brought reports that threat Actors in China had launched “Great
Cannon” DDoS attacks on GitHub, probably targeting censorship-evasion
projects, and Great Cannon also attacked anti-censorship organization
GreatFire. The Drudge Report was one of the sites serving up malvertisements
leading to an EK and the click-fraud Trojan Bedep. Interpol, Microsoft and
several security companies collaborated on two takedown operations seizing
the infrastructure hosting the Simda and Beebone botnets. Pawn Storm
and CozyDuke cyber-espionage campaigns aligned with Russian national
security were the focus of several intelligence reports we collected in April.
InterContinental Hotel Group, Sally Beauty and FireKeeper’s Hotel and Casino
joined the list of payment card data breaches in May. We collected
reports of cyber-espionage attacks on the German Parliament, the Bundestag
and Penn State University but details were scarce and actionable intelligence
was absent altogether. The banking Trojans leading reports in May were
Vawtrak, Dyre and Tinba.
Health insurance breaches were bumped off the top of the headlines for
mega-breaches in June when the US Office of Personnel Management (OPM)
reported another breach. OPM had been breached in March 2014 according
to a New York Times report. The initial tally for the 2015 OPM breach was 4
million persons, but eventually grew to 21 million. ThreatConnect was able to
connect the OPM breach to Anthem. Fortune magazine published a four-part
investigative report on the SPE breach. Wired and Der Spiegel published
reports on the cyber-espionage attacks on the Bundestag initially reported
in May. Cisco reported three security products had a common default Secure
Socket Shell (SSH) key for remote support.
July ushered in a bonanza of data breach reports including Harvard University,
a second breach at Penn State University, Trump Hotels and UCLA. Two other
breaches would echo for several weeks. Social network/online dating site
Ashley Madison suffered a data breach and almost 100 GB of stolen data was
exposed. Italian security and surveillance company Hacking Team was also
breached and 400 GB of data was exposed. Events would unfold and reveal
several previously unknown vulnerabilities in Hacking Team’s stolen data.
The breach bonanza continued in August with reports from American Airlines,
the US Department of Defense, the US Department of Health and Human
Services and the US Internal Revenue Service. The data breach at Carphone
Warehouse was the first report the VCIC collected of a compound attack when
the victim is targeted with a DDoS attack to occupy and distract defenders
while a data breach attack is launched. Wireless networking company Ubiquity
reported it was the victim of a $47 million BEC. AOL and the Huffington Post
were serving up malvertising again. Another malvertising campaign struck MSN,
Telstra and dating site PlentyofFinancial institutionsh.com.

New intelligence on the Chinese cyber-espionage Actor Blue Termite emerged
in September in multiple reports of attacks on Japanese companies. Proofpoint
contributed a report on a different Chinese cyber-espionage operation
targeting Russian military and telecoms. Yet another Blue Cross and Blue
Shield member reported a data breach when Excellus announced a breach
that began in December 2013 compromising the PII and personal financial
information (PFI) of 10 million people.

Data breach reports resumed in October when Experion reported their
system with personal information for 15 million T-Mobile customers had been
breached. UK wireless provider TalkTalk and four million of its customers made
up another breach reported in October. The Daily Mail exposed as many as
15 million visitors to malvertisements. Trend Micro connected Pawn Storm to
multiple attacks using Adobe Flash and Java vulnerabilities first discovered
in the Hacking Team data cache. Another major botnet takedown took place
with seizure of the Dridex banking Trojan’s infrastructure and arrests of Andrey
Ghinkul, Dridex’s author.

In early November the VCIC began collecting intelligence that Dridex
was recovering and resuming operations. Extortion DDoS threat Actor “The
Armada” appeared on the scene attacking several email service providers.
Indictments for the criminals responsible for 2014’s breach of JP Morgan
Chase were made public revealing the bank attacks were part of a stock
fraud scheme. Australian grocery retailer Farmer’s Direct reported the
breach of the account registration information of more than 5,000
customers, but their payment information was not compromised.

It seems every year ends with the InfoSec community fixated on the most recent mega-breach. In December, it seemed that it would be the breach at
the Australian Bureau of Meteorology (BOM). Leaks from the investigation
attributed it to Chinese threat Actors. Virtually no details accompanied
any reports or leaks from the BOM breach. Malvertisements struck The
Independent, The Guardian and The Daily Motion. Juniper reported the
discovery of backdoor vulnerabilities in ScreenOS. As the month and year
were winding up, news broke of power outages that occurred on December
23 in Ukraine. BlackEnergy malware was found on systems in Ukrainian power
companies. It was this breach that the VCIC and many of our colleagues in
InfoSec were focused on at the end of the year.

The segue into February was provided by The New York Times and the Wall Street Journal, with new
reports of targeted cyber-espionage. And Sophos reported a new Citadel-based Trojan crafted to attack
Point-of-Sale (POS) systems using a Canadian payment card processor. We would soon learn that www.
iphonedevsdk.com became a watering hole, using a surprise attack on Java late in the month. Most InfoSec
professionals well remember February as the month Mandiant (now FireEye) released its superb APT1
report. February was also the start of reports of data breaches from large enterprises, courtesy of the
aforementioned iPhoneDevSDK: Facebook, Twitter, Apple, and Microsoft were all victims. Noteworthy
retailer POS data breaches were reported by Bashas’ and Sprouts, two discrete grocery chains in the U.S.
Southwest. Bit9 reported a data breach that began in July 2012, attacking its code-signing infrastructure.
MARCH
Fifty million Evernote users remember that March was the month they were forced to change their
passwords. On March 20, the Republic of Korea suffered a large-scale cyber-attack that included disk
corruption. We remain skeptical that the Cyberbunker-CloudFlare-Spamhaus DoS attack almost broke
the Internet at the end of March. Group-IB reported “Dump Memory Grabber” (a.k.a. BlackPOS), a new POS
Trojan that would go on to make headlines when news broke of Target Stores’ breach in December.

In April, another U.S. grocery retailer, Schnucks, reported a POS data breach. The Syrian Electronic Army
(SEA) did some damage when it hijacked the Associated Press’ Twitter account, sending a tweet reporting
an explosion at the White House and causing a spasm on Wall Street. Operation Ababil continued, but OSINT
cannot support attributing DoS attacks on several European banks to the QCF.

Cyber-espionage continued in May, with reports from QinetiQ and the U.S. Army Corps of Engineers. The
SEA hijacked the Twitter accounts of both The Guardian and The Financial Times. A watering hole attack
targeted nuclear weapons researchers in the U.S. for cyber-espionage, probably from China. More cyberespionage campaigns reported in May included Operation Hangover, targeting Pakistan; Safe, targeting
Mongolia; and operations by the Sunshop actors against Tibetan activists. The U.S. Department of Justice
shut down Liberty Reserve, the go-to bank for cyber-criminals.

Early in June, Raley’s, yet another U.S. grocer with stores in California and Nevada, reported its payment
card systems were breached. NetTraveller, a global cyber-espionage campaign targeting diplomats in
countries with interests not aligned with China occurred. A day later, The Guardian published the first
intelligence leaked by Edward Snowden… and then InfoSec intelligence became the “All-Snowden-All-theTime” channel.

July’s largest retailer data breach was reported by Harbor Freight, a U.S. tool vendor with 445 stores
– nearly 200 million customers and we still don’t know how many records were compromised. The QCF
initiated Phase IV of Operation Ababil. The SEA breached Viber, Tango, and the Daily Dot. The US
Department of Justice indicted four Russians and one Ukrainian for high-profile data breaches, including
Heartland and Global Payments.

In August, the SEA hijacked the Twitter accounts of CNN, The Washington Post, Time Magazine, SocialFlow,
and both The New York Times and New York Post. Attendees of the G8 Summit in St. Petersburg, Russia,
were targeted for cyber-espionage by the Calc Team actors.

In September, Vodafone notified two million customers their personal and financial information had been
breached. Espionage reported in September involved the EvilGrab Trojan and separately, the Hidden
Lynx actors who seem to engage in both espionage and cybercrime. New intelligence linked the Bit9
attack from February with Operation Deputy Dog, Hidden Lynx, and watering hole attacks on Japanese
financial institutions. At the end of the month Brian Krebs began his reports on intelligence extracted from
ssndob[dot]ms. The site was home to data stolen from some of America’s largest data brokers: Lexis-Nexis,
Kroll, and Dun & Bradstreet. Cryptolocker made its first appearance in September, extorting money from
victims that were willing to pay to decrypt their essential files.

On October 3, Adobe announced its systems had been breached; eventually 38 million accounts were
identified as affected. Intelligence connected this to the ssndob[dot]ms actors. Nordstrom, the luxury U.S.
department store, discovered skimmers on some of its cash registers. Two of 2013’s big wins also occurred
in October: Dmitry “Paunch” Fedotov, the actor responsible for the Blackhole exploit kit, was arrested in
Russia, and Silk Road, an online fraud bazaar, was taken down.
NOVEMBER
The proverbial calm before the storm, November was fairly quiet. Banking malware evolved with reports
of Neverquest and another version of IceIX. BIPS, a major European bitcoin payment processor, was the
victim of one of the largest bitcoin heists recorded up to that point in time.
DECEMBER
The last significant entry under cyber-espionage for 2013 was the targeting of foreign ministries in
European countries by Operation Ke3chang. The Washington Post reported its second breach of the year.
And then InfoSec intelligence became the “All-Target-All-the-Time” channel. Although the breach of this
major U.S. retailer was a little more than half the size of Heartland and three-fourths the size of TJX, it’s
vying to become the event for which 2013 will always be remembered.  

Take a deep, calming breath before diving into this last one; it may
result in mental or even bodily harm.

But wait — there’s more!

Remember that promise from last year — “We may be able to reduce
the majority of attacks by focusing on a handful of attack patterns?”
Consider it fulfilled. To us, this approach shows extreme promise as a
way to drastically simplify the seemingly endless array of threats we
must deal with to protect information assets.

Obviously not every organization needs to focus on point of sale
attacks. To make the analysis actionable, we pulled all incidents within
each industry and then applied the patterns to create the work of
art that is Figure 19. It shows the proportion of incidents within each
industry represented by the nine patterns over the last three years.
In order to use Figure 19, identify your industry in the left hand
column. Refer to the NAICS website if you’re unsure where your
organization fits. The percentages are relative to each industry. For
example, 10% of all Retail incidents fall within the “web app attack”.
The coloring should help you quickly identify “hot spots” for your
industry and/or discern differing threat profiles across multiple
industries.

Look up the industry (or industries) that matter to you, identify
which patterns are most relevant, and pay special attention to those
sections in the report (you’ll still want to read the whole thing, of
course).
We’ve heard the (constructive) criticism from some of you noting that
it’s difficult to pick out exactly which findings from the DBIR apply
to your organization, and we spent a lot of time figuring out how to
address that. We hope you’ll agree this is a step in the right direction,
not only for this report, but also for threat analysis and decision
support in general.

Remote attacks against the environments where retail transactions are conducted, specifically
where card-present purchases are made. Crimes involving tampering with or swapping out
devices are covered in the Skimming pattern.

Given recent headlines, some may be surprised to find that POS intrusions are trending down
over the last several years. That’s mainly because we’ve seen comparatively fewer attack sprees
involving numerous small franchises. Brute forcing remote access connections to POS still leads
as the primary intrusion vector. A resurgence of RAM scraping malware is the most prominent
tactical development in 2013.

We know many of you will come to this section hoping to
find all the particulars and dirty laundry of a certain breach
involving a major U.S. retailer in late 2013. Prepare to be
disappointed;  As a consolation prize, however, we hope you’ll
accept our overall analysis of two hundred POS intrusions
that occurred in 2013, along with recommendations on how
you can avoid increasing that number in 2014.

The industries most commonly affected by POS intrusions are
of no surprise: restaurants, hotels, grocery stores, and other
brick-and-mortar retailers are all potential targets. Recent
highly publicized breaches of several large retailers have brought
POS compromises to the forefront. But at the risk of getting
all security-hipster on you — we’ve been talking about this for
years. In fact, this is the main cause of the large dip in 2012 seen
in many of the “over time” charts in this report. We were writing
about RAM scrapers before anyone heard of them and we’re
quite frankly not all that into them anymore because they’ve sold
out and gone mainstream.

Jokes aside, while POS hacks are getting more press recently,
they really have been going on for years and we really have talked
quite a bit about them in previous DBIRs. The media frenzy
makes quite a splash, but from a frequency standpoint, this
largely remains a small-and-medium business issue. Focusing
too much on outliers and headlines can reflect cognitive bias.
For instance, some may be surprised that the number of POS
attacks in 2012 and 2013 is substantially lower than the number
recorded in 2010 and 2011 (despite having ten times more
contributors in the latter years). Figure 20 reminds us that our
understanding of risk should always come back to the data, not
what makes good headlines and marketing fodder.

From an attack pattern standpoint, the most simplistic narrative
is as follows: compromise the POS device, install malware to
collect magnetic stripe data in process, retrieve data, and cash
in. All of these attacks share financial gain as a motive, and most
can be conclusively attributed (and the rest most likely as well)
to organized criminal groups operating out of Eastern Europe.3
Such groups are very efficient at what they do; they eat POSs like
yours for breakfast, then wash ‘em down with a shot of vodka.
While the majority of these cases look very much alike, the steps
taken to compromise the point-of-sale environment offer some
interesting variations.

Once it was stolen, it essentially became
a default password and the attackers also gained knowledge of
the customer base. Armed with this information, the familiar
modus operandi of installing malicious code that captured and
transmitted the desired data began.

Let’s start with the most frequent scenario, which affects small
businesses that may or may not realize just how lucrative a
target they are. This event chain begins with the compromise of
the POS device with little to no legwork; the devices are open to
the entire Internet and, to make matters worse, protected with
weak or default passwords (and sometimes no passwords).

While not as common as the simpler POS intrusions, our dataset
does include several incidents from the first quarter of 2013
that feature a compromise at a corporate location, leading to
widespread compromise of individual locations and malicious
code installations across a multitude of stores. Some cases have
begun with a store compromise that led to penetration of the
corporate network, but the hub-and-spoke architecture allowed
for efficient traversal of the network and the impact of the
compromise was magnified regardless of where “device 0” was
located.

It’s interesting, but not necessarily surprising, that RAM
scraping has usurped keyloggers as the most common malware
functionality associated with POS compromises. One could
theorize that keyloggers (most of which were common varieties
such as Perfect Keylogger and Artemis) are more easily spotted
than the memory-scraping code we witnessed in this data set. Or
perhaps the RAM scrapers, which hook into specific processes of
the POS software, simply do the job better and more efficiently.

One finding that intrigued us is the renaissance of RAM scraping
malware as the primary tool used to capture data. RAM scrapers
allow payment card data to be grabbed while processed in
memory (where it is unencrypted) rather than when stored on
disk or in transit across the network (where it is (ostensibly)
encrypted).

The top three threat actions tell the story rather well (Figure 21).
The perpetrators scan the Internet for open remote-access ports
and if the script identifies a device as a point of sale, it issues
likely credentials (Brute force) to access the device. They then
install malware (RAM scraper) to collect and exfiltrate (Export
data) payment card information.

In years past, we analyzed attack sprees that spanned multiple
victims with no association with each other beyond the use
of truly awful passwords. This report features almost 200
incidents, but in prior years we saw over 200 victims for one
criminal group. The two biggest sprees in our 2013 dataset,
one involving several franchisees of the same company, and the
other affecting multiple corporations, are a bit different, and
lead us to our second common scenario: the use of stolen vendor
credentials. In one case the credentials stolen belonged to a
point-of-sale vendor and were compromised via Zeus malware
infecting the vendor’s systems.

Furthermore, they note an apparent incentive problem when
it comes to mitigating these crafty menaces. Since the
impact of a botnet is often spread around the globe, federal
authorities aren’t always able to amass resources to fight it
on a national level. While the total damage of such a botnet
might be large, specific countries only deal with a small
part of these damages. The initial costs for fighting such a
botnet don’t seem to outweigh the benefits of its takedown.
Nevertheless, the NHTCU continue to fight botnets. In
February of 2013, public broadcaster NOS presented
findings on part of a dropzone of the so-called Pobelka
botnet. After an online checking tool was made available,
500,000 people checked to see if their machines had (at
some time) been infected; of that group, 23,000 self-identified as victims.

Regardless of how large the victim organization was or which
methods were used to steal payment card information, there
is another commonality shared in 99% of the cases: someone
else told the victim they had suffered a breach. This is no
different than in years past, and we continue to see notification
by law enforcement and fraud detection as the most common
discovery methods. In many cases, investigations into breaches
will uncover other victims, which explains why law enforcement
is the top method of discovery and the top contributor of POS
intrusions in our dataset. Long story short, we’re still discovering
payment card breaches only after the criminals begin using their
ill-gotten gains for fraud and other illicit purposes.

Review the interconnectivity between stores and central
locations and treat them as semi-trusted connections. Segment
the POS environment from the corporate network.

Trends observed by the EC3 across member states in
2013 include substantial increases in intrusions, malware,
phishing, grooming, DDoS, espionage, and botnet activity.
It also reports a boom in criminal infrastructure on the
darknet, growth in malware affecting mobile devices,
and wider distribution of malware from cloud services. In
combating these trends, the EC3 has prioritized identifying
criminal network operations and cases, with the potential
for major and lasting impact.

Larger, multi-store companies and franchises should consider a
couple of additional recommendations to limit the impact of a
single-location breach and prevent a mass compromise.

Bottom line: Make it difficult for miscreants to log into a
device that accepts the most targeted piece of information for
financially motived criminals.

The EC3 trained more than 100 law enforcement experts
all over the EU in cyber investigation, tools, and obtaining
forensic evidence. It built a new central forensic laboratory
to assist member state colleagues in obtaining evidence.
It distributed alerts, intelligence notifications, and
threat assessments to stakeholders. Memorandums
of understanding (MoUs) were signed with key private
stakeholders, and a new Advisory Group consisting of
experts outside the law enforcement community was
established (Verizon is happy to be among them).

Do not browse the web, email, use social media, play games, or do
anything other than POS-related activities on POS systems.

Make absolutely sure all passwords used for remote access
to POS systems are not factory defaults, the name of the POS
vendor, dictionary words, or otherwise weak. If a third party
handles this, require (and verify) that this is done, and that they
do not use the same password for other customers.

The job of the EC3 isn’t a small one: it serves 28 European
Union (EU) member states and dozens of countries, and
coordinates protection for 500 million citizens, almost
three-quarters of whom have Internet access. In terms
of operations, the EC3 prioritizes four areas: cyber
intelligence, intrusion, online fraud, and child sexual abuse.
As with any new venture, much of the first year focused
on building infrastructure and capabilities to fulfill these
priorities. Secure network connections to EU and non-EU
partners were rolled out, as well as centralized forensic
analysis environments and tools.

Limit any remote access into POS systems by your third-party
management vendor, and have serious business discussions
regarding how and when they will perform their duties.

Last year’s DBIR featured an appendix from Troels Oerting,
Assistant Director of the European Cybercrime Centre
(EC3), discussing the plans and priorities of the newly
established division of Europol. Law enforcement agencies
play a critical role in this report, and it’s not often we get
to see them in their formative stages. Thus, we thought it
would be interesting to include some reflections on EC3’s
first year of operations.

Monitor network traffic to and from the POS network. There
should be a normalized traffic pattern, and while easier said than
done, anomalous traffic must be identified and investigated.

Stronger passwords would cut out a huge chunk of the problem,
but larger organizations should also consider multiple factors to
authenticate third-party and internal users.

Web applications remain the proverbial punching bag of the Internet. They’re beaten in one of
two ways: by exploiting a weakness in the application (typically inadequate input validation), or
by using stolen credentials to impersonate a valid user. Many of the attacks in our 2013 dataset
targeted off-the-shelf content management systems (eg, Joomla, Wordpress, or Drupal) to gain
control of servers for use in DDoS campaigns.

There’s no question about it – the variety and combination
of techniques available to attackers make defending web
applications a complex task. Regrettably, our discussion of this
complexity is hampered by the level of detail provided on these
incidents. Unless a forensics investigation was performed (a
small subset of the overall dataset), the specific techniques
utilized went largely unreported or were recorded with broad
categorizations. While we have enough material to discuss web
application data breaches at a high level, our ability to draw
conclusions drops as we dig further into the details (which often
aren’t there).

Greed takes a back seat to ideology when it comes to web app
attacks in the 2013 dataset. Just under two out of every three
web app attacks were attributable to activist groups driven by
ideology and lulz; just under one out of three came by the hand
of financially motivated actors; with the small remainder linked
to espionage. After some slicing and dicing we found some very
distinct sub-patterns divided along these motives. The financial
and ideological attacks deserve unique discussion since the
treatment for each may be slightly different.

Financially motivated attackers are hyper-focused on gaining
access to the money, so it follows that their two primary target
industries are the financial and retail industries (where data
that easily converts to money is abundant and, all too often,
accessible). Within the financial industry, they focus on gaining
access to the user interface of the web (banking) application
more so than exploiting the web application itself, because the
application grants logical access to the money. This means they
target user credentials and simply use the web applications
protected with a single factor (password) as the conduit to their
goal. These could have been included in the section on crimeware
(and some did slip through cracks in the algorithm to land there),
but the use of web applications as a vector of attack causes
them to show up here. The tactics used by attackers are all the
usual suspects: a) phishing techniques to either trick the user
into supplying credentials or installing malware onto the client
system, b) the old stand-by of brute force password guessing,
and c) rarer cases of targeting the application through SQL
injection or other application-level attacks as a means to retrieve
credentials, bypass the authentication, or otherwise target
the user-management system. When attribution is possible,
the majority of external attackers utilizing stolen credentials
somewhere along the attack chain hail from Eastern
Europe.
Within the retail industry, we see a slightly different focus. The
primary aim is payment card information (targeted in 95% of
the incidents), which is often accessible simply by exploiting the
web application. Social actions (such as phishing) are mostly nonexistent, most likely because exploiting vulnerabilities inherent
in web applications works plenty well enough. SQL injection was
leveraged in 27 of the 34 (80%) attacks against web applications
in the retail industry, followed by techniques to install and use
web shells (remote file inclusion, etc.) in five of the 34.

Discovery method looks a little bleaker for activists. 99%
of the notifications were external parties (primarily CSIRTs)
contacting victims to let them know their hosts were involved in
other attacks. This is heavily influenced by ideological attackers
quietly using the platform to attack others rather than, for
instance, simple defacements (which are rare in the dataset).
Even though the timeline data is a little sparse, it paints the
picture of quick entry with 60% of the initial compromises
occurring within minutes or less. This reflects the highly
repetitive CMS exploits in this pattern; if it works, it works
quickly. Just over 85% of the incidents are discovered in days
or more, with about 50% taking months or longer to discover.
Once discovered though, we see fairly good reaction time, with
about half of the organizations taking days or less to respond
and contain the incident. This is far better than the norm, which is
typically weeks or longer.

When the actor is financially motivated and the discovery
method is recorded, we see a leading notification method that we
don’t see anywhere else: customers. Perhaps customers notice
the fraudulent activity before anyone else, but something is
definitely tipping them off before any internal mechanism. With
all internal discovery methods combined, only 9% of victims
discovered data breaches of their own accord.

This focus on opportunistically owning just the web server
becomes plain when looking at the assets compromised in the
attack. The web server was the only asset recorded in nearly all
incidents attributable to ideological motives. The actors didn’t
appear to be interested in pushing deeper and wider into the
network. This result may be the product of simply not reporting
those secondary components of the incident — so don’t take this
as advice to only focus on the web server — but it is logical and a
point of contrast to other types of attacks in our dataset.

Ideological actors (whether their motivation is social, political, or
just for plain fun) are less concerned about getting at the crown
jewels than they are about getting a platform (in all senses of the
word) to stand on. With that in mind, it’s not surprising that we
see two types of results from ideological attackers going after
a web server: defacements to send a message or hijacking the
server to attack (including by DDoS) other victims.

Ideology represents the largest identified portion of motives
for web application attacks, and the actors also tend to be
the most geographically diverse. 74% focus on tried and true
exploits targeting, above all else, unvalidated inputs in executed
code. Nowhere is this exploited on a larger scale than Content
Management Systems (CMS) such as Joomla!, Drupal, and
WordPress, and even then, more in the added plugins than the
core CMS code itself.

What we found is a non-finding and the only valid conclusion
to draw from this is that more work is needed to understand
the relationship between web application vulnerabilities and
security incidents. With a non-finding, we can only speculate
on why we are seeing these results. Perhaps this is telling us
that no industry is doing enough. We know three out of four
web-based compromises occur in hours or less of first contact,
and maybe fixing vulnerabilities in 10 days versus 70 days
doesn’t help all that much. Plus, the attacker only exploits one
(maybe two) vulnerabilities. But a different explanation could
be that our lens was focused too wide, and we could learn
more by matching the high-quality WhiteHat data with specific
incident data within the same sample. Whatever the causes,
we do know that web application attacks occur often enough
to repeat what is said in the WhiteHat Website Security
Statistics Report, “What’s needed is more secure software,
NOT more security software.”


Wouldn’t it be great to know that (quickly) patching web
application vulnerabilities helps? This year we partnered
with WhiteHat Security in order to combine and compare
the incident data we’ve collected against the vulnerability
assessment data they collect from tens of thousands
of websites across hundreds of the most well-known
organizations. After some back and forth, we decided first to
break out the data by industries (because patterns emerge
across industries), then we decided to compare two data
points on web vulnerabilities to the incident data: the average
(mean) vulnerabilities per site and median days to patch.
We assumed that industries with fewer vulnerabilities and
quicker patch time would be less represented in the breach
data (i.e., have fewer incidents) and so we applied some good
old-fashioned statistics and were admittedly let down when
we didn’t see the relationship we were expecting.

Remember that action varieties in VERIS are not mutually
exclusive, and it’s common to see more than one in a single
incident. Unapproved hardware and email misuse/data
mishandling (tied) round out the top three actions in the
misuse category, but they’re more a function of how the data is
exfiltrated rather than how it’s acquired. Unapproved hardware
refers to employees using devices like USB drives that are
either forbidden altogether or allowed but subject to various
restrictions. An employee sending intellectual property out to his
or her personal address is an example of email misuse. We also
reviewed cases where system administrators abused the email
system, posing as another user and sending messages under that
identity, with the goal of getting them fired. Data mishandling
takes place when someone uses data in a manner counter to
the organization’s policies. For example, a call center employee
who writes customer credit card numbers down on paper, or an
engineer who skirts policy by taking restricted documents home
to examine on a personal computer.

Note that not all are within the
misuse category [mis]; stay tuned for more on that later. Not
unexpectedly, privilege abuse — taking advantage of the
system access privileges granted by an employer and using
them to commit nefarious acts — tops the list. We realize that
encompasses a very broad range of activities, but the overall
theme and lesson differ little: most insider misuse occurs within
the boundaries of trust necessary to perform normal duties.
That’s what makes it so difficult to prevent.

An organization’s intellectual property is among its most
valuable assets, frequently driving its ability to compete in the
market. In many cases, organizations also have custody of vast
amounts of data about the customers they serve, the employees
who serve them, and the relationships they rely upon to do
business. This data has value to the organization, but also to
those who would seek it for their own personal benefit or myriad
other reasons. For the misuse pattern, we focus on those who
already have a trusted place inside the organization. Arguably,
the most prominent case of internal misuse in the headlines this
past year has been that of U.S. government contractor Edward
Snowden. While this is an extreme example of the damage that
determined insiders can inflict, it illustrates the risk that exists
when an organization must place trust in individuals.

With more incidents than ever before involving trusted
parties, we could more easily see how they go about acquiring
the data when their own access is insufficient. In addition
to abusing entrusted privileges and resources, we observed
hacking techniques to elevate privileges (often by stealing
others’ credentials) and circumvent controls, various forms of
social engineering, and the use of malware like keyloggers and
backdoors. These cads have even resorted to physical theft,
taking documents such as blueprints and other intellectual
property, often denying availability to the original organization
by taking the only copy.

As mentioned in the beginning of this section, insiders aren’t the
only ones who misuse entrusted privileges and resources. Figure
33 gives an account of external and partner actors who directly
or indirectly participated in incidents of misuse. Organized
criminals bribe insiders to steal data for fraud schemes. Former
employees exploit still active accounts or other holes known
only to them. Competitors solicit intellectual property to gain
business advantages. To mount a proper defense, organizations
must take into account that such players are on the field.
Nearly all misuse incidents prior to 2013 centered on obtaining
information to use for fraud. As Figure 34 shows, we saw more
insider espionage targeting internal organizational data and
trade secrets than ever before.

Let’s take a look at the people committing these crimes. While
payment chain personnel and end-users were still prominent,
managers (including those in the C-suite) came in higher than in
prior years. You know the type; one of those straight shooters
with upper management written all over him. They often have
access to trade secrets and other data of interest to the
competition and, tragically, are also more likely to be exempted
from following security policies because of their privileged
status in the company. One of those “white-collar resort
prisons” won’t do for their ilk.

Desktops are the most frequently compromised asset in this
pattern, which makes sense because desktop computers are
an employee’s primary interface to the rest of the network
(Figure 36). Typically, this is where the data is stored, uploaded,
or emailed out of the organization, or copied onto removable
media. Databases and file servers, both repositories of so much
valuable information, are also targeted regularly. Payment
cards doesn’t refer to the variety of data, but rather actual
cards that were run through handheld skimming devices (or
otherwise copied) in the classic “evil waiter” scenario. As far
as asset ownership, we see insiders abusing corporate-owned
rather than employee-owned (“BYOD”) assets allowed for
corporate use. However, we do see evidence they often leverage
unapproved personal devices to help them get the data out of the
organization (which shows up as use of unapproved hardware).

The CERT Insider Threat Center (another partner of ours)
focuses research on insider breaches, and it determined that
in more than 70% of the IP theft cases, insiders stole the
information within 30 days of announcing their resignation.8
On quite a few occasions, a review of the activity of outgoing
employees with access to sensitive information allowed
impacted organizations to detect the incident and act quickly to
retrieve the information (hopefully before irreparable damage
had been done).

The root cause of data theft and other illicit acts by trusted
parties is, rather obviously, an employee breaking bad. No, not
in a Walter White sense; more like a white collar crime sense
(though Walter did ***SPOILER ALERT*** murder his boss
and execute a forced takeover of the business, so it is rather
apropos). While it’s impossible to stop all rogue employees, there
are some steps that can reduce the likelihood of an incident
occurring, or at least increase your chances of catching it quickly.

Speaking of laptops, they’re the most common variety of asset
reported with this pattern. Incident reports — especially to
CSIRTs — often don’t specify the asset lost or stolen. Thus,
“some kind of user device” is all we can infer and explains why
“Other (user dev)” is so frequent. Beyond that, it’s what you’d
expect: computers, documents, and drives.
The next thing to note is the ratio of loss to theft; losing
information assets happens way more than theft, by a 15-to-one
difference. And that’s important because it suggests the vast
majority of incidents in this pattern are not due to malicious
or intentional actions. Thus, the primary challenge is to a) keep
employees from losing things (not gonna happen) or b) minimize
the impact when they do. The smart money is on option b, though
bio-implanted computing devices do hold some future promise
for option a. That’s about all we’re going to say about loss, but
theft still has a few more lessons for us.

Studying the findings yielded a few interesting observations
that may help inform practice, and that’s where we’ll focus our
attention in this section. As we begin, keep in mind that we’re
specifically talking about information assets;10 whatever was
lost or stolen had to store, process, or transmit information in
order to get our attention.

To be honest, we debated whether or not to include a section on
lost and stolen assets in this report. We decided, however, that
we simply couldn’t ignore the blatant fact that such incidents
— while not sexy or “cyber-y” — are among the most common
causes of data loss/exposure reported by organizations. This
is especially apparent in industries like Healthcare, where the
disclosure of all incidents that potentially expose sensitive data
is mandatory. And if there’s anything we know to be true about
human nature, it’s that losing things and stealing things seem to
be inherent predispositions.

We find it quite surprising that the highest proportion of thefts
occur in the victim’s work area, which basically refers to the
main office space or cube farm (Figure 40). That suggests
simply having sensitive information “behind locked doors” isn’t
enough; there are still a lot of people inside those locked doors.11
Notice that thefts in internal high security areas are much less
common, but still post higher than public facilities. That last bit
is counterintuitive to the point of irrationality; we can’t help but
suspect that people whose laptops are stolen when they take a
potty break at the coffee shop simply report them as “lost” to
save face.

Personal residences and personal/partner/public vehicles serve
as the venue for nearly 40% of thefts and remind us that devices
on the go are prone to go missing.

While it’s usually not known/reported exactly how actors gained
physical access to these locations, over 80% of thefts where we
do have that info involved disabling or bypassing controls. The
remainder had access already, either because they were granted
privileges or because it was a publicly accessible location.

The final set of observations covers the variety of data that
was compromised or, more often, potentially exposed when
assets were lost or stolen. It’s worth pointing out that the
primary reason most of these incidents are included is because
they tripped some kind of mandatory reporting/disclosure
requirement. The asset went missing, was determined to
contain regulated information that is now exposed to potential
unauthorized access, and therefore had to be reported. This
explains the predominance of regulated data like personal or
identifying information and medical records in Figure 42.

The primary root cause of incidents in this pattern is
carelessness of one degree or another. Accidents happen. People
lose stuff. People steal stuff. And that’s never going to change.
But there are a few things you can do to mitigate that risk.

Considering the high frequency of lost assets, encryption is
as close to a no-brainer solution as it gets for this incident
pattern. Sure, the asset is still missing, but at least it will save
a lot of worry, embarrassment, and potential lawsuits by simply
being able to say the information within it was protected. Also,
periodically checking to ensure encryption is still active is
right up there too. This will come in handy when the auditor or
regulator asks that dreaded question: “How do you know for sure
it was encrypted?”

Encourage employees to keep sensitive devices in their
possession and in sight at all times. Yes, this applies to fancy
client dinners and visits to the restroom. It’s not a bad principle
to apply to mobile devices in a corporate setting either. It may
be awkward, but it’s safer than leaving it in the car or unattended
in a room full of strangers. If it absolutely must be left in the car,
lock it in the trunk before you leave the office and don’t leave it
there overnight.

Back it up.

Regular (and preferably automatic) backups serve a threefold
purpose. They salvage weeks/months/years’ worth of
irrecoverable work, get you productive again on a new device
with minimal down time, and help establish what data was on the
device to determine if disclosure is necessary.

In light of the evidence that so many thefts occur in the office,
cabling or otherwise securing equipment to immovable fixtures
should at least be considered. The big caveat, however, is that
the majority of such thefts were documents taken from the filing
cabinet and mobile devices (including laptops). A more effective
strategy would be to move highly sensitive or valuable assets to
a separate, secure area and make sure they stay there.

Yes, it’s unorthodox as far as recommendations go, but it might
actually be an effective theft deterrent (though it will probably
increase loss frequency). That shiny new MacBook Air on the
passenger seat may be too tempting for anyone to resist, but
only those truly dedicated crooks will risk incarceration for a 4”
thick mid-90’s lap brick. Or, if being the fastest hunk of junk in
the galaxy is a must, perhaps there’s a lucrative aftermarket for
clunky laptop covers. She may not look like much, but she’s got it
where it counts, kid.

There are only two difficult problems in computer science: cache
invalidation, naming things, and off-by-one errors. Misdelivery
(sending paper documents or emails to the wrong recipient)
is the most frequently seen error resulting in data disclosure.
One of the more common examples is a mass mailing where
the documents and envelopes are out of sync (off-by-one)
and sensitive documents are sent to the wrong recipient. A
mundane blunder, yes, but one that very often exposes data to
unauthorized parties.

Misdelivery (sending paper documents or
emails to the wrong recipient) is the most
frequently seen error resulting in data
disclosure.

Nearly every incident involves some element of human error.
For example, failing to apply a WordPress patch certainly leaves
the application vulnerable to attack, but it doesn’t directly
compromise the system. Some other threat actor/action is
required to do that. Without drawing that distinction, this
category would be so bloated with “incidents” that it would be
difficult to extract useful information.

According to our sample, government organizations
frequently deliver non-public information to the wrong
recipient; so much so, in fact, that we had to remove it from
Figure 43 so that you could see the other error varieties.
Why is that number so large? The United States federal
government is the largest employer in that country, and
maintains a massive volume of data on both its employees
and constituents, so one can expect a high number of
misdelivery incidents. Public data laws and mandatory
reporting of security incidents also cover government
agencies. Since we have more visibility into government
mistakes, it creates the impression that government
mistakes happen more frequently than everyone else’s,
which may not be the case. This is not unlike the way we
see higher numbers of overall breaches in U.S. states that
have had disclosure laws on the books the longest. Case
in point: even with government misdelivery removed from
the results, misdelivery still dominates the list of errors
resulting in exposed data.

Oxford Dictionaries declared that “selfie” was 2013’s word of
the year, but did you know that posting content to the web and
later regretting it was a meme in the corporate world too? That’s
right, the second most frequent error variety is publishing errors,
which often involve accidentally posting non-public information
to a public resource, such as the company web server. That’s
why web application takes the number two spot on the affected
assets chart. Rounding out the top three in this
category is disposal error, where the affected asset is thrown
away without being shredded or, in the case of digital media,
properly cleared of sensitive data.

Who’s making all these mistakes? Well, it’s
almost entirely insiders, of course. End-users,
sysadmins, and developers lead the pack when
it comes to mucking things up, though pretty
much all of us are guilty.

Organizations only discover their own mistakes about one-third
of the time. Otherwise, an external entity makes them aware
of the incident, and most frequently it’s the organization’s own
customers. You could try the “Inconceivable!” tactic when a
customer calls to say they found their unprotected personal data
on your website — but if you keep using that word, they’ll figure
out it doesn’t mean what you think it means.

Who’s making all these mistakes? Well, it’s almost entirely
insiders, of course. End-users, sysadmins, and developers lead
the pack when it comes to mucking things up, though pretty much
all of us are guilty. But the interesting thing is that there’s quite a
large number of incidents (70) caused by partner errors — more
than any other pattern.

Bob Ross, everyone’s favorite painter of fluffy little clouds, once
said, “We don’t make mistakes, we just have happy accidents.”
Even still, organizations can take steps to decrease the
frequency of all manner of accidents by reducing their exposure
to the common error patterns that result in data disclosure.

Consider implementing Data Loss Prevention (DLP) software
to reduce instances of sensitive documents sent by email. DLP
can identify information that follows a common format, such as
credit card numbers, social security numbers, or medical billing
codes.

An example from the VCDB shows how bad things can get
when document disposal goes wrong. A medical center
arranged to have a vendor pick up documents and shred
them prior to disposal. Apparently, an actual “pickup” truck
was used, because the files ended up all over the roadside
instead. “It looked like a blizzard of white paper had struck
the area,” according to one witness. These were old medical
records with all manner of protected information. When
people found them and called law enforcement, an inmate
crew doing regular trash pickup in the area was sent to
retrieve these sensitive documents. And that’s the sound of
the men working on the “cha-ching” gang.

Decrease the frequency of publishing errors by tightening up
processes around posting documents to internal and external
sites. For example, have a second reviewer approve anything
getting posted to company servers, develop processes to
regularly scan public web pages for non-public data, and
implement a blanket prohibition against storing un-redacted
documents on a file server that also has a web server running. It’s
amazing how easy it is for a spreadsheet to migrate over to the
htmldocs folder. Make sure there’s a process to test the security
controls after a change — we’ve often seen a failure to put
controls back in place result in a publishing breach.


Say that three times really fast. When sending large postal
mailings (also prone to error at higher speed and repetition),
spot-check a sample to ensure that the information in the
document matches the name on the envelope. Watch out for
window envelopes too – sometimes that window might be too
big or your content might not be centered properly, allowing
sensitive information to show through. A lot of these incidents
could have been prevented if someone had popped a few
envelopes off the stack and inspected them before they went in
the mail. Any disposal or sale of information assets should
be coordinated by the IT department. Educate users to think of
disposing of a computer the same way they think of disposing
of hazardous materials. “You can’t just throw that in the trash
(or sell it on eBay)! Send it to IT for proper handling.” Test the
disposal process by sampling devices to verify they’ve been
sanitized properly. If a third-party handles this, ensure that
contracts stipulate how to transfer, store, and dispose of data,
along with roles, responsibilities, verification, and penalties for
non-compliance.

Any malware incident that did not fit other patterns like espionage or point-of-sale attacks.

The primary goal is to gain control of systems as a platform for illicit uses like stealing
credentials, DDoS attacks, spamming, etc. Web downloads and drive-bys are the most common
infection vectors.

Many incidents in this section come from our CSIRT partners,
reflecting a roll-up across many victim organizations. The
level of detail tends to be lower because there was no forensic
investigation or similar in-depth analysis (or the report wasn’t
provided to the CSIRT), leaving VERIS metrics a bit sparse.
But the high number of incidents still offers some insight into
day-to-day malware infections where the victim’s anti-virus (AV)
and intrusion prevention system (IPS) shields could not repel
firepower of that magnitude.

As expected, this incident pattern consists mainly of
opportunistic infections tied to organized criminals with some
kind of direct or indirect financial motive (hence the title
“crimeware”). Once malicious code has acquired a level of access
and control of a device, the myriad possibilities to make a buck
are opened up for the attacker.

In not-so-shocking news, Zeus continues to be a favorite way
to make a buck with crimeware in 2013 (see sidebar for more
detail). Zeus and its offspring, Citadel, primarily focus on
stealing money via bank account takeovers, though they can also
be used for other functions. Zitmo (“Zeus in the Mobile”) also
shows up in the data.

Zeus (sometimes called “Zbot”) is sort of the cockroach of
malware. It has managed to survive and even thrive despite
many attempts to eradicate it. International arrests and the
supposed retirement of the original author have not slowed
it down, and once the source code behind it was published,
other programmers could modify and extend Zeus for their
own purposes, including evading antivirus software. In fact,
Citadel started off as a variant of Zeus but has evolved
substantially. Zeus can be used to install other malware
but often grabs login and banking credentials from within
browsers. Despite the efforts of many, it has continued to
elude the good guys that are trying to shut it down.

This one primarily targets Android and Blackberry mobile
devices for similar purposes. While Zeus serves as an example
of crimeware families reported all around the world, others had
a more localized presence. Nitol, for instance, was quite common
among incidents reported to MyCERT of Cybersecurity Malaysia,
but we have no instances of it infecting systems outside Asia.
Nitol allows backdoor access and frequently causes infected
systems to participate in DDoS attacks.
Expanding online markets, where specialists offer cybercrime-as-a-service, became a growing trend in 2013. A good example
in the Netherlands was the wave of DDoS attacks on banks
and specific institutions since March, 2013. So-called “booter
websites” have made this type of attack available to literally
anyone who wants to attack a company or institution.

Crimeware incidents are light on timeline and discovery details
because the response is often to just wipe the system and
get it back to work (remember, this pattern comprises a lot of
one-off infections that don’t fit other patterns). When known,
notification by unrelated third parties (namely CSIRTs) were by
far the most common way victims learned of the incident.

Like us, your first reaction might be “why not technologies like
IDS and AV?” This reflects the role of CSIRTs as the primary
provider of crimeware incidents in this dataset. The discovery
method wasn’t known for 99% of incidents; it’s not usually within
their visibility or responsibility. For all we know, CSIRTs only saw
the 1% not discovered by AV or IDS. The discovery timeline in
Figure 52 hints that this might, in fact, be the case. Notice the
difference in N between Figure 51 and Figure 52 and how many
infections are discovered within seconds — only automated
detection methods would be so quick.

The majority of crimeware incidents start via web activity —
downloads or drive-by infections from exploit kits and the like
— rather than links or attachments in email.15 Adware still shows
up, though Bonzi Buddy thankfully remains extinct. For malware
with a social engineering component, both scams and phishing
play important roles.16 Infected assets usually weren’t identified,
but interestingly, those that were reported more servers than
user devices. Wow. So vectors. Much families. Many incident.

Victims don’t always report malware functionality, but when they
do, they prefer C2 (according to the most interesting CSIRTs
in the world, at least). This makes perfect sense, as the goal is
to achieve and maintain control of a device to command it to
do your bidding. Whether the little compromised minions are
participating in a spam botnet, stealing banking credentials, or
hijacking a browser to artificially boost ad revenue, there are
numerous ways to leverage compromised workstations that
don’t entail deeper penetration into a network.

These results led us to develop some specific recommendations
to help keep incidents of crimeware down. The natural question
to ask is “what happened to antivirus?” AV technologies
play an important role in catching many types of commodity
malware and preventing compromise in the first place. So this
pattern reflects a reverse sort of “survivorship bias” in which
we look primarily at the sorts of things AV does not do as well
— or organizations that don’t do AV particularly well. Nitol, in
particular, infected many systems at the factory before shipping
and likely before users or administrators had deployed any
sort of AV. The Zeus and Citadel family has a well-deserved
reputation for evolving quickly to evade signature-based
detection of the sort used by many AV products.

Keep browsers up to date.

Zeus frequently uses a technique called “man in the browser”
that involves using browser vulnerabilities and add-on functions.
Keeping browsers and plugins secure will go a long way toward
reducing the impact of this sort of incident. Apply browser
patches as quickly as software producers make them available.

Legacy apps may complicate this, but if possible, avoid using
Java browser plugins, given the difficulty in sandboxing content
and the history of vulnerabilities here.

Our results link crimeware to stolen credentials more often than
any other type of data. This points to the key role of crimeware
when the attack objective is to gain access to user accounts.
Two-factor authentication won’t prevent the theft of credentials,
but it will go a long way toward preventing the fraudulent re-use
of those credentials.

Change is good, except when it isn’t.


Consider how best to deploy system configuration change
monitoring. Unlike iocane powder, many of the vectors and
persistence methods used by crimeware can be easily detected
by watching key indicators on systems. This goes to the general
theme of improving detection and response rather than solely
focusing on prevention.

Given the high incidence of C2 communications, using feeds of
threat data that identify IP addresses and domain names used to
control botnets, then matching this data against firewall or proxy
logs can help accelerate detection and thus containment. We
generally don’t recommend using these lists for outright blocking
due to possible operational issues. But malware researchers do
a fine job of implementing sinkholes and reverse-engineering
malware quickly to identify infrastructure used by the bad guys.

The Internet threat landscape in Poland is largely defined by
banking Trojans — crimeware aimed at stealing users’ online
banking credentials. These use a combination of social
engineering and software vulnerabilities to gain access to
a user’s computer, and subsequently to their bank account.
Whenever new financial malware or attack methods surface,
Polish users are often among the first hit. 2013 also saw
numerous financial malware botnets using Polish Internet
properties for C2 purposes (including .pl ccTLD domain
names). Over 20 such botnets were taken over or disrupted
by CERT Polska.
The attacker’s tool of choice is a variety of web-inject
malware (with Zeus/Citadel being the most popular
malware family), which infects a user’s machine, and then
injects code into the browser whenever that user visits a
banking site deemed of interest (a “Man-in-the-Browser”
attack). A common theme is the use of social engineering
techniques to obtain credentials. For example, attempts
are made to install one-time password stealers to intercept
mobile transaction authentication numbers (mTANs) used
by banks to authenticate transactions. In such cases, a
user is prompted to provide his mobile number, supposedly
to install a new security certificate designed by the bank
on his smartphone – but what, in reality, is malware used
to intercept and redirect text messages to the attacker.
Cruder methods to subvert two-factor authentication
are also employed: fictitious bank messages are injected,
notifying the recipient of an erroneous bank transfer and
asking for the money to be returned - to an attacker’s
account. Real world events are often exploited: a recent
brand merger involving the largest Polish online bank
resulted in attacks forcing users to redefine lists of
permanent transfers – redirecting them to an attacker’s
bank numbers – under the auspices of changes resulting
from the merger.
However, it’s not just web-inject malware at work: other
tricks observed in 2013 include malware that switches
bank account numbers to those of the attacker during a
copy/paste operation in Microsoft Windows. It’s not always
about malware either: late 2013 saw large scale attacks
against home routers, which had their DNS server settings
subsequently reconfigured to point at rogue DNS servers.
These were then used to perform Man-in-the-Middle
attacks through a series of proxies, subverting SSL and
two-factor authentication mechanisms by using social
engineering methods similar to those described above.

All incidents in which a skimming device was physically implanted (tampering) on an asset that
reads magnetic stripe data from a payment card (e.g., ATMs, gas pumps, POS terminals, etc.).

There’s not a ton of variation in this pattern at the VERIS level: criminal groups install skimmers
on ATMs (most common) and other card swipe devices. On a more qualitative level, the skimmers
are getting more realistic in appearance and more efficient at exporting data through the use of
Bluetooth, cellular transmission, etc.

In 2013, most skimming occurred on ATMs (87%) and gas pumps
(9%) due to the relative ease with which they can be approached
and tampered with. Gas pump skimmers are often installed by
a small group of people acting in concert. One scenario involves
one or more conspirators going into the station to make a
purchase and distract the cashier’s attention, while a partner in
crime plants the device inside the machine using a universal key.
ATM skimmers, on the other hand, are installed on the outside
of the machine. While some ATM skimming devices are clunky
homemade affairs that might afford an opportunity for
observant customers to spot them, the design of many skimmers
(both those created by the criminal and those purchased “off the
shelf”) can be so realistic in appearance that they are virtually
invisible to the end user. In most cases they can be snapped in
place in a matter of seconds and can be produced in sufficient
quantities to make the attacks scalable and highly organized.
This, however, has been the norm for some time and warrants
only a cursory mention in this report. What has changed over
time, however, are the methods by which the data is retrieved by
the criminals.

For a wide array of criminals ranging from highly organized crime
rings to garden variety ne’er-do-wells who are turning out no
good just like their mama warned them they would, skimming
continues to flourish as a relatively easy way to “get rich quick.”
While most incidents are linked to Eastern European actors,
nearly all victims of payment card skimmers in this report are
U.S. organizations (the U.S. Secret Service and public disclosures
being the primary sources for this data). While some don’t think
we should include this type of attack in the DBIR, we can’t justify
excluding a tried-and-true method used by criminals to steal
payment card information.

In the past it was necessary for the criminal to return to the
scene and physically remove the device in order to collect the
stolen data. While we continue to see this, it’s normally indicative
of the less organized and more small time crooks out to “make
some sweet moolah with Uncle Rico.” They’re often apprehended
when retrieving the skimmed card data. In keeping with what
we find with network-based attacks, the successful criminal is
the one who can maintain a safe distance between themselves
and the target. Therefore, the more highly skilled criminals now
collect data via Bluetooth or SIM cards with remote caching and
tampering alerts. Some devices actually send an SMS alert to the
criminal each time the ATM is used.

Like any technology, the tendency is to develop from bulky
and slow toward streamlined and efficient. Skimming
devices are no different. Many people still think of the
traditional skimmer as the classic wedge – the small handheld skimmer typically used by waitstaff to illicitly obtain
mag stripe data while they had the card away from the
customer. Because they were so easy to use, they became
the stock-in-trade for most criminals for a long time.
On the flip side, it was often relatively easy for the good
guys to pinpoint the culprit after the fraud had transpired.
Common Point of Purchase (CPP) algorithms could be used
to determine the restaurant responsible for the fraudulent
charges. When law enforcement arrived at the restaurant
they could obtain access to the receipts, and with relative
ease determine that the same waiter/waitress served all
the victims. The individual would then be interviewed and…
well, you know the rest.
Fast-forward to the year 2000, when the first gas pump
skimmer was found at a gas station in California. The
skimmer was placed inside the pump, and (since it only
captured track information) the criminals set up a wireless
video camera 300 yards away in a weatherproof case. In
this particular instance, the camera was discovered and
unplugged by an investigator. Within minutes, the bad guys
showed up at the gas station to see what went wrong, and
were promptly taken into custody.
Eventually the risk of discovery during retrieval became too
great, so more criminals started manufacturing skimmers
and selling them online. This new wave of devices was
Bluetooth equipped, which allowed someone to download
the track and PIN data from the safety of the parking lot.
It’s now possible to buy online skimming devices with
built-in SIM cards that allow for remote configuration,
remote uploading of data, and tampering alerts that, if
triggered, will cache the data and send it out immediately,
greatly reducing risk.

With subterfuge and fraud being the objectives behind skimming,
it’s not surprising that it’s most commonly detected by a third
party. Most of the time that third party is a payment card
company or a customer who has noticed fraudulent activity.
Other times it’s a phone call from a law enforcement agency after
they’ve arrested a gang with a trunk full of skimming devices
and white plastic cards. Hanging close to that pack of external
discovery methods are internal users who spot tampering and
report it to management. Way to go, folks. As skimmers become
more difficult to detect visually, however, we can’t help but
wonder if this latter scenario will become increasingly rare.

Though some might argue, we find no obvious mistake or
oversight on the part of organizations that allows skimming to
succeed when it otherwise wouldn’t. But there are some things
that can be done to make it harder for the criminal and shorten
the window of exposure.

As the merchant, this probably isn’t something you can do
yourself, but be aware that certain designs are more susceptible
to skimming devices than others. Many modern ATMs are
designed with this in mind; choose those if possible.

Use tamper-evident controls.
Do things that make it obvious (or send an alert) when tampering
occurs. This may be as simple as a sticker over the door of a
gas pump or more sophisticated tactics like visual anomaly
monitoring on ATMs.

Regularly check terminals for signs of unauthorized tampering.
Also train employees to spot skimmers and recognize suspicious
behavior from individuals trying to install them. If a criminal is
able to place a skimmer on one of your devices, these regular
inspections will help curb the damage.

When entering your PIN, cover your hand to block tiny cameras
that may be recording it. You wouldn’t want a ne’er-do-well
getting ahold of your PIN now, would you?

Trust your gut.

If something looks out of the ordinary at your ATM or gas pump,
something Financial institutionshy may be afoot. While criminals are increasingly
sly at designing difficult-to-detect skimmers, you still might be
able to notice something amiss, especially if the terminal looks
different than others around it. If one of these things is not like
the others, don’t swipe your card!

See something, say something.
If something seems out of place to you at a payment terminal,
don’t keep it to yourself. Be sure to tell the merchant or bank
that you may have found a skimmer. Not only will you be helping
them, you’ll also be helping your fellow consumers.

Most surprising to us is the consistent, significant growth of incidents in the dataset. We knew
it was pervasive, but it’s a little disconcerting when it triples last year’s already much-increased
number. Espionage exhibits a wider variety of threat actions than any other pattern. The most
evident changes from our last report include the rise of strategic web compromises and the
broader geographic regions represented by both victims and actors.

Comprehensive information about “cyber”19 espionage is really
hard to come by. Organizations typically aren’t required to
publicly disclose breaches of internal information and trade
secrets, as they are with regulated consumer data. Additionally,
there’s no fraud algorithm to alert victims about illicit use of
such data, leaving many cases of espionage undiscovered. Most
of what we know publicly about this genre of threat comes
from incident responders, intelligence analysts, and malware
researchers who compile and share their knowledge with the
community. Thus, we’re excited to have quite a few contributors
from these circles, whose information has more than tripled the
number of espionage incidents in this year’s dataset, to 511.

Before someone concludes we’re asserting a vast increase in
espionage in 2013, we’re quite sure countless organizations
have been consistently targeted for several years. Instead,
we attribute this increase primarily to our ever-expanding set
of contributors conducting research in this area, along with
more community information sharing that improves discovery
capabilities. Like a streetlight illuminating cars parked along
the street, more contributors allow us to see more cars.
Unfortunately, we can also see that those cars have broken
windows and stolen stereos.

In addition to geographic broadening, we see a wide distribution
of both sizes and types of victim organizations. Unfortunately,
victim size is often not tracked, so there are a lot of unknowns
here. Insofar as we can determine from the data before us,
however, size doesn’t seem to be a significant targeting factor.
Industry, on the other hand, does: the Public, Professional, and
Manufacturing sectors are more targeted by espionage than
the rest of the field (which still runs a fairly wide gamut). There
is little doubt that figures for the Public sector, which spans
embassies, economic programs, military, and other support
organizations, are boosted by our government contributors.
There is also little doubt that they are a prime target for
espionage. Victims within the Professional, Scientific, and
Technical Services category typically deal with custom computer
programming services, research and development, engineering
and design, and legal practices. Many of these organizations
are targeted because of the contracts and relationships they
have with other organizations. For some, they can serve as
both a valuable aggregation point for victim data and a trusted
exfiltration point across several target organizations. Lastly, and
not unexpected, Manufacturing industries are also targeted for
their intellectual property, technology, and business processes.

Attribution is also probabilistic in nature. Be wary of threat
intelligence vendors claiming to be 100% sure an attack is X
actor group from Y country with Z motives; they are “likely”
incorrect. There are many methods for determining attribution
— sometimes it’s following the breadcrumbs left by the actors.
Other times it’s ruling out the alternatives using something like
analysis of competing hypothesis.20 None of these methods
are perfect. It’s important to carefully evaluate information
to make sure one isn’t suffering from some type of cognitive
bias.21 It would be more helpful if probabilistic language like
Sherman Kent’s “Words of Estimative Probabilities”22 was used
when describing attribution to particular countries, regions, and
threat actors. With that in mind, the following would fall between
“Probable” and “Almost Certain.”

To set the tone, we need to understand the victims represented
within the data. We don’t claim to cover all espionage activity in
2013 — quite far from it, actually. As is evident in Figure 57, the
sample is still largely (over half) U.S. based, but not as exclusively
as in previous years. We expect this to continue as more global
organizations join the cause. We can’t help but wonder why we
have no examples of Italian victims of espionage in our dataset.
Our best hypothesis is that sophisticated actors remember the
classic blunder of “go[ing] in against a Sicilian when death is on
the line” when selecting targets (the most famous blunder, of
course, is getting involved in a land war in Asia).

With respect to actor origin, the percentage of incidents
attributed to East Asia is much less predominant in this year’s
dataset. Two countries in particular, the People’s Republic of
China and the Democratic People’s Republic of Korea, represent
that region. This underscores the point we made in our last report
– that, despite our China-exclusive results, China definitely was
not the only country conducting espionage.

The 2013 dataset shows much more activity attributed to
Eastern European actors, Russian-speaking ones in particular.
As before, we don’t propose these are the only active regions/
countries engaged in espionage. More comprehensive research
into different actor groups is continually driving better detection
and attribution, and we hope future versions of this report will
show the fruits of those efforts. At a high level, there doesn’t
seem to be much difference in the industries targeted by East
Asian and Eastern European groups. Chinese actors appeared to
target a greater breadth of industries, but that’s because there
were more campaigns attributed to them.

One aspect of this pattern that sets it apart from others is the
wide variety of threat actions. Many of the other patterns have
simpler stories with relatively few VERIS actions. Espionage
breaks that mold in a big way, though the specific actions
involved won’t be a surprise to many readers. State-affiliated
groups often deploy a wide range of tools (or tools that have
wide range of capabilities).

It’s interesting that, while the array of tools is diverse, the basic
methods of gaining access to a victim’s environment are not. The
most prolific is the old faithful: spear phishing. We (and others)
have covered this ad nauseam in prior reports, but for both of
you who have somehow missed it, here goes: A well-crafted and
personally/professionally-relevant email is sent to a targeted
user(s), prompting them to open an attachment or click a link
within the message. Inevitably, they take the bait, at which
point malware installs on the system, a backdoor or command
channel opens, and the attacker begins a chain of actions moving
toward their objective. The proportion of espionage incidents
incorporating phishing is lower than our last report (it was 95%),
but not because of a drop in actual frequency. This is primarily
due to a big increase in the use of strategic web compromises
(SWCs) as a method of gaining initial access.

Instead of email bait, SWCs set a trap within (mostly) legitimate
websites likely to be visited by the target demographic. When
they visit the page, the trap is sprung, the system infected, and
the rest is the same as described above. Even if detected quickly,
SWCs can provide a very high reward for attackers. Furthermore,
the industry has observed some maturation of the SWC
technique, which assists the actors in focusing their targets and
avoiding detection (see sidebar on next page for more on SWCs).

Strategic website compromises (SWCs) have proven to be
an effective tactic of state-affiliated threats to infiltrate
the networks of target organizations. In 2012, SWCs made
their debut with the “VOHO Affair”24 and continued in 2013
with attacks focused against the Public, Manufacturing,
Professional, and Technical sectors.
SWCs leverage websites that are of critical or
complementary value to an industry’s line of business to
distribute malware traditionally contained in spear phishing
emails. Visitors are hit with a drive-by download, granting
attackers access/ownership of the system. State-affiliated
SWCs in 2013 exhibited three new browser-based zero-day
vulnerabilities (constituting over 75% of publicly disclosed
SWCs), which upped the rate of compromise per event.
So, why has the use of SWCs in espionage campaigns
increased? Well, there’s no doubt that attackers have
realized this tactic scales well and provides reasonable
assurances of ambiguity. By opting out of direct attacks
like phishing, attackers effectively remove themselves
from the tribulations of poor grammar, scanners, and astute
users. And by leveraging zero-day exploits, they achieve
higher success rate that no longer rely on carefully coerced
actions.
In 2014, we’d like to predict SWCs will fade, but that
seems unlikely. While there are downsides to SWCs for the
attackers (high visibility and high cost to weaponize and
burn a zero day), the benefits of a low-cost way to support
long-term operations generally outweigh the risks.

Examining discovery timelines and methods for espionage
incidents reveals ample room for improvement. While this
information is often not known or provided (for various reasons,
including the visibility and focus of our contributors), there’s
enough to discern the general state of affairs. It typically takes
victims months or more to learn they’ve been breached and it’s
usually an outside party notifying them.

The most common method of discovery is ad hoc notification
from threat intelligence and research organizations that
observe, for instance, the victim communicating with C2
infrastructure of a known threat group. While this isn’t good news
per se, it does suggest intelligence operations are an important
tool for combating espionage.

Once the phishing email or SWC has done its work, and an
internal system is infected, the name of the game is moving
determinedly through the network to obtain the prize. This may
happen quickly, but it also may last for years. Common methods
involving loading backdoors on systems to maintain access,
dropping spyware/keyloggers and password dumpers to steal
user credentials, and then using those credentials to elevate
privileges and expand control.

Isolating the root cause of an espionage-related breach is a bit of
a snipe hunt. Sure, victims make mistakes (minor and otherwise)
that are exploited in the process, but the real root issue is a
determined, skillful, patient, and well-resourced adversary who
will keep poking until he finds (or makes) a hole. With that in
mind, let’s take a closer look at the holes and other thin spots
these adversaries often take advantage of.

First, we’ll start with a few blocking and tackling fundamentals
that you really ought to be doing regardless of whether or not
you’re worried about espionage. If you don’t do these, all those
super-advanced cybertastic APT kryptonite solutions may well
be moot.

Patch ALL THE THINGS!

Exploiting browser, OS, and other third-party software (e.g.,
Flash and Java) vulnerabilities to infect end-user systems is a
common initial step for attackers. Keeping everything up to date
will make that step a lot harder to take.

Use and update anti-virus (AV).

While many proclaim AV is dead, not having it is akin to living
without an immune system. It might not protect you from the
dreaded zero day, but let’s be honest — many espionage victims
still fall to one-zero-zero days (or higher). An up to date AV (in-line
and on the endpoint) can go a long way to detect anomalies in
applications and find pesky shells and other malware.

Train users.

Some will consider this a lost cause, but we counter with a
reminder that, over the years we’ve done this research, users
have discovered more breaches than any other internal process
or technology. It’s not all about prevention; arm them with the
knowledge and skills they need to recognize and report potential
incidents quickly.

Segment your network.

Good network and role segmentation will do wonders for
containing an incident, especially where actors intend to leverage
access to one desktop as a stepping-stone to the entire network.

Keep good logs.

Break the delivery-exploitation-installation chain.
Users will be phished, and they will eventually click; we’ve got
the data to prove it. Focus on implementing a solution that more
completely defends against phishing, such as not relying solely
on spam detection and blocklists, but also doing header analysis,
pattern matching based on past detected samples, and sandbox
analysis of attachments or links included.
For more mature organizations, check out the growing collection
of Data Execution Prevention (DEP) and Endpoint Threat
Detection and Response (ETDR) solutions. We don’t promote
specific products in this report, but you’ll find some good
options in this space by starting your search with some of our
contributors.

Spot C2 and data exfiltration.
Collect and/or buy threat indicator feeds. In and of themselves,
they aren’t intelligence, but they’re certainly useful within
intelligence and monitoring operations.
Monitor and filter outbound traffic for suspicious connections
and potential exfiltration of data to remote hosts. In order to
recognize “abnormal,” you’ll need to establish a good baseline of
what “normal” looks like. Those indicators you collected/bought
will come in handy here.
Monitor your DNS connection, among the single best sources
of data within your organization. Compare these to your threat
intelligence, and mine this data often.

Stop lateral movement inside
the network.
After gaining access, attackers will begin compromising systems
across your network. ETDR, mentioned above, can help here too.
Two-factor authentication will help contain the widespread and
unchallenged re-use of user accounts.
We mentioned network segmentation in the basics, but since
doing it well is challenging, we’ll mention it here again. Don’t make
it a straight shot from patient zero to a full-fledged plague.
Watch for user behavior anomalies stemming from compromised
accounts.

Log system, network, and application activity. This will not only
lay a necessary foundation for incident response, but many
proactive countermeasures will benefit from it as well.

Beyond the basics, there are some specific practices that
organizations concerned with state-affiliated and other
determined adversaries should consider. These roughly follow
critical points in the path of attack, where victims have the best
chance to recognize and respond.

The headliner for DDoS in our 2013 dataset was the QCF campaign against the financial industry,
which compromised vulnerable CMSs to create high-bandwidth attacks from hosting centers.
DNS reflection attacks also became “big” but sightings of the DoS equivalent of Bigfoot (DDoS
distractors covering up other nefarious activities) remain rare.

A new trend started developing in September of 2012. In the
past, DOS attacks were primarily generated from compromised
home computers or by willing participants. Think “your parents’
desktop” system – you know, the one you’re always cleaning up

when you visit during the holidays. Obviously, such systems,
reserved largely for normal home Internet users, have relatively
small bandwidth from DSL or cable modems. The attackers then,
harnessing their botnet of DoS drones, could send commands to
direct attacks at a specific target. Flash forward to September
2012 and you see a different scenario altogether, along with a
different method for building a better botnet. In this situation,
attackers scanned for and exploited vulnerable websites and
CMSs. Then they placed specific DOS attacks scripts onto these
sites. The primary script used by these attackers is a customized
version of a Russian kit known as Brobot or itsoknoproblembro.
So what’s different? Well, for starters these botnet drones aren’t
sitting on the Internet pipes of home broadband users.

They’re in hosted/cloud/private cloud/etc. data centers with
high bandwidth pipes. The servers are also optimized for heavy
traffic. Put these two together and you have the makings of what
gamers might call a DoS BFG. Or, if music is your game — “these
go to 11.” High packet, high bandwidth attacks. In some of the
Brobot attacks in the last year we saw upwards of 97 Gbps/100
Mpps attacks. These were some of the largest attacks we (and
likely anyone else) have ever seen.

So who exactly was behind this new wave of DoS attacks? The
simple answer is the Izz ad-Din al-Qassam Cyber Fighters (or
QCF for short). The group first popped up in September 2012
with the stated goal of using DoS attacks to wreak havoc on U.S.
financial institutions – part of a campaign they dubbed Operation
Ababil. And they did just that; for several weeks near the end
of 2012 and well into the first half of 2013, the QCF launched
wave after wave of DoS attacks against U.S. banks using their
powerful Brobot ion cannons (think Hoth).

All of this begs the question: Why was the QCF so determined
to wage a campaign against prominent U.S. financials? If you
believe the propaganda the group regularly posted to Pastebin
during their attacks, then the answer to the question of what
motivated them is simple: ideology. Like a broken record, the
QCF repeatedly stated they would attack U.S. banks until all
forms of a highly controversial and disparaging video named
“Innocence of Muslims” was removed from YouTube. They even
created a mathematical formula to convert the number of “likes”
the video had to how long the campaign would continue.


While this was the show the QCF put on for public consumption,
there were theories circulating in the security community that
Operation Ababil was nothing more than a front for state affiliated attackers based in Iran.
Are they truly hacktivists looking to get YouTube videos taken
down or are they state-affiliated threat actors probing for
weaknesses in the U.S. financial infrastructure at the bidding of
the Iranian government? Unfortunately, the multilayer command
and control infrastructure utilized in botnet creation makes
it incredibly difficult to say with certainty from open sources
that Iran is indeed the wizard behind the green curtain, so we
ultimately decided to go with the publicly stated purpose of the
actors and chalk it up to hacktivism.

While it’s true that exactly what motivated the QCF isn’t entirely
certain, the tactics used to carry out the group’s attacks are well
known. Not only did the group use more traditional attacks such
as UDP and SYN floods to clog up a target website’s bandwidth
and tie up server resources, it also carried out application-layer
DoS attacks. In these low and slow attacks the QCF would send
multiple HTTPS GET requests for PDF files on the target site.
These types of attacks are especially frustrating: they don’t
require significant resources, they can be difficult to defend
against, and they can be incredibly effective. The use of HTTPS is
particularly problematic for mitigation because the packets are
encrypted, which makes it difficult for defenders to determine
junk traffic from legitimate traffic.

Speaking of DoS attacks that don’t require a vast botnet to be
devastating, we’ve observed another trend stealing the limelight
recently: DNS reflection attacks. Not as clumsy or random as
a botnet; these are an elegant weapon for a more civilized age.
Remember the biggest DoS attack in history?27 If not, allow us to
refresh your memory. In March 2013, the anti-spam organization
Spamhaus was the target of a massive and sustained DoS target
that some security vendors claim spiked at nearly 300 Gbps
of traffic. The key word here is spiked (and we can’t emphasize
that enough); the average amount of traffic hitting Spamhaus
during the attack ranged anywhere from 85-120 Gbps, which
still represents a sizable bombardment. The method behind
generating an attack this large is DNS reflection.
So how does it work? Typically, an attacker sends a bunch of
DNS queries to open DNS resolvers. The attacker forges the
source address on his requests to make it look as though they
originated from his desired target. The open resolvers then send
their typically larger responses to the targeted address, which
is quickly swamped with seemingly legitimate traffic. Hence,
“reflection.” Much like the low and slow attacks described above,
DNS reflection doesn’t require significant computing resources
on the part of the attacker to produce devastating results.

If there’s one thing we’ve learned from the attack on
Spamhaus and others like it, it’s the importance of
understanding the numbers behind DoS attacks. Let’s look
at an example. Say there was a 200 Gbps attack at 25 Mpps,
200 Gbps = 2 x 10 or so bps, divide that by 25,000,000
and that is about 8,500 bits per packet or just over 1k bytes
per packet on average. This indicates many of the packets
are pushing towards the maximum packet size most ISPs
will route. We’ve seen attacks with a higher packet rate, but
never anything close to that in bandwidth. Both attackers
and defenders tend to sensationalize attacks like this. Both
have motives for inflating them. Attackers want to call
attention to their attacks and defenders will say, “Look,
it was so large there was no way we could keep that site
up and running.” Or if it’s a vendor, “Look how powerful our
service is. We can stop all the attacks!”
That being said, data compiled by our DoS defense team
shows an increase in the average size of attacks over the
past three years — as shown in figure 65. In 2011, the
average attack involved 4 Gbps of bandwidth with a
411 Kpps packet rate. Move forward to 2012 and those
averages jumped to 6 Gbps at 2 Mpps. It shouldn’t
surprise you to learn that in 2013 the average DoS attack
clocked in at 10 Gbps at close to 8 Mpps. While the QCF
and its powerful arsenal likely shoulder some of the blame
for this year-over-year increase, the increasing popularity
of reflection attacks and the power they generate are the
primary culprits.

Now that we’ve talked about the problem at length it’s a good
time to discuss what can be done to lessen or even prevent DoS
attacks against your organization.

Let’s start with the basics.
Servers/services should always be turned off when not in use,
patched when in use, and available only to the people who need
them, especially in the case of reflection attacks.

Segregate key IP/servers from non-essential IP space. Any IP
space not in active use for key servers should be announced
out of a separate circuit, perhaps even purchase a small backup
circuit and announce IP space. That way if it’s attacked, the
attack won’t compromise your primary facilities/servers.

Don’t be shy about using your provider’s anti-DDoS service. You
should be able to test it quarterly without charge. Make sure that
your key operations teams will react in a timely manner if there
is an actual attack. Even if your provider offers “auto-mitigation,”
this shouldn’t be an install-and-forget kind of service.

We’ve heard many clients and colleagues express concern
about attackers using DoS attacks as a “smokescreen” to
hide fraudulent automated clearing house (ACH) transfers
and other illicit activity. Although there are scattered reports
of this happening, hard evidence we’ve managed to collect
doesn’t indicate the rate or impact justifies the level of angst.
We sometimes jokingly refer to this as the “DoS Bigfoot,” not
because we don’t think it’s real, but because we’re intrigued and
want to capture it on film. Data collection for the 2015 DBIR is
already underway, and we invite any with shaky night vision film
clips of this thing to set the record straight.

Aside from the rise in DNS reflection attacks and converting
webservers into high-powered DoS bots, not much has changed
over the past few years. Sure, there are always new DoS toolkits
making their way into the underground and new waves of attacks
taking place, but the general principles remain the same, as do
the targets. We saw many attacks directed at the financial, retail,
professional services, and public sectors. What better way to
inflict pain on a bank or retailer than to go after its website –
something critical to customer service? And though carrying out
a DoS attack isn’t as difficult as it seems, results may vary. For
the financially challenged attacker it’s possible to download open
source tools such as Low Orbit Ion Cannon (LOIC), but he’ll need A
LOT of friends to do the same if the attack has a chance of being
successful. On the other hand, if he’s got some cash to burn, the
attacker can rent out a DirtJumper or Athena botnet and pummel
the target of his choice for less than $10 an hour. The more
enterprising (and development-minded) individual might even
go so far as to write his or her own DoS script and herd a botnet
together. And trust us, these three scenarios play out every day
in the cybercriminal underground.

Do the math.

Know that most attacks are about the FUD numbers cited by the
news media. They’re above your SSL server capacity, or perhaps
a few times your ingress circuit line rate. But attackers don’t
have infinite resources either – the biggest attack will be just
over what you can manage.

Understand that all ISPs will have to, at some point, protect their
general network over your company’s specific traffic. Ask your
anti-DDoS provider about its upstream peering capacity – if
they can’t get the (good and bad) traffic in no matter how much
mitigation capacity they have, your good traffic will be dropped
at the outside edge of their ISP’s network and your call queues
will light up with unhappy customers.

This last “pattern” isn’t really a pattern at all. Instead, it covers all incidents that don’t fit within
the orderly confines of the other patterns. Given that, one might assume you’d never find a more
wretched hive of scum and villainy than this random assortment of outcasts. But what we actually
find looks nothing like the riffraff of a Mos Eisley cantina; it’s almost entirely dominated by two
related species of incidents.

“Why not make two more patterns, then?” you might well ask.
Great question; allow us to explain.

First, let’s describe what we see here among these 7,269
incidents. Actors are 99.9% external. Generic “hacking” (variety
unknown), phishing, and browser-busting malware lead known
threat actions, with everything else below the 1% line. Threequarters of all incidents involved compromised web servers; the
rest were unknown.

“Then why do you say they’re related species?” you might counter.
A little digging into the data uncovers the fact that these
incidents actually represent mass attacks reported to a CSIRT. In
one, thousands of servers in hosting facilities were compromised
and used to host phishing sites. The other involved hundreds of
servers hijacked to host malware for drive-by exploits. Nothing
else was reported about the method of compromise or the
phishing/malware campaigns themselves.

We also looked at the success rate of different tactics in
phishing. Are users more likely to visit a link than run an
attachment? Are they more likely to click an attachment than
enter their passwords in a web form? In general, it appears
that about 8% of users will click an attachment and about 8%
will fill in a web form. And while most users are skeptical about
clicking an attachment (though not skeptical enough) they
are less fearful of visiting a link in an email. 18% of users will
visit a link in a phishing email. Users unfamiliar with drive-by
malware might think that simply visiting a link won’t result in a
compromise.

It’s fascinating to study what goes wrong. But the real purpose of
this research is to help you reduce the risk that these bad things
will happen to you. At the end of the day, we do this work to support
evidence-based risk management. We think the perspective of
studying clustered incident patterns enables more tailored strategies
to reduce risk, and toward that end, we did two specific things this
year. First, we mapped industries to attack patterns to help answer
the question, “For my industry, which threats am I most likely to
face?” Second, for each pattern we made specific recommendations,
including priority controls from the Critical Security Controls (CSCs)
based on our collaboration with the Council on Cybersecurity. This
included mapping patterns to controls.
To wrap up, let’s connect the dots in one more way. Since we’ve
used the data to map industries to incident patterns and patterns
to controls, we figured we also have a decent foundation to map
industries directly to recommendations for controls. Figure 69 on
the next page shows which controls we think are key to the threat
patterns each industry faces. And it weights each control by how
often we see the patterns the control addresses in that industry.
Essentially, we just did a whole bunch of multiplication to save you the
trouble.

The Chain of Compromise is a user-centric model that illustrates
how cyber attacks combine different
techniques and resources to compromise
devices and networks. It is defined by
4 main phases: Inception, Intrusion,
Infection, and Invasion.

The Dukes are a well-resourced, highly
dedicated and organized
cyberespionage group believed to be
working for the Russian Federation since
at least 2008 to collect intelligence in
support of foreign and security policy
decision-making.

Europe was particularly affected by
the Angler exploit kit. Users across
the region also frequently reported
Trojan:JS/Redirector detections, and
document files with embedded macros
that download ransomware.

Conflict used to be about borders. A long time ago, we would defend ourselves
by living in cities surrounded by walls. Those walls kept the enemies away. Over
time, the walls around cities became higher, longer and wider. The longer and
wider these walls, the more invisible they became, marking areas of wealth,
prosperity, power and belief systems. Eventually, those walls became borders.
And for hundreds of years, conflict was about borders. Conflicts were about
conquering land or converting people from one belief system to another.
Conflict and war have always been fueled by technology. Technology like
gunpowder, steel blades, and fighter jets. The staggering possibilities of
technology always seem to shine the strongest during periods of war. War has
been a real driver of technology. And technology has driven war.
One of the side effects of the cold war was that the Internet was created. The
US military created a way to uphold a chain of command during nuclear war. So
the Internet was created as a piece of military infrastructure. By developing the
Internet, mankind opened up a whole new way of waging war on one another.
And the Internet has no geography. It has no borders. By creating the Internet,
mankind opened up a Pandora’s Box where tangible borders and recognizable
enemies ceased to exist.
In addition, conflict used to be symmetric. Armies would fight other armies.
But now, the technology of war has moved on. We no longer know, or can
clearly describe who the enemy is, what they want to achieve, or what their
motives are. We go into battle using technologies we don’t fully understand,
against enemies that remain in the shadows, and into wars that we will never
know if they are over or not. Who is the enemy? Hackers? Anonymous? The
Russian Mafia? North Korea?
It’s a complex world of online conflict. And the only thing we can really be sure
of is that we’ve seen the beginning of the next arms race: the cyber arms race.

Malware exploits have been a commodity for more than a decade. So much so that during
2006, the day following Microsoft’s monthly “Patch Tuesday” began to be jokingly referred
to by InfoSec analysts as “Exploit Wednesday”. Quick turnaround was the key to success.
On Tuesday, Microsoft released its updates which were then quickly reverse engineered in order to
discover the underlying vulnerability. And then, once the vulnerability was known, an exploit was
crafted for use in malware attacks, which aimed to hit those who had not yet updated.
In late 2006, malware became further commoditized with the advent of malware kits. Early kits
such as MPack were victims of their own success, unable to scale rapidly to meet the ever-growing
demand. But such growing pains were soon enough overcome by malware services and today there
are numerous exploit kits available via underground markets.
Exploit Wednesday is no longer a thing. Microsoft’s software[1] is far more secure than it was
10 years ago and its patches roll out much more quickly. Exploit kits moved on from Microsoft
to Adobe. Reader was the biggest target for a time (also Flash). But browsers began to offer native
PDF support and Reader became unnecessary for most. Adobe adopted strong update cycles and its
software moved, for a time, out of harm’s way. Then Java’s browser plugin became the favorite target
— the weakest of the herd. Browser developers more or less forced it into a very restricted place.
And so at the moment… Adobe’s Flash is the last “best” plugin still standing for exploit kits
to target. But for how long?
On April 29, 2010, Steve Jobs published an open letter called “Thoughts on Flash” explaining
why Apple would not allow Flash on iOS devices. Many technology analysts point to this as
the beginning of the end for Flash Player, at least on mobile devices. This proved to be true.
On June 28, 2012, Adobe announced there would be no certified implementations of Flash
Player for Android 4.1 and it would limit installations via Google Play on August 15th 2012.
Flash has since hung on to its desktop market, but everywhere you look, it’s being deprecated.
In August 2015, Amazon announced that “Beginning September 1, 2015, Amazon no longer
accepts Flash ads.” Google followed Amazon’s lead in February 2016. Its ad networks, AdWords
and DoubleClick, will no longer accept Flash-based display ads starting from June 30th, 2016.
They’ll disable Flash-based ads on January 2nd, 2017.
It’s at this point that I’ll make the following prediction for early 2017 — once it no longer needs
to support Flash-based ads — the Google Chrome browser will start aggressively forcing users to
whitelist sites that require any sort of Flash. Mozilla’s Firefox and Microsoft Edge will do the same, and
by spring of 2017… Flash will be effectively decapitated as far as exploit kits are concerned.
Exploit kits face a disruptive future without much new fruit in sight. Commoditized
malware services will turn even further toward the use of malware attachments such as the
macro-based malware that is currently trending.
If only we could keep people from clicking “okay” to make the box go away.

Ironically, a great deal of Android malware is pushed at people via deceptive ads claiming that a Flash update is required. Even when there is no Flash,
its legacy provides a social engineering vulnerability. Google’s search engineers are beginning to configure Chrome to warn about sites that display such ads.

While they don’t impact the majority of consumers,
Advanced Persistent Threats (APTs) are of particular interest
to governments and major corporations. In 2015, F-Secure
Labs published a whitepaper detailing the various toolsets
used by the Dukes, a well-resourced, highly dedicated and
well-organized cyber espionage group that we believe has
been working for the Russian Federation since at least 2008 to
collect intelligence in support of foreign and security policy
decision-making.

While the aging Downadup (also known as Conficker) worm’s
position as the perennial top detection has helped keep worms
in general more prominent in the threat landscape than they
would be otherwise, the appearance of several new families
that have successfully spread through networks in certain parts
of the world has made worms as a whole a more noticeable
presence overall.

The most prominent of these new families is Njw0rm — a VBS
worm that spreads through removable drives, malicious email
attachments and drive-by downloads. The worm has backdoor
capabilities and is essentially designed to steal information from
victims. Njw0rm was detected far more often in the latter half
of the year than the former, but it was more than enough to
make it the most notable new malware family in last year’s threat
landscape.

Dorkbot is another new worm that made a noticeable impact.
It shared many characteristics with Njw0rm. Both spread
using removable drives. Both use backdoors, are capable of
stealing information from their victims, and communicate
with remote servers in order to receive additional instructions
from attackers. However, Dorkbot is also able to spread itself
by posting malicious links in instant messages and social media
sites.

Ippedo, a third new worm that was reported frequently enough
to be considered one of 2015’s top threats, is another infostealer
that is distributed on removable drives. However, it was not
observed to be capable of spreading via other means, making it
considerably less prevalent than other families.

As a broad threat classification, worms gained notable traction
last year.

However, exploits and exploit kits continue to be trending
threats facing people and companies in Europe and North
America. Not only are they frequently detected, but 2015 saw
indicators that they continue to expand their capabilities and
work across a variety of attack vectors. 2015 saw a decrease
in police-themed ransomware, but also more activity from a
diverse number of crypto-ransomware families distributed through both exploit kits and macro malware.

The threat landscape in 2015 had similarities with trends that
were observed in 2014, but there were also some significant
differences. One surprise was the resurgence of macro
malware — something that hasn’t been seen since the early
2000s. Worms also accounted for a greater percentage of
overall malware detections, which is attributable largely to
the appearance of several new families in certain parts of the
globe.

However, with the exception of the
now infamous Downadup worm (which was prominent all over
the globe), detections of many of these worm families came
from countries in Asia, the Middle East, and to a lesser extent,
South America. Despite this increase, trojans (such as Gamarue
and Kilim) remained the predominant type of malware users
encountered in 2015.

Exploits were a notable threat in 2015, and were observed to be
active in many different countries. The Angler exploit kit was
particularly noticeable in the detection reports from different
areas around the globe, and was the most prevalent threat in
the United States, the United Kingdom, Sweden and Australia.
The Angler exploit kit demonstrated the most comprehensive
arsenal of exploits last year, but part of its success (and the
success of the exploit kit business in general) appeared to be
an increasingly efficient use of different attack vectors. This is
signified by the prominence of the generic Trojan:JS/Redirector
detection reports. These trojans are insinuated onto legitimate
websites by attackers to redirect website visitors to sites hosting
exploit kits, including Angler and Nuclear. They were prominent
enough last year to earn the dubious distinction of being the top
threat in Switzerland and Denmark.

Flash vulnerabilities greatly contributed to the success of
exploits, even when not used as part of an exploit kit. Exploits
identified by the generic Exploit:SWF/Salama detection were a
noticeable part of the threat landscape, particularly in Europe
and the US. While not quite as significant as Angler, both
capitalized on the seemingly endless supply of users running
versions of Flash containing security vulnerabilities.
Overall, the threat posed by exploits did not evolve much from
previous years. They still capitalize on people and companies
running outdated or unpatched software (for example, the WormLink exploits require users to open a document file that
contains code to exploit an unpatched vulnerability). They still
accounted for eight percent of overall malware detections,
just as they did in 2014. However, the malicious payloads that
exploits deliver, such as ransomware, have diversified and
become more severe, making it more important than ever to
ensure people update their software when new security patches
become available.

One interesting development in 2015’s threat landscape was the
resurgence of macro malware. Macro malware — documents
containing hidden malicious code — was a major threat in the
late 1990s to early 2000s. But when Microsoft released Office
2003, the default security settings were amended to stop
macros from automatically running when a document opened,
greatly stymieing attackers looking to spread malware with this
method.
However, beginning in June 2015, macro malware became a
notable presence in the telemetry reports again. While it was by
no means as prevalent as other threats, macro malware made an
impact in several European countries. They are typically spread
via malicious documents attached to emails, and utilize social
engineering techniques to manipulate users into opening the
documents and enabling the macros, allowing the malicious
code to run.

In this instance, the attackers took advantage of a situation
unique to China, wherein developers having difficulties
accessing Apple’s download servers outside the country
resorted to obtaining copies of the tool from local sources. This
laid the foundation for the tainted apps to successfully make
their way into the App Store.

2015 saw the biggest attack yet on Apple’s ecosystem. In the
XcodeGhost incident, tainted copies of the company’s Xcode
app development tool being shared on public download
forums in China allowed attackers to infiltrate the highly secure
AppStore. By compromising the copies of the legitimate tool
that developers used to create their apps, the attackers were
able to insert malicious code into the apps which were eventually
uploaded to the App Store.

Backdoors were the dominant type of malware detected on
Apple’s operating systems in 2014 — an interesting contrast to
the consistent dominance of trojans for Windows and Android.

While these toolsets are highly targeted and are unlikely to ever
be seen by most consumers, they are of much more interest
and direct relevance to individuals who are associated with state
bodies or corporations that are thought to be of interest to the
Dukes.

The Android ecosystem saw Slocker rise to become a more
prominent threat in 2015. While premium SMS-sending trojans
remained significant — particularly in France — the growing
popularity of Slocker signals a shift in mobile malware toward
targeting content users store on their devices. Backdoors such
as CoudW also indicate an increasing shift of compromise types
on the operating system compared to previous years.

The toolsets — CozyDuke, MiniDuke, HammerDuke and
SeaDuke to name just a few — were all built with varying
functionalities, such as password stealing, opening backdoor
access on an affected system, and carrying out Distributed
Denial of Service (DDoS) attacks.

This type of macro malware is similar to exploits in that the
intent is to compromise users in a way that allows attackers
to drop malicious payloads. In 2015, these payloads included
severe threats such as the Dridex banking trojan, and cryptoransomware such as Cryptowall.

APTs are sophisticated programs that are usually customdesigned to stealthily infiltrate and lurk in the computer systems
and networks of targeted organizations, making them a silent
menace to companies that deal in sensitive trade or production
information. In 2015, a whitepaper from F-Secure Labs exposed a
group behind such a threat - the Dukes cyber espionage group.
The Dukes are responsible for a family of toolsets that have been
in use since 2008, and developed continuously over the past
seven years.

Above is a representation of the volume of detection
reports we receive from a particular country versus the
user population in that country (the number of people
using our security products).

When one malicious program shares distinctive code
or behavioral features with another, they are usually
considered to belong to the same family. Individual
threats in a malware family are often caught by security
software using a detection that identifies the family’s
unique characteristics.

There were notable upticks in the prevalence of these top malware families throughout 2015. Detections
of new worm families increased dramatically in the latter half of 2015. The changing prevalence levels
can be due to various reasons: for example, attackers more actively distributing malware by launching
new phishing campaigns, or a change in the logic for a particular family’s detection.

Some malware remains persistent in the wild for years
on end. These legacy families can persist for a number of
reasons: for example, new users may become infected for
the first time, or the attackers alter the existing malware
to re-attack the same targets.

Some malware isn’t identified by family detections, but
is instead found by a generic detection that looks for
broadly similar characteristics.

Availability of the source code
for various backdoors could
be a factor that contributes to
backdoors making up a huge
portion of OS X threats.

73% of the total trojan samples
belong to the Flashback
family, a group of malware that
connects to a remote site to
download additional malicious
files.

Europe was particularly
affected by the Angler exploit kit. Users across the region also
frequently reported Trojan:JS/Redirector detections, and document
files with embedded macros that download ransomware. Some
European countries reported notable levels of particular threats.

The Angler and Nuclear exploit
kits, as well as Trojan:JS/
Redirector, were the most
frequently reported threats.

Most attacks today are directly targeted at the user.
One popular method attackers use to get their malicious
programs onto a user’s device is by tricking the user into
unwittingly downloading and installing the malware
themselves (a tactic known as social engineering). Another
popular, but technically difficult, method used by attackers
is to exploit flaws or loopholes to quietly slip the malware
into the device. Usually these weak points are in the app
or the device itself. More rarely, they are in programs,
processes or systems that aren't directly related to the user.

Investigations by researchers from, among other
organizations, Weibo, Alibaba and Palo Alto Networks, traced
the 'contamination' of these apps to the use of a compromised
version of Apple’s Xcode software creation tool. Xcode is used
to compile apps for the iOS and OS X platforms and is provided
for free from the company’s own servers. As with many other
enterprise or commercial programs however, copies of the
Xcode program are also unofficially available on file-sharing
services, where they can be downloaded by developers who,
for one reason or another, are unable to use the official source.

Sometimes however, attacks don’t take the direct path.
In late 2015, the Apple App Store became the target of a
notable string of incidents that underlines the possible
ramifications when attackers change tactics and target the
app developers. In these incidents, the developers used
compromised tools to unwittingly create apps with secretly
malicious behavior. The apps were then able to bypass
Apple’s app code review procedures to gain entry into the
store, and from there, onto an ordinary user’s iOS device.

According to news reports, due to the idiosyncracies of
Internet connectivity in mainland China, developers there
face significantly slower connections to servers located
outside the country, particularly for large files (the latest
Xcode version is about 3.5GB in size). This difficulty led many
of the developers to use copies of the Xcode tool that had
been hosted on servers within the country. Unfortunately,
some of these copies included extra lines of code. When the
developers used the compromised Xcode program to create
apps, it also quietly inserted additional code, without the
developer's knowledge.

Apple’s software repository requires all submitted programs to
pass a rigorous vetting process before they can be offered in
the store, and has historically been admirably free of malicious
programs. In 2012, the first reported instance of malware was
found in the App Store[1] when the Find & Call app turned out
to be misusing contact information on the device to send
spam. In the following three years, there were only a couple
of relatively minor incidents, when apps that did not play by
Apple's strict rules were booted from the store.

The inserted code was designed to report details of the
affected device to a remote server. Researchers have however
been quick to note that they have found no evidence of actual
data theft or harm. Nevertheless, for users of affected apps,
the recommended action is to remove them from all devices
until an updated clean version is published by the developer,
and in the meantime change the login credentials for any
email and social media accounts associated with the apps.

This happy state of affairs was shattered in September 2015
when news broke that Apple had removed multiple apps from
the store because they were found to be embedded with a
malicious program, dubbed XCodeGhost. Initial news reports
said over 30 apps were affected, though subsequent reports
put the number at over 300.

What was particularly noteworthy about the affected apps
was that at least some of them were from well-known and
reputable software development companies. Perhaps the best
known was WeChat, a popular messaging program. But other
apps such as Railway 12306, Camcard, NetEase Cloud Music
and so on had millions, or even tens of millions of users. While
the majority of these users were located in mainland China,
many of the apps also had users in other regions around the
world, such as the United States and Europe.

Shortly after news of the XcodeGhost apps broke, security
researchers at PwC announced that they found cloned copies
of the Unity framework being distributed that had been
modified in a similar manner to the cloned Xcode programs,
leading them to name the altered clones UnityGhost [4],.
Unity is a commercial third-party development framework
that can be used to create iOS apps (as well as programs for
other platforms such as Android and Windows). Fortunately,
in this instance there were no reports of apps created with the
compromised framework copies being found in the App Store.

A month later, a broadly similar situation cropped up again,
when apps were booted out of the App Store for quietly
collecting user and device information [5]. In this case,
the affected apps had been created using the third-party

Youmi software development kit (SDK), which allowed the
developers to include ads in their app. Again, the developers
were unaware that the apps they had created were poisoned
with routines that would break Apple's strict security and
privacy guidelines. Though Apple didn’t specify how many
apps were removed, news reports mentioned “over 250 apps”
were affected. The China-based company that developed the
Youmi SDK also subsequently issued an apology [6] and said it
was “working with Apple to resolve the issue”.

Taken together, these incidents clearly demonstrate that it is
possible to circumvent the protective walls guarding the App
Store and reach iOS users by using a more indirect path and first
targeting the app developers. In each case, the development
tool was tinkered with so that when it was used in good faith by
the developer, it also silently introduced unwanted code into
the final product.
This was not the first instance of this kind of attack - for
example, in 2010 the Delphi program was targeted by a virus
that inserted code whenever an executable was created using
the affected program [7, 8]. It is, however, the first instance
where such an attack had this kind of public impact.

Most of today’s mobile device users have become wearily
familiar with the standard security advice: 'be wary of thirdparty app stores'. Clearly, app developers are no more
immune from the same pressures that drive ‘the average user’
into patronizing such sources - most commonly, limitations
on bandwidth, restrictions on accessing extra-national
websites, and the tempting accessibility offered by third-party
repositories. To at least partly address some of these factors,
Apple has announced that it planned to also offer the official
Xcode program on servers located in China, making it more
accessible to developers in the country [9].
Despite the recent misfortune, the Apple App Store remains
a tougher nut to crack than the Android ecosystem (where
directly attacking the users via social engineering remains
the easiest and most effective attack vector). However, it is
not impregnable. The incidents highlight the importance of
maintaining rigorous security along the entire length of the
app development chain. The domino effects that result from
such a successful compromise impact not only the users’
security, but also the reputation and trustworthiness of the
affected developers and the App Store itself.

The Dukes are a well-resourced, highly dedicated and
well-organized cyber espionage group that we believe
has been working for the Russian Federation since at
least 2008 to collect intelligence in support of foreign
and security policy decision-making.

The Chain of Compromise is a user-centric model that
illustrates how cyber attacks combine different techniques
and resources to compromise devices and networks. Such
models are a necessity given the evolution of the threat
landscape over the past decade. Gone are the days of hobbyist
hackers who write computer viruses out of mere curiosity.
Today’s threats are dynamic and sophisticated, and created
by criminals, saboteurs, hacktivists, and even nation states,
who all have different goals and objectives they use cyber
attacks to achieve. As the threats have evolved, so must the
understanding of security researchers, IT administrators,
and the general public. The Chain of Compromise highlights
the sophistication of today’s threats by showing attacks as
multi-phased events, where the completion of each phase
has unique effects that are often combined by attackers to
increase the potential damage done during an attack.
The Chain of Compromise is not the only model designed
to break down cyber attacks as dynamic, multi-phased
processes. Lockheed Martin’s Cyber Kill Chain and
Mandiant’s Exploitation Life Cycle, for example, are both
familiar to security researchers around the world. But whereas
those models are designed to explicate the attack process
utilized by APTs, the Chain of Compromise provides a usercentric model to help companies and individuals understand
threats in relation to their own systems. By understanding how
systems are compromised differently by different phases of an
attack, the hope is that IT administrators and other readers can
gain new insights into how to predict, prevent, and intercept
attacks before they escalate into costly data breaches or other
security incidents.
There are 4 main phases to the Chain of Compromise. While
attacks and toolsets can limit themselves to a single phase, this
is rarely the case in today’s threats. Modern attacks typically
connect different phases together, as this allows attackers
to accomplish much more with their efforts. And while it
is possible for attackers to abandon a particular attack or
campaign, it is often better for companies and individuals to
be prepared to defend against attacks with multiple phases
and components, which will prevent them from being an easy
target for attackers.

Because of this, the resources used in each phase are relative
to the way the user experiences the attack. The use of social
engineering exemplifies this dynamic: attackers can employ
such tactics during both the inception and intrusion phases.
Additionally, defenders should realize that becoming
compromised at one level does not mean that they are
suddenly “pwned”. This is particularly important for IT
administrators responsible for the security of networks.
Companies, even small ones, should have solutions in place
that can disrupt an attack at any point in the chain, as well as
a plan for limiting how attackers can move along this chain to
accomplish their goals.

Users may unknowingly come into contact with
threats during normal activities, such as web
browsing, emailing or using removable drives.
Attackers may also try to lure or force potential
targets into a position where they can be
compromised, either with technical means
(such as a redirectors or malware) and/or social
engineering (such as phishing campaigns).

Attackers might use special code (exploits) to take
advantage of vulnerabilities in the exposed systems.
They might also trick the user into unknowingly
granting the attackers access to the system (social
engineering).

Attackers can install (“drop”) a payload, which
typically runs malicious code or software to produce
unwanted effects. These payloads include malware
such as ransomware, bots, viruses, or trojans.
The attack persists on the user’s system, or
escalates to further compromise the user’s system
or exposed networks.

The main purpose of these threats is to put
the user into a position where they can be
compromised. Redirector trojans send users to
malicious sites, where they are often exposed to
exploit kits. Macro, Autrorun, shortcut icon file trojans
(and the Njw0rm worm) generally use the facade of an
innocuous file to get themselves saved on the user’s
system, where there’s a greater chance they will be
launched.

These threats (all exploits) are used by attackers to
gain direct access and/or control of a user’s system by
taking advantage of vulnerabilities in the system, or in
programs installed on it.

These threats are often dropped onto a system by
other malware, or are delivered once a system has been
compromised by an exploit kit.

Once present on an affected system, these threats
can spread copies of themselves to other machines
on the same network, compounding the effects of the
original infection. Some, such as Gamarue or Dorkbot,
also include the ability to contact a remote server
and retrieve additional instructions from an attacker,
potentially increasing the impact of an infection.

A VBS worm that spreads via removable drives, in files that are attached to email messages crafted to
target particular people or companies, and drive-by downloads when a user visits malicious web sites.
Once the malware is in the user’s system, it executes other files; steals usernames, passwords and the
details of the online portal that they are used for; updates or uninstalls itself, and contacts a remote
command and control (C&C) server for additional instructions. It also sends information about the
affected system such as IP addresses visited, operating system details, etc. to the attacker.

Njw0rm is distributed through infected
removable drives, email messages that have
been crafted to target particular people or
companies (spear-phishing) and malicious
websites.

Njw0rm intrudes into the system in two ways;
via drive-by downloads when visiting malicious
websites or by exploiting the user’s curiosity to
lure them into executing an email attachment or
file on an infected removable drive.

Once launched, Njw0rm is installed in several
key system folders. It also manipulates the
system registry to make sure it is executed every
time the system is rebooted.

Njw0rm opens backdoor access to the affected
system so that an attacker can potentially
compromise it in the future. It also functions
as a bot, so that the affected system can be
remotely controlled by the attacker. It is capable
of stealing online credentials, updating or
uninstalling itself, as well as sending information
about the affected system to the attacker.

CosmicDuke is one of the toolsets used by the Dukes — a cyber espionage group believed to be
operating with Russian state sponsorship. The Dukes have been
actively using at least nine different toolsets to steal information in intelligence gathering operations
since at least 2008. Their typical targets include governments, political organizations, and other
entities that possess information about the security and foreign policies of different countries.
The CosmicDuke toolset is designed around a main information stealer component. This information
stealer is augmented by a variety of components that the toolset operators may selectively include
with the main component. The components provide additional functionalities, such as multiple
methods of establishing persistence, and modules that attempt to exploit privilege escalation
vulnerabilities in order to execute CosmicDuke with higher privileges.

Users’ initial exposure to CosmicDuke is via
spear-phishing campaigns containing malicious
attachments.

CosmicDuke intrudes on systems by either
exploiting software vulnerabilities when a user
opens or views a malicious attachment,
or exploiting the user’s curiosity to execute
the attachment’s code.

CosmicDuke infects systems with an
information stealer capable of keylogging,
taking screenshots, exporting decryption keys,
and stealing credentials from browsers and
email or chat clients.

CosmicDuke uses command-and-control servers
to exfiltrate stolen data via HTTP, HTTPS, FTP,
or WebDav. This data includes login credentials
that attackers use to access the system remotely
without the use of additional malware or special
tools, allowing the compromise to persist well
beyond the initial infection.

Inception is the first phase in the Chain of Compromise. It involves users,
whether individuals or companies, exposing themselves to a particular threat
or threats. Businesses and individuals will often expose themselves to threats
unknowingly, which is something anticipated by today’s attackers. However, many
attackers will take a more active role, and employ technical means (such as malware),
social engineering (such as phishing campaigns) or both to manipulate potential
targets into a position where they can be compromised.

Redirectors wreak havoc on US, Europe
Trojan:JS/Redirector is a set of web-based attacks that allow attackers to redirect users from the
website they intend to visit toward a different, unsolicited website. While legitimate websites may
redirect visitors for a number of reasons, attackers have repurposed this tactic to manipulate traffic
toward malicious websites. Attackers typically conduct these attacks by compromising a legitimate
website with the intent to reach that website’s visitors.

Redirects are a popular and effective way for attackers to
initiate attacks on potential victims, and using them to steer
traffic toward exploit kits was a particularly common attack
strategy in 2015.
Trojan:JS/Redirector FE was the most active redirector
observed in 2015. It redirected users to servers hosting exploit
kits, including prominent threats such as Angler and Neutrino.
The majority of Trojan:JS/Redirector FE hits were detected in
Germany, followed by France, Sweden, and the United States.
Trojan:JS/Redirector FD (also known as BizCN) was another
active redirector in 2015. Initially it directed users toward
servers hosting FiestaEK, but later switched to NuclearEK
and NeutrinoEK. The redirector was detected far more
prominently in the United States than in other countries, but
was also a significant threat to users in the United Kingdom,
Finland, and Canada.
Trojan:SWF/Redirector EW (also known as EITest Flash
Redirector) was another redirector that was quite
pronounced in 2015, particularly in the spring and autumn.
Attackers seemed to use it as an initiation technique for
AnglerEK campaigns in the spring and autumn. And once
again, it was detected prominently in North American and
European countries, including Sweden, the United States, the
United Kingdom, and the Netherlands.
As mentioned above, many of the websites hosting these
redirectors are not malicious themselves. They are simply
legitimate websites that have been built in a way that leaves
them susceptible to attacks, making them victims rather than
perpetrators. For example, many of the websites discovered
to be hosting Trojan:SWF/Redirector EW were built using
WordPress. It’s entirely possible that the compromised
websites were targeted based on this, or possibly because
they were using a vulnerable plug-in.

However, there is also strong evidence suggesting the use of
other techniques, including the use of brute-force password
attacks to gain administrative rights to the targeted websites.
Once an attacker has administrative access to a website, it is
a trivial matter for them to upload malicious scripts such as
redirectors.
The prominence of Trojan:JS/Redirector detections in North
America and Europe is consistent with observations regarding
the prevalence of exploit kits in these regions. As such,
redirectors should be recognized as significant threats used
by attackers looking to initiate attacks against people and
companies in these regions.

Intrusion is the phase of the Chain of Compromise where the actual attack begins.
Companies and individuals who expose themselves to threats give attackers the
opportunity to break into exposed systems. Social engineering techniques can be
used to manipulate users into giving attackers access to systems. But perhaps the
most common Intrusion resource used by attackers is the exploit. Exploits allow the
attackers to utilize software vulnerabilities in exposed systems to acquire a degree of
access, or even control, over their targets.

Exploits are bits of code written to take advantage of
vulnerabilities found in computer software, and are a prominent
attack resource used in the modern threat landscape.
The success of an exploit is contingent upon it finding a
corresponding software vulnerability in users’ systems. These
exploits are often bundled together into groups by attackers
as part of an exploit kit, which scans the software on a user’s
device to find unpatched vulnerabilities and match it with an
appropriate exploit. After accomplishing this, the exploit kit
uses this vulnerability as a security hole to deliver malicious
payloads, such as ransomware.

those countries. Angler has demonstrated the capability to
add support for Flash vulnerabilities faster, and more often
than other prominent exploit kits. The 2015 hack of the Italian
surveillance software firm Hacking Team exemplifies how
efficiently exploit kits, particularly Angler, make use of new
vulnerabilities.

Exploit kits making use of Flash vulnerabilities were a prominent
threat in 2015, which is consistent with observations from the
previous year. Whereas in 2013 exploit kits targeting multiple
Java vulnerabilities were found, exploit kit authors have moved
on to targeting Flash.

The first vulnerability, CVE-2015-5119, was taken into use the
very same day by three different exploit kits (Angler, Neutrino,
and Nuclear). Two additional exploit kits added support for the
vulnerability within two days, despite Adobe releasing a patch
the day after the vulnerability was disclosed to the public [1].

AnglerEK was the most active and most efficient at integrating
exploits for Flash vulnerabilities, and was observed to be active
in a number of campaigns throughout last year. The authors
behind AnglerEK integrated support for Flash vulnerabilities
more often than other prominent exploit kits. AnglerEK
detections in 2015 were prevalent in many different countries,
and prominent enough in Sweden, the United States, the
United Kingdom and Australia to make it the top threat in

AnglerEK adopted the second zero-day vulnerability, CVE2015-5122, the day after it was discovered in Hacking Team’s
data [2]. NeutrinoEK added support for the vulnerability the
next day. Two days later, the authors behind NuclearEK and
RigEK added exploits for the vulnerability to their kits. All of
this occurred before Adobe was able to develop and release
a patch.

In the first week of July 2015, a considerable amount of Hacking
Team’s data was made publicly available. Two zero-day Flash
vulnerabilities were included in this data dump.

Flash vulnerabilities are attractive targets for exploit kit
developers because of Flash’s use on multiple platforms. The
speed and popularity of developing exploits targeting Flash
vulnerabilities contrasts with how exploit kit developers
adapt to vulnerabilities targeting other pieces of software.
For example, a Silverlight vulnerability (CVE-2015-1671) was
disclosed in May, but wasn’t integrated into Angler and
Magnitude exploit kits for two months. The top five exploit

Flash vulnerabilities are attractive targets
for exploit kit developers because of Flash’s
use on multiple platforms

kits had a similar reaction to an Internet Explorer vulnerability
disclosed in mid-July, waiting until August before adding
support for the vulnerability.
AnglerEK was detected prominently enough in 2015 to make it
one of the most commonly encountered forms of Intrusion by
both individuals and businesses.

Last April, the United States Computer Emergency Readiness
Team issued an alert regarding the use of unpatched software
by companies. According to the alert, as many as 85 percent
of targeted attacks are preventable, and companies should
be more diligent in patching and updating their software to
minimize their attack surface.
However, it appears that this warning has yet to have its
intended effect. According to research, over one in four
versions of WordPress currently in use within Finland contains
exploitable vulnerabilities. And historical data implies this is
getting worse. A similar investigation conducted in 2014 found
that 24 percent of WordPress versions were vulnerable, which
rose to 26 percent in 2015.

Software vulnerabilities are the lifeblood of the exploit
market, and the ongoing use of outdated/unpatched software
explains why this market is able to thrive. The use of software

with exploitable vulnerabilities helps create demand for the
very exploits that target them, thereby encouraging exploit
writers to create more supply for this demand. The fact that
the observed use of WordPress in Finland doubled between
2014 and 2015 should be seen as an indicator of the growing
potential market for WordPress exploits.


Infection is the phase of the Chain of Compromise that involves a malicious payload
executing code within the user’s system. Once a user’s system has been broken into,
attackers are free to install (“drop”) a malicious payload, which typically runs some
kind of malicious code to produce unwanted effects. These payloads can include
malware such as ransomware, bots, viruses, or trojans.

Ransomware was a popular payload for many of the most
prevalent exploit kits detected in 2015, and has become an
effective tool to extort money from both organizations and
individuals. Ransomware families are designed to extort
victims by locking them out of their devices and data until they
pay a fee to the attackers (hence the phrase “ransomware”).
Different families employ different approaches to locking
users out of their devices. However, they can all typically be
classified as either “police-themed ransomware” or “cryptoransomware.” Police-themed ransomware will prevent users
from accessing their devices and data by masquerading
as law enforcement officials, claiming that the user has
broken some type of law and needs to pay a fine to use the
infected device. Crypto-ransomware relies on encrypting
the contents found on a device, essentially preventing users
from accessing the device or contents until a ransom is paid
for the decryption keys.
While 2015 saw a decrease in detections of police-themed
ransomware, several families of crypto-ransomware rose in
popularity, essentially maintaining the overall threat posed
by ransomware.
The Browlock family of police-themed ransomware was not
as dominant in the detections as it was in 2014, although it
was still prominent enough in early 2015 to account for more
detections than any other family for the entire year. Several
crypto-ransomware variants, on the other hand, became
more active as the year progressed. Cryptowall saw enough
growth in 2015 to eclipse several police-themed ransomware
families that were prominent in 2014, making it the most
active crypto-ransomware family for the majority of the year.
Both the Crowti and Teslacrypt crypto-ransomware families
saw increasing amounts of activity in the final quarter of the
year, giving them a more noticeable presence in the threat
landscape. All in all, more crypto-ransomware families were
more active in 2015 than in the year before.

Malware infections generally cause the most concern for
companies and individuals, as it is what typically determines
how the attack impacts targets. After an attacker is able to
access an exposed system, they are able to drop a malicious
file or files to produce unwanted effects. Depending on the
specifics of the attack, these malware infections lead to
things like data breaches, loss of control over information
or critical infrastructure, degraded system performance,
and other security incidents or violations. Malicious
payloads used to infect systems in 2015 include things like
banking trojans (such as Dridex), bots (such as Ramnit or
Sality), infostealers (such as Fareit), and different families of
ransomware.

Ransom demands displayed by police-themed ransomware
Browlock (left) and crypto-ransomware CryptoWall (right)
Researchers have previously noted that crypto-ransomware
campaigns have support infrastructure in place to encourage
their targets to pay the ransom, and consistently give
victims decryption keys after receiving the payment.
Attackers behind police-themed ransomware campaigns,
on the other hand, tend to simply take the ransom without
helping the victims remove the infections. The fact that
the attackers behind crypto-ransomware families will often
help their victims to ensure payment, combined with the

relatively affordable payments requested, has lead the FBI to
recommend companies simply pay the ransom if infected.
In 2015, several police departments followed this advice and
paid online extortionists hundreds of dollars each to release
systems locked by ransomware. Stories like these highlight
both the effectiveness of using ransomware as an extortion
tool, and the importance of disrupting attacks before they
infect systems.

Invasion is the final phase in the Chain of Compromise. During this phase, the initial
infection will persist until the victim takes measures to disrupt the attack, or escalate
in order to further compromise the user’s system or network. While the specifics of
such escalation varies according to the particulars of the attack, there are two main
evolutionary paths today’s attacks typically follow: infiltration and infestation.
Infiltration allows attackers to penetrate further into the user’s system, which can be
used in planning future attacks or creating a persistent compromise that prolongs the
effects of the attack for an extended period of time.

Out of the observed DNS hijacks in 2015, the most common
strategy used by attackers was to direct Internet traffic to
malicious IP addresses that would then infect devices with
Kelihos malware. Kelihos is a prominent botnet that can be
used for sending spam and DDoS attacks. It can also steal
information from infected devices, including credentials.
Other prominent payloads delivered through these hijacks in
2015 included the Fareit, Pkybot, and Zbot trojans, as well as
the Aprox malware used in the botnet of the same name.
Like many modern cyber attacks, these DNS hijacks did not
limit themselves to a single phase in the Chain of Compromise
– altering DNS configurations was rather a means to an end.
Nearly half of all of these detections – 48 percent – were
used to establish botnets. Additionally, both Pkybot and
Fareit function as both information stealers and downloaders,
allowing the attackers to download additional payloads after
the initial infection.
Thus, an overwhelming 77 percent of these cases attempted to
give attackers persistent access to the users’ systems, allowing
them to further infiltrate systems to create new infections or
otherwise compromise devices on an ongoing basis. Based on
this, users should consider DNS hijacks as a potential ongoing
compromise of their devices and networks that can help
attackers achieve a variety of goals over an extended period
of time.

Although DNS hijacks are a frequent type of attack in today’s
threat landscape, a significant spike in these hijacks was
observed during the spring and summer months of 2015,
specifically April through August. These attacks changed the
default DNS configuration in order to manipulate Internet
traffic.

While DNS hijacks differ based on their target (for example,
a DNS hijack against a large corporation would look quite
different from the DNS hijack targeting an individual or micro
business), their basic aim is to alter the domain name system
(DNS) configurations of their targets in order to monitor or
manipulate Internet traffic. Various security flaws can lead
to these DNS hijacks, including weak passwords, software
vulnerabilities, or malware. DNS hijacks are an effective way
for attackers to make contact with a large number of potential
targets at once, as it provides them with the opportunity to
compromise all of the devices connected to a particular
network.

Invasion is the final phase in the Chain of Compromise. During this phase, the initial
infection will persist until the victim takes measures to disrupt the attack, or escalate
in order to further compromise the user’s system or network. While the specifics
of such escalation varies according to the particulars of the attack, there are two
main evolutionary paths today’s attacks typically follow: infiltration and infestation.
Infestation occurs when an attack successfully propagates beyond a single device or
system to compromise a larger network.

Could Downadup find new life through the Internet of Things?
In 2015, Downadup retained its previous position as the most
frequently detected type of malware. The computer worm
was first discovered in 2008, and is now recognized as one
of today’s most widepread malware infections. Downadup is
a computer worm that infects unpatched Windows machines
(including various versions of Windows Server), and then
invades exposed networks attached to infected devices.
Downadup has infected millions of computers since its release
in 2008, and caused disruptions on an industrial scale as
organizations attempted to combat the worm [1]. At one point,
Microsoft was offering a USD 250,000 reward for information
regarding the worm’s authors [2]. Downadup’s combination
of different tactics gave it a sophistication beyond other
computer worms known to researchers at the time, making it
one of history’s most invasive families of malware.
Downadup remains a prominent malware family to this
day, and is the most frequently detected threat in Finland,
France, Germany, India, Italy, and Norway. While many antivirus products can detect and remove the infection for
individual consumers, it is still quite challenging to purge the
worm once it infects large networks, such as those run by
telecommunication companies or global enterprises.


And in spite of its age, Downadup is finding new ways to
propagate itself. Downadup infections were discovered
on wearable cameras manufactured for police officers in
November 2015 [3]. Because devices like wearable cameras and
many other Internet of Things (IoT) devices are unable to run
traditional anti-virus software, it is entirely possible that threats
such as Downadup could see a resurgence if non-secure IoT
devices proliferate.

The malware commoditization market continues to strengthen, with malware-as-a-service ventures becoming increasingly sophisticated and organized.
Customized end-to-end malware campaigns can be easily purchased online.
Once launched, botnets distribute spam which leads to infected machines.
Delivered payloads report back to a managed backend infrastructure that is
provided as part of the service. We’ve even observed beta tests happening
before a real campaign begins. Although predominantly being used for
financial gain, these malware-as-a-service campaigns are sometimes used
for other nefarious purposes, such as data theft or public embarassment.
Compared to the cost of damages that an organization or individual can incur,
the price of a malware campaign is incredibly cheap.
We’re continuing to see an upsurge in ransomware, and that trend is likely
to continue through 2016. Not only are ransomware campaigns becoming
increasingly organized and sophisticated, the malware itself is becoming
more insidious. Crypto-ransomware represents some of the most destructive
software we’ve seen in recent times. Files are encrypted wholesale on the
end-user’s machine, and the only recourse a victim has is to pay the criminals
for the decryption key. More recently, this malware has been able to encrypt
files on non-mapped network shares, making it a nightmare for company
networks. Ransoms per infected system can be upwards of $400.
We also expect APTs to continue gaining prevalence during 2016. Organized
groups such as nation states, hacktivists, industrial espionage and sabotage
providers, and cyber criminals are turning their eyes towards corporations and
government agencies with financial gain, data theft, operations disruption,
and destruction of reputation as their motives. Unlike criminalware, which will
indiscriminately target every system it can, advanced persistent threats are
highly focused and difficult to detect by conventional means. Tackling such
threats requires a much wider cyber security strategy.
Both ransomware and APTs are making headline news on an almost weekly
basis, and these stories are no longer confined solely to tech news sites. As
awareness of the reality of cyber crime and cyber war increases, governments
are scrambling to draft new regulations which will inevitably guide businesses
going forward. This coming year will also see a great deal of debate on
encryption and access to personal data — something that is likely to affect
every single computer and smartphone owner. On top of all of this, the
Internet of Things will continue to expand, and could very well play a role in
the ongoing dialogue about cyber security.

WORM:W32/DOWNADUP spreads by exploiting an unpatched vulnerability
in Windows machines in order to distribute
copies of itself.

WORM:W32/NJW0RM spreads via infected removable drives and files
attached to e-mails. If the user unwittingly uses
the drive or file, it opens a backdoor on the
device, steals saved passwords, and contacts a
web site for more instructions.

WIN32 SALITY adds an infected device into pool of similarly affected
machines (a botnet) that an attacker can control and
use to perform various malicious activities.

TROJAN:W32/GAMARUE uses the infected machine to send out spam emails.
May also download and install other malware.

Alpha Crypt	is ransomware that silently encrypts files on the user’s
machine and demands a ransom to provide the decryption
key needed to decrypt the files.

CTB-Locker is ransomware that encrypts files on the affected
machine and demands payment in return for the decryption
key needed to restore access to the files.

AnglerEK (Angler exploit kit) identifies the code used by the Angler Exploit
Kit to find and target vulnerabilities on the user’s machine.

Asprox, also known as Kuluoz, is able to download and
execute additional components. It is able to send spam emails
and has been associated with ad-fraud activities.

The Dorkbot	worm spreads via removable drives and Instant
messaging networks. It steals passwords, downloads malware
and contacts a web site for more instructions.

Banker variants attempt to steal access information for
various online banking and payment system websites. Details
stolen include login credentials, passwords, PINs and so
on. The stolen information is usually uploaded to a hacker’s
website using a webform.

The Downadup/Conficker worm family exploits a vulnerability in unpatched
Windows systems to spread copies of itself to any other
accessible machines on the same network. It also attempts to
download a file from a web site.

The CosmicDuke toolset is designed around a main
information stealer component. This information stealer
is augmented by a variety of components that the toolset
operators may selectively include with the main component
to provide additional functionalities.

CozyDuke is a modular malware platform formed around
a core backdoor component. This component can be
instructed by the C&C server to download and execute
arbitrary modules.

Trojan:W32/Cryptowall is a ransomware that silently encrypts
files on the user’s machine and demands a ransom to provide
the decryption key needed to decrypt the files.

Dridex is a banking Trojan that steals credentials of online banking
websites. One of its components is a document file
containing macro code that downloads the banking Trojan.

FakePDF	is distributed via fraudulent spam e-mail
attachments; once it has infected a system, the trojan
downloads additional files onto the affected machine.

This malware steals credentials from FTP and mail clients,
browsers, etc. It also harvests email addresses from the
infected machine, and functions as a bot that is able to send
spam emails using a peer-to-peer (P2P) infrastructure.

This family of malicious web browser extensions post
unauthorized content to a user’s Facebook Wall.

A family of malicious applications which when installed on a
computer will download a payload from a remote site, then
modify targeted webpages displayed in the web browser.
Variants in the Flashback family may include additional
malicious functionalities or characteristics.

The MiniDuke toolset consists of multiple downloader and
backdoor components. Additionally, a specific loader is often
associated with the MiniDuke toolset and is referred to as the
“MiniDuke loader”.

Also known as Andromeda, this malware ropes an infected
device into a botnet. It will often also download and install
other malware onto the infected machine.

This detection identifies the code used by the Neutrino
Exploit Kit to find and target vulnerabilities on the user’s
machine.

The GeminiDuke toolset consists of a core information
stealer, a loader and multiple persistence-related
components. It primarily collects information on the victim
computer’s configuration.

Gingerbreak	exploits a vulnerability in Android operating systems prior to
version 2.34 to gain root privileges on the device.

This detection identifies the code used by the Nuclear Exploit
Kit to find and target vulnerabilities on the user’s machine.

The OnionDuke toolset includes at least dropper, a loader, an
information stealer trojan and multiple modular variants with
associated modules.

HammerDuke is a simple backdoor. The only known
infection vector for HammerDuke is to be downloaded and
executed by CozyDuke onto a victim that has already been
compromised by that toolset.

Reveton fraudulently claims to be from a legitimate law
enforcement authority and prevents users from accessing
their infected machine, demanding that a ‘fine’ must be paid
to restore normal access.

TeslaCrypt is ransomware that silently encrypts files on
the user’s machine and demands a ransom to provide the
decryption key needed to decrypt the files.

Troldesh is ransomware that silently encrypts files on the user’s
machine and demands a ransom to provide the decryption
key needed to decrypt the files.

UnityGhost identifies iOS apps that include code introduced when the
software was created using a maliciously-modified version of
the Unity app development tool.

Urausy is ransomware that fraudulently claims to be from a
legitimate law enforcement authority and prevents users
from accessing their infected machine, demanding that a
‘fine’ must be paid to restore normal access.

Viruses belonging to this family (also known as Virut) infect
files with EXE and SCR extensions. All viruses belonging to
the Virut family also contain an IRC-based backdoor that
provides unauthorized access to infected computers.

Trojan:W32/Zbot (also known as Zeus or Wsnpoem) is a large
family of malware that steals information from an infected
system.

The result is a collection of independent episodes rather than one long movie. So pop some popcorn, get comfy, and binge-watch this season’s adventures.
CUE '80s TV-SHOW THEME MUSIC.
Episode 1: Indicators of Compromise: “Sharing Is Cyber-Caring”
Episode 2: Phishing: “Attn: Sir/Madam”
Episode 3: Vulnerabilities: “Do We Need Those Stinking Patches?”
Episode 4: Mobile: “I’ve Got 99 Problems, and Mobile Malware Isn’t Even 1% of Them”
Episode 5: Malware: “Volume, Velocity, and Variation”
Episode 6: Industry Profiles: “Raising the Stakes with Some Takes on NAICS”
Episode 7: Impact: “In the Beginning, There Was Record Count”
Episode 8: “Internet of Things”

Threat intelligence indicators have become the new brass rings on the cybersecurity merry-goround. These precious trinkets of compromise gain increasing status as more organizations and
governments jump on the sharing bandwagon.

Threat intelligence
indicators are the
new brass rings of
cybersecurity. But is this
threat sharing helpful?

GOTTA CATCH ’EM ALL.
For the past 18 months, Niddel has been collecting and analyzing open-source feeds of IP
addresses and domain name indicators. Their goal was to evaluate a diverse array of indicators
and understand how these sources of information can be leveraged to provide defenders with
an asymmetrical advantage they so desperately lack. One of the most important experiments
conducted was to determine the overlap between these feeds and whether or not there were any
“special snowflakes” to be found.
Niddell combined six months of daily updates from 54 different sources of IP addresses and
domain names tagged as malicious by their feed aggregators. The company then performed a
cumulative aggregation, meaning that if ever two different feeds were to mention the same
indicator throughout the six-month experimental period, they would be considered to be in
overlap on this specific indicator.

When biologists want to measure the population of Financial institutionsh in a lake, they use a very simple statistical trick to avoid counting every single Financial institutionsh in there. They will gather, say, 100 Financial institutionsh from the lake and tag them, then promptly release them back to their natural habitat. Later, after they have given the poor animals some time to recover from the trauma, they will gather samples of Financial institutionsh from different parts of the lake. The percentage of tagged Financial institutionsh on each of the different parts of the lake can be used to create a statistical measure of what percentage of Financial institutionsh on the lake are our original 100 tagged scaly heroes, thus estimating the total population on the lake.

Although everyone is
subjected to the same
threats, the overlap
in what is reported
on outbound feeds is
surprisingly small.

Sadly, when you look at our malicious Financial institutionsh the percentage
of indicators that are unique to only one feed over our six-month period is north of 97% for the
feeds that we have sampled. And that includes the much more overlapping inbound feeds. That
means that our “malicious Financial institutionsh samplers” are only encountering less then 3% of overlap across all
of them.
It is hard to draw a positive conclusion from these metrics, and it seems to suggest that if threat
intelligence indicators were really able to help an enterprise defense strategy, one would need to
have access to all of the feeds from all of the providers to be able to get the “best” possible coverage.
This would be a herculean task for any organization, and given the results of our analysis, the result
would still be incomplete intelligence. There is a need for companies to be able to apply their threat
intelligence to their environment in smarter ways so that even if we cannot see inside the whole lake,
we can forecast which parts of it are more likely to have a lot of Financial institutionsh we still haven’t caught.

Ideally, sharing intelligence should lead to a form of “herd alertness,” similar to the way plains
animals warn each other when predators are nearby. This would seem to require that intelligence
must be shared at a faster rate than the spread of attack in order to successfully warn the rest of
the community. “How fast is that?” you might ask, and it’s a great question.
Over 40% hit the second organization in less than an hour. That puts quite a bit of pressure on us
as a community to collect, vet, and distribute indicator-based intelligence very quickly in order to
maximize our collective preparedness.

We need to close the gap
between sharing speed
and attack speed.

Ultimately, the data speaks to a need for urgency: The faster you share, the more you
(theoretically) will stop. This is just one data source, though, and one that is geared toward
threats of a more opportunistic, high-volume, and volatile nature (e.g., brute forcing, web app
exploits, etc.) rather than more “low and slow” targeted attacks. To test whether these findings
apply more broadly, we’d be happy to incorporate data from a wider range of willing participants
next year. In the meantime, we encourage others who have such data to share it. Only when we
measure our intelligence systems will we know what they’re really doing for us and how we
can improve them.
But the overall takeaway would appear to be valid regardless: We need to close the gap between
sharing speed and attack speed.

Ultimately, what is presented here is good news (organizations are indeed sharing). However,
we’d like to recommend that if you do produce threat intel, focus on quality as a priority over
quantity. Where an opportunity for detection presents itself, seize it in the way that offers the
greatest longevity for your efforts. Certainly, anything that leads to the discovery of an incident
is worthwhile, but in most cases, context is key. Those consuming threat intelligence, let it be
known: An atomic indicator has a life of its own that may not be shared with another. Focus less
on being led to water and work on characterizing where the well resides. Expect more out of
your communities, and where possible, reciprocating context enables a wider audience to make
additional determinations that enable a broader defensive capability.

Social engineering has a long and rich tradition outside of computer/network security, and the
act of tricking an end user via e-mail has been around since AOL installation CDs were in vogue.
Do you remember the “free cup holder” prank? Someone sending you an attachment that opened
your CD-ROM drive was cute at the time, but a premonition of more malicious acts to come.
The first “phishing” campaigns typically involved an e-mail that appeared to be coming from
a bank convincing users they needed to change their passwords or provide some piece of
information, like, NOW. A fake web page and users’ willingness to fix the nonexistent problem led to account takeovers and fraudulent transactions.

Phishing campaigns have evolved in recent years to incorporate installation of malware as the
second stage of the attack. Lessons not learned from the silly pranks of yesteryear and the
all-but-mandatory requirement to have e-mail services open for all users has made phishing a
favorite tactic of state-sponsored threat actors and criminal organizations, all with the intent to
gain an initial foothold into a network.
In the 2013 DBIR, phishing was associated with over 95% of incidents attributed to statesponsored actors, and for two years running, more than two-thirds of incidents that comprise
the Cyber-Espionage pattern have featured phishing. The user interaction is not about eliciting
information, but for attackers to establish persistence on user devices, set up camp, and continue
their stealthy march inside the network.

For two years, more than two-thirds of incidents that comprise the
Cyber-Espionage pattern have featured phishing.
Financial motivation is also still alive and well in phishing attacks. The “old” method of duping
people into providing their personal identification numbers or bank information is still around,
but the targets are largely individuals versus organizations. Phishing with the intent of device
compromise is certainly present, and there were hundreds of incidents in the Crimeware section
that included phishing in the event chain. Regardless of motive, the next section will show that
good things will come to those who bait.
In previous years, we saw phishing messages come and go and reported that the overall
effectiveness of phishing campaigns was between 10 and 20%. This year, we noted that some of
these stats went higher, with 23% of recipients now opening phishing messages and 11% clicking
on attachments. Some stats were lower, though with a slight decline in users actually going to
phishing sites and giving up passwords.
Now, these messages are rarely sent in isolation—with some arriving faster than others. Many
are sent as part of a slow and steady campaign. 9 The numbers again show that a campaign of just
10 e-mails yields a greater than 90% chance that at least one person will become the criminal’s
prey, and it’s bag it, tag it, sell it to the butcher (or phishmonger) in the store.

50% OPEN
E-MAILS AND CLICK ON
PHISHING LINKS WITHIN
THE FIRST HOUR.

How long does an attacker have to wait to get that foot in the door? We aggregated the results of
over 150,000 e-mails sent as part of sanctioned tests by two of our security awareness partners
and measured how much time had passed from when the message was sent to when the recipient
opened it, and if they were influenced to click or provide data (where the real damage is done). The
data showed that nearly 50% of users open e-mails and click on phishing links within the first hour.

The reality is that you don't have time on your side when it comes to
detecting and reacting to phishing events.
How long do you suppose you have until the first message in the campaign is clicked? Not long at
all, with the median time-to-first-click coming in at one minute, 22 seconds across all campaigns.
With users taking the bait this quickly, the hard reality is that you don’t have time on your side
when it comes to detecting and reacting to phishing events.
THERE ARE PLENTY OF PHISH IN THE SEA.
We looked at organization demographics to see if one department or user group was more likely
than another to fall victim to phishing attacks. Departments such as Communications, Legal, and
Customer Service were far more likely to actually open an e-mail than all other departments.
Then again, opening e-mail is a central, often mandatory, component of their jobs.

The payload for these phishing
messages has to come from
somewhere. Data from the
Anti-Phishing Working Group
(APWG) suggests that the
infrastructure being used is
quite extensive (over 9,000
domains and nearly 50,000
phishing URLs tracked each
month across the Group's
members). The charts in Figure
9 also show that the attackers
have finally learned a thing or
two from the bounty of their
enterprise breaches and may
even have adopted a Lean
Six Sigma approach to
optimize operations.

So what do we do about this? Hire only robots? Bring back command-line mail? There is obviously no
one-shot antidote for the problem at hand.
Taking measures to block, filter, and alert on phishing e-mails at the gateway is preferred, but no
technological defense is perfect, which leads us straight to… people.

"One of the most
effective ways you can
minimize the phishing
threat is through
awareness and training."

There is some hope in this data in that three-quarters of e-mails are not opened or interacted with. We
wondered if there was a way to bump that number up (e.g., by giving users a quick way to flag potential
phishes and become a detective control), so we asked Ellen Powers, The MITRE Corporation’s
Information Security Awareness Program Manager, about the effectiveness of making users part of
the active defense against phishing. She noted that “MITRE employees, our human sensor network,
detect 10% of advanced cyber attacks that reach employee e-mail in-boxes.”
Lance Spitzner, Training Director for the SANS Securing The Human program, echoes Ellen’s
sentiments, noting that “one of the most effective ways you can minimize the phishing threat is
through effective awareness and training. Not only can you reduce the number of people that
fall victim to (potentially) less than 5%, you create a network of human sensors that are more
effective at detecting phishing attacks than almost any technology.”

Do We Need Those Stinking Patches?

Of all the risk factors in the InfoSec domain, vulnerabilities are probably the most discussed,
tracked, and assessed over the last 20 years. But how well do we really understand them? Their
link to security incidents is clear enough after the fact, but what can we do before the breach to
improve vulnerability management programs? These are the questions on our minds as we enter
this section, and Risk I/O was kind enough to join us in the search for answers.
Risk I/O started aggregating vulnerability exploit data from its threat feed partners in late 2013.
The data set spans 200 million+ successful exploitations across 500+ common vulnerabilities
and exposures (CVEs) 11 from over 20,000 enterprises in more than 150 countries. Risk I/O does
this by correlating SIEM logs, analyzing them for exploit signatures, and pairing those with
vulnerability scans of the same environments to create an aggregated picture of exploited
vulnerabilities over time. We focused on mining the patterns in the successful exploits to see if
we could figure out ways to prioritize remediation and patching efforts for known vulnerabilities.

99.9% OF THE EXPLOITED
VULNERABILITIES
WERE COMPROMISED
MORE THAN A YEAR
AFTER THE CVE
WAS PUBLISHED.

NOT ALL CVES ARE CREATED EQUAL.
And therein, of course, lies the challenge; once the
“mega-vulns” are roped in (assuming you could identify them ahead of time), how do you approach
addressing the rest of the horde in an orderly, comprehensive, and continuous manner over time?

About half of the CVEs
exploited in 2014 went
from publish to pwn in
less than a month.

The industry standard for rating the criticality of vulnerabilities is CVSS, which incorporates
factors related to exploitability and impact into an overall base score. The idea
is to determine which CVSS factors (if any) pop out and thus might serve as a type of early warning
system for vulnerabilities that need quick remediation due to high likelihood of exploitation.

The Common Vulnerability Scoring System (CVSS) is designed to provide an open and standardized method for rating IT vulnerabilities.

None of the exploitability factors appear much different across the groups; it seems that just
about all CVEs have a network access vector and require no authentication, so those won’t be
good predictors. The impact factors get interesting; the proportion of CVEs with a “complete”
rating for C-I-A rises rather dramatically as we move from all CVEs to quickly exploited CVEs.
The base score is really just a composite of the other two factors, but it’s still worth noting that
most of those exploited within a month post a score of nine or ten. We performed some statistical
significance tests and found some extremely low p-values, signifying that those differences are
meaningful rather than random variation. Even so, we agree with RISK I/O’s finding that a CVE
being added to Metasploit is probably the single most reliable predictor of exploitation in the wild.

A CVE being added to
Metaspoit is probably
the single most reliable
predictor of exploitation
in the wild.

Outside the CVSS score, there is one other attribute of a “critical” vulnerability to bring up, and
this is a purely subjective observation. If a vulnerability gets a cool name in the media, it probably
falls into this “critical vulnerability” label. As an example, in 2014, Heartbleed, POODLE, Schannel,
and Sandworm were all observed being exploited within a month of CVE publication date.

I Got 99 Problems and Mobile Malware Isn’t Even 1% of Them.

The dearth of stats and trends around mobile devices in the DBIR has been a rather obvious
void for years. But the DBIR has its roots in forensic breach investigations, and mobile
breaches have been few and far between over the years. Adding dozens of new contributors
didn’t change that, and we’ve come to the same data-driven conclusion year after year: Mobile
devices are not a preferred vector in data breaches. This year, however, we set our minds to
analyzing the mobile space, come cell or high water.

Our data-driven
conclusion: Mobile
devices are not a
preferred vector in data
breaches.

Before we get too far, let's just get this out of the way now— Android™ wins. Not just wins, but
Android wins so hard that most of the suspicious activity logged from iOS devices was just failed
Android exploits. So while we’d love to compare and contrast iOS to Android, the data is forcibly
limiting the discussion to the latter. Also, the malicious activity recorded on Android is centered on
malware, and most of that malware is adnoyance-ware and similar resource-wasting infections.
We chopped, sliced, and flipped the data more times than a hibachi chef, since we didn’t want to
simply share a count of overall malware infections and enumerate vulnerabilities. There is already
good research in this area, and we didn’t think we could add much more. However, we did have
one big question when it comes to the security of mobile devices: How big of a problem is it? It’s
difficult to attend a conference or see some top-whatever list without “mobile” showing up, yet
it’s not a theme in our primary corpus, or any of our partners’ exploit data.

They came through with a lot of data. With our first
pass through the data, we found hundreds of thousands of (Android) malware infections, most fitting
squarely in the adnoyance-ware category. In our second through eighteenth passes, we turned the
data inside out but ended up just coming back to the malware. Finally, we stripped away the “lowgrade” malware and found that the count of compromised devices was truly negligible. The benefit of
working with an internal team is that we knew how many devices were being monitored. An average of
0.03% of smartphones per week—out of tens of millions of mobile devices on the Verizon network—
were infected with “higher-grade” malicious code. This is an even tinier fraction than the overall 0.68%
infection rate (of all types of unwanted software) from Kindsight Security Labs’ biannual report.

OUT OF TENS OF
MILLIONS OF MOBILE
DEVICES, THE
NUMBER OF ONES
INFECTED WITH TRULY
MALICIOUS EXPLOITS
WAS NEGLIGIBLE.

We asked one of our contributors — FireEye — to give us its view of the vulnerabilities it
catches in various mobile platforms and applications. FireEye noted that two main platforms
dominate the mobile market today: Google’s Android and Apple’s iOS. FireEye researchers
analyzed more than 7 million mobile apps on both platforms from January to October 2014. 96% of mobile malware was targeted at the Android platform (which tracks well with
our active malware findings in this report). More than 5 billion downloaded Android apps are vulnerable to remote attacks. One
significant vulnerability is known as JavaScript-Binding-Over-HTTP (JBOH), which enables
an attacker to execute code remotely on Android devices that have affected apps.

EnPublic apps bypass Apple’s strict review process by hijacking a process normally used to
install custom enterprise apps and used for beta testing. We also found that 80% of EnPublic
apps19 invoke risky private APIs that are also in violation of Apple’s Developer guidelines. In the wrong hands, these APIs threaten user privacy and introduce many vulnerabilities.

Adware is software that delivers ads to make money. While adware is not in itself harmful, it
often aggressively collects personal information from the mobile device it’s installed on,
including name, birth date, location, serial number, contacts, and browser bookmarks. Often,
this data is collected without users’ consent. In our review, we examined ad libraries in
Android apps. Adware is an increasingly popular option for app publishers, growing from
almost 300,000 apps in 2013 to more than 410,000 in the first three quarters of 2014 alone.

Mobile devices are not a theme in our breach data, nor are they a theme in our partners’ breach
and security data. We feel safe saying that while a major carrier is looking for and monitoring the
security of mobile devices on its network, data breaches involving mobile devices should not be in
any top-whatever list. This report is filled with thousands of stories of data loss—as it has been
for years—and rarely do those stories include a smartphone.

A quick look at the types of malware being used shows they are overwhelmingly opportunistic
and relatively short-lived. Even though we looked at data just over a six-month period, 95% of
the malware types showed up for less than a month, while four out of five didn’t last beyond a
week.

We are not saying that we can ignore mobile devices; far from it. Mobile devices have clearly
demonstrated their ability to be vulnerable. What we are saying is that we know the threat actors
are already using a variety of other methods to break into our systems, and we should prioritize
our resources to focus on the methods that they’re using now.
When it comes to mobile devices on your network, the best advice we have is to strive first for
visibility and second for control. Visibility enables awareness, which will come in handy when the
current landscape starts to shift. Control should put you into a position to react quickly.

Malware. Malware is what bwings us together today. This year, data from FireEye, Palo Alto
Networks, LastLine, and Fortinet gave us a unique opportunity to peer into the malevolent
machinations of criminals across nearly 10,000 organizations—large and small—in every industry
vertical over the course of calendar year 2014. 21 In previous years, we were only able to show
how malware contributed to confirmed security incidents. This year, we drank straight from
the firehose of breaches that might have been. Staring into this malicious abyss renewed our
admiration and respect for those responsible for defending their organizations, and we hope
our overview of the volume, velocity, and variation of malware will first inform, and then
inspire you to take your security operations crew out for a round of drinks.

As we said, that’s simple math, and arriving at the actual malware threat-event frequency for any
given organization is nowhere near as cut and dried. To get a more precise handle on this, we looked
at the likelihood of an organization having a malware event on any given day. It may be difficult to
believe, but not every organization experiences one of those every day. 23 Our analyses of the data
showed that half the organizations experienced 35 or fewer days of caught malware events during
an entire calendar year. Keep in mind, by the time it hits these appliances, controls like firewalls,
intrusion detection systems (IDS)/intrusion prevention systems (IPS), spam filters, etc., will have
already reduced the raw stream of malware. Speaking of these devices, when malware events are
seen and caught by them, it’s more likely to be dozens (or fewer) than hundreds or thousands.

The takeaway here is that while we’ve provided a baseline view of malware threat-event frequency,
you should be capturing this data in your own environment, using it to understand how this overview
compares to your own organization, and analyzing how your organization’s own view changes over time.

We use “unique” here from a signature/hash perspective; when compared byte-to-byte with all
other known malware, there’s no exact match. That’s not to say that what the malware does is also
distinct. Criminals haven’t been blind to the signature- and hash-matching techniques used by antivirus (AV) products to detect malware. In response, they use many techniques that introduce simple
modifications into the code so that the hash is unique, yet it exhibits the same desired behavior.
The result is often millions of “different” samples of the “same” malicious program.
This is more than just the malware analyst form of omphaloskepsis (look it up). It has real-world
consequences, which basically boil down to “AV is dead.” Except it’s not really. Various forms of
AV, from gateway to host, are still alive and quarantining nasty stuff every day. “Signatures alone
are dead” is a much more appropriate mantra that reinforces the need for smarter and adaptive
approaches to combating today’s highly varied malware.
There’s another lesson here worth stating: Receiving a never-before-seen piece of malware
doesn’t mean it was an “advanced” or “targeted” attack. It’s kinda cool to think they handcrafted
a highly custom program just for you, but it’s just not true. Get over it and get ready for it. Special
snowflakes fall on every backyard.

The key differences between the malcode of 2005 and malware of 2014 are that the older
viruses were noisy e-mail worms with varying backdoor capabilities, whereas the common
components of the 2014 top seven involve stealthy command-and-control botnet membership,
credential theft, and some form of fraud (clickfraud or bitcoin mining). Alas, those were
simpler times back in 2005.

We managed to borrow a Wayback machine to take a trip to 4 B.D. (before DBIR) to pluck some
research wisdom from one of our elder researchers. Specifically, we wanted to compare one of
his findings from yesteryear against the current malware climate to see how much (or little)
has changed.
The observation was that back in 2005, “just seven families represented about 70% of all
malcode activity.” (For those interested, those were Mytob, Netsky, Zafi, Sober, Lovgate,
Mydoom, and Bagle.) Fast-forward to 2014, and our analysis of the data from our network
malware defense partners suggests that should be updated to read, “20 families represented
about 70% of all malware activity.”(Today’s sinister seven are zbot, rerdom, zeroaccess,
andromeda, expiro, asprox, gamaru, and sality.)

The 2005 analyses mostly came from data in the WildList, an effort started by Joe Wells and Sarah Gordon to maintain a list of
malicious binaries that are active “in the field” for use by researchers and defenders.

This “Before and Beyond the Breach” section paints a picture of the volume, velocity, and variation of malware by looking at the
problem from within organizations. Thanks to a new DBIR participant—BitSight—we can also take a look at the view from the outside.
BitSight uses publicly accessible indicators of compromise to create a rating that measures the “security hygiene” of an
organization. Specifically, we combed through BitSight’s botnet index (which is one component of the overall BitSight rating) to get a
feel for how frequently organizations are seen communicating with malicious nodes.
An organization’s BitSight rating (and the components that make up that rating) will take a hit each time BitSight’s monitoring
infrastructure sees a beacon attempt from the IP space allocated to the company. We took the average number of botnet triggers in
2014 (for each company), then built a distribution across all organizations within an industry and compared those distributions across
all industries.
(NOTE: BitSight refers to the time of first trigger to the time the beaconing stops as “Time to Fix” vs. “Beacon Days.”)
Financial institutions are not immune to successful malware deployments, but most of them have relatively few (and other analyses
of the BitSight data show that financial institutions detect and generally clean up infections pretty quickly).
Insurance and Retail organizations begin to show more diversity—hence, more infections—with the situation getting worse as we move to
Utilities. Ultimately, the “leader” in near-pervasive infections across the majority of underlying organizations is Education. This should come
as no surprise, given the regular influx of unmanaged devices as hordes of innocent youth invade our halls of higher learning. Toga! Toga!
The major takeaway was that different industries exhibit substantially different
threat profiles and therefore cannot possibly have the same remediation priorities. That may be a
rather “no duh” finding, but keep in mind most security standards treat all requirements as equal
stepping stones on a path to 100% compliance. Past reports have emphasized that with security,
there is no ”one size fits all” approach. It is our fervent hope that that data sowed some seeds of
change, and this year we’d like to help grow that crop a bit more.

With security, there
is no “one size fits all”
approach.

Whereas last year’s report asked “Do all organizations share similar threat profiles?”, we now want
to explore what we believe to be a much better question: “Which industries exhibit similar threat
profiles?” Just as our nine patterns helped to simplify a complex issue last year, we believe that
answering this question can help clarify the “so what?” question for different verticals. Figure 19
measures and provides, at least in part, the answer to that question.

Although we realize that at first glance it may look like a drunken astronomer’s attempt at describing
a faraway galaxy, once correctly deciphered, it is actually a godsend of interesting
observations.

SOME OF THESE THINGS ARE NOT LIKE THE OTHERS.
Some of these things just don’t belong. Can you tell which things are not like the others before we
finish this section?

The U.S. is traditionally described as a homogenous “melting pot” of cultures, but some suggest
it’s more like a salad bowl where individual cultures mix together while retaining their own unique
aspects.
There are a few closely grouped subsectors (e.g., the 44x retailers on the upper side of the main
pack), but by and large, the colors/numbers intermingle in melting-pot fashion. And that’s a rather
important discovery. It means that many subsectors in different industries actually share a
closer threat profile than do subsectors in the same overall industry.

Many subsectors in different industries actually share a closer threat
profile than do subsectors in the same overall industry.
For instance, see the bottom of the figure where Monetary Authorities-Central Bank (financial
and insurance industry (521) falls between two subsectors in the manufacturing industry (32). In
other words, each of the manufacturing subsectors have more in common with central banks than
they do with each other. You know, sort of like the majority of us have more in common with our
friends than we do our families.

Similar to but separate from observation two is that some subsector neighbors seem as though
they were bad matches on Tinder. For instance, why are general merchandise stores (452) right
on top of data processing, hosting, and related services (518)? If I had a dollar for every time
someone said, “I bet this data center sees the same attacks as my local mall,” I’d still be broke.

We don’t understand the motives of our adversaries as well as we think we do. Maybe cyber risk
has more to do with business models or organizational structure or company policies than which
high-level industry category one falls under. We definitely have some more work to do to peel
back the covers on this topic.
WE NEED MORE CROSS-SECTOR SHARING.
WHY DOES EVERYBODY WANT TO KEEP IT LIKE THE KAISER?
Likewise, information sharing, compliance, and regulatory standards imposed on an industry level
may not be the best approach. Perhaps regulating common “risk activities” is the better route
(e.g., how the Payment Card Industry Data Security Standard applies to all those who process,
store, or transfer payments rather than any one particular industry). Maybe it’s some other
way/means/criterion we haven’t thought of yet. But it’s clear that before we begin creating and
enforcing a bunch of “cyber regulations” in the wake of the “cyber craziness” that was 2014, we
need to better understand the true effects and efficacies of such actions.

Information sharing,
compliance, and
regulatory standards
imposed on an industry
level may not be the
best approach.

It follows that our standard practice of organizing information-sharing
groups and activities according to broad industries is less than optimal.
It might even be counterproductive.
Given the above, it follows that our standard practice of organizing information-sharing
groups and activities according to broad industries is less than optimal. It might even be
counterproductive. Is this a case where our biases and faulty assumptions are blinding us? (Say it
ain’t so!) With all the focus, innovation, and regulation around cyber info/intel sharing these days,
this is something we really need to consider and investigate further.

If we had $201 for every time someone asked us, “Do you have data on the cost of breaches?”, we’d
have $128,037. For the past seven years, we’ve had to answer that question with an apologetic
“No,” while doing our best to explain why. But not this time; we’re absolutely ecstatic to offer an
anticipatory “Yes!” to that question in this long-overdue section. It took us eight years to get here,
but “better eight than never,” right?

That we always get the impact question is completely understandable. When budgeting and
operating an InfoSec program, accurately assessing what’s likely to happen and how much it’ll
cost are both critically important. A lack of reliable estimates leads to a creative environment
for decision making, 31 where underspending, overspending, and useless spending invariably
result. Regrettably, there is a large and glaring gap in the security industry when it comes to
quantifying losses. To fill that gap, organizations typically use qualitative methods of rating loss
or something like the cost-per-record estimate promoted in the “Cost of Data Breach Study”
from surveys conducted by the Ponemon Institute.


Larger organizations
have higher losses
per breach, but they
typically lose more
records and have higher
overall costs.

Keep in mind that we’re not saying record count is all that matters; we’ve already demonstrated
that it accounts for half of the story. But it’s all that seems to matter among the data points we
have at our disposal. What we’ve learned here is that while we can create a better model than
cost per records, it could be improved further by collecting more and different data, rather than
specifics about the breach, to make better models.
LET IT GO, LET IT GO.
The cold (cost-per-record) figure never bothered us anyway, but we think it’s time to turn away
and slam the door. To that end, we wrap up this section with a handy lookup table that includes a
record count and the single-point prediction that you can use for “just give me a number” requests
(the expected column in the middle). The rest of the columns show 95% confidence intervals, first
for the average loss and predicted loss. The average loss should contain the mean loss (if there
were multiple incidents). The predicted loss shows the (rather large) estimated range we should
expect from any single event.

There are a few interesting things to note about the breakdown of incident patterns. It may not be obvious at
first glance, but the common denominator across the top four patterns—accounting for nearly
90% of all incidents—is people. Whether it’s goofing up, getting infected, behaving badly, or
losing stuff, most incidents fall in the PEBKAC and ID-10T über-patterns. At this point, take your
index finger, place it on your chest, and repeat “I am the problem,” as long as it takes to believe it.
Good—the first step to recovery is admitting the problem.

A lot of threat patterns
didn’t reveal major trend
changes. For this reason,
some may wish to refer
back to the 2014 DBIR
for a primer on incident
patterns.

Did payment card skimmers and POS attacks go extinct in 2012?
Nope. We just tripled contributors that year and brought in a large
volume of new threats.
Showing Figure 25 is risky because it may cause more confusion than valid conclusions, but
what the heck—we live on the edge. Although we’d like it to purely reflect changes in the
external threat environment over the years, it more realistically reflects changes to our data
set caused by a rapidly expanding base of contributors. Did Payment Card Skimmers and Pointof-Sale Intrusions really go extinct in 2012? Nope. We just tripled contributors that year and
brought in a large volume of new/different threats (e.g., Miscellaneous Errors). Given that kind
of volatility in the data set, it’s amazing that some, like Insider Misuse and Web App Attacks,
remain quite stable over time.

We debated at length whether to rename this pattern to “The POS Paradox” or keep it at just
plain ol’ “Point-of-Sale Intrusions.” You can see where we ended up, but you might want to pop
some more popcorn as we take you on a walk through memory lane to see where POS incidents
have been and where they are today.
When POS breaches were at their peak (back in the 2011 and 2012 DBIRs), there was little buzz
about them in information security circles. We suspect that’s because those breaches generally
involved small businesses and low-dollar amounts. In truth, it seemed a bit strange to us to make
a big deal out of 43 pwnd PANs from “Major Carl’s Italian Eats” too, especially given the jackpot
hauls of just a few years earlier.

After the dust settled from prosecutions of perpetrators involved in the mega-breaches in the
2005–2009 time frame, we were beginning to think that massive payment card plunders were
becoming a bit passé—with smaller, opportunistic POS intrusions becoming commonplace. The
fruitful combination of Internet-facing POS devices and default passwords made compromise
trivial for attackers, and the smaller amounts of compromised data mixed with the lack of logging
(or any controls, really) limited the likelihood of getting caught.
Then Q4 of 2013 happened, crushing the idea that high-profile, headline-grabbing, payment card
breaches had been put out to pasture, with Code Red, SQL Slammer, and phone phreaking. The
evolution of attacks against POS systems continued in 2014 with large organizations suffering
breaches alongside the small retailers and restaurants that had been the cash cows for years.
Despite the actors and actions being the same for the majority of breaches, the impacts to large
and small organization POS breaches are far from identical. Yep, we did. That’s how we roll. But, we’re really fun at parties. Honest.

There has been a definite evolution in POS attacks from simple storage scraping to active RAM
skimming across all breach types. We can, however, see distinct differences between large and
small organizations in the methods used to gain access to the POS devices. For small orgs, the
POS device is directly targeted, normally by guessing or brute-forcing41 the passwords. Larger
breaches tend to be a multi-step attack with some secondary system being breached before
attacking the POS system.

In 2014, the evolution of attacks against POS systems continued,
with large organizations suffering breaches alongside the small
retailers and restaurants.

Larger breaches tend to
be a multi-step attack
with some secondary
system being breached
before attacking the
POS system.

Criminal innovation is not limited to the Payment Card Skimmers pattern. Last year, there
were several instances where vendors providing POS services were the source of the
compromise. Some vendors had keyloggers installed via successful phishing campaigns or
network penetrations. All breached POS vendors ended up with their remote access credentials
compromised, inviting attackers into customer environments where the card harvesting began.
We also noticed a trend in a shift from a reliance on default credentials to the capture and use of
stolen credentials. These are also not mere opportunistic attacks. Many incidents involved direct
social engineering of store employees (often via a simple phone call) in order to trick them into
providing the password needed for remote access to the POS.
Attacks on POS systems are not new, and they are relevant to organizations big and small that
are swiping cards to collect revenue. The attack methods are becoming more varied, even against
small businesses. This is an indication that the threat actors are able to adapt, when necessary, to
satisfy their motives (and greed will not be trending down any time soon).
HOW DO I LEARN MORE?
Find out what monitoring options are available for your POS environment (if any) and start using
them. Your level of diligence must match the increased level of sophistication and patience being
demonstrated by the hackers.
While we have tried to refrain from best practices advice this year, there’s no getting around the
fact that credentials are literally the keys to the digital kingdom. If possible, improve them with a
second factor such as a hardware token or mobile app and monitor login activity with an eye out
for unusual patterns.

One bit of good news. Detection times are definitely getting better,
shifting from months and weeks to hours and days.
OUT OF SIGHT, OUT OF CASH?
The stories in this pattern may read like ancient sagas, but the actors continue to innovate.
Previous DBIRs document the use of locally mounted pinhole cameras and remote cameras (both
designed to obtain the coveted PIN) and the use of remote stripe-data collection via Bluetooth®
or cellular devices. This year’s improvements include the use of ridiculously thin and translucent
skimmers that fit inside the card reader slot as well as direct tapping of the device electronics
to capture the data with nary a trace of visibility. Gone (mostly) are the days of the quick tug to
test for the presence of these devices. Still, all it really takes to thwart certain classes of these
card-present cybercrime advancements is shielding the video capture component with your hand;
and—remember—be as creative as you like when doing so.

In October of 2015, the Europay, MasterCard, and Visa (EMV) chip-and-PIN mandate goes into full
effect in the U.S., just as we learn that poor implementations are still left vulnerable to attack.
Furthermore, despite a date being set, it will take time to deploy new equipment to a critical mass
of merchants and to re-issue cards to the still unPINned masses.
U.S. consumers who are eagerly awaiting the deadline may want to curb their enthusiasm just a bit.
The main change that is taking place is an invisible (to the consumer) shift in liability. You’ll still
see mag-stripe readers a-plenty, and when there is an incidence of card fraud, whichever party
has the lesser technology—merchants who haven’t upgraded their terminals or banks that haven’t
issued new EMV cards—will bear the blame.

In October 2015, the
chip-and-PIN mandate
goes into full effect in
the United States. A
word of caution—poor
implementations are still
vulnerable to attack.


Merchants should work with their providers to understand their chip-and-PIN reader options
and look for solutions that are less prone to indirect attacks. Don’t just replace one bad bit of
technology with another.
Monitor your physical card environments through video surveillance and tamper monitoring to help
continue the positive shift in time to detect (which will also help reduce overall merchant liability).
For those merchants who deal primarily in card-not-present or online transactions, you might want
to consider upping your game when it comes to fraud monitoring (you do have fraud monitoring
systems/processes in place now, right?) and ensuring you have response plans in place when fraud
eventually happens (and it will).

To tag something solely as a malware incident is a common over-generalization and, as we as all
know, all generalizations are false. Malware is part of the event chain in virtually every security
incident (it’s difficult to get a computer virus onto paper records in a box of file folders, though
we suspect Hollywood will find some way to do that soon).

Once these malevolent bytes invade a system, they surreptitiously usurp existing functionality
and start performing actions of their own design. We see common one-two mal-punches in a few
places, from maintaining persistence and staging advanced attacks (ref: Cyber-Espionage pattern)
to capturing and exfiltrating data (ref: Point-of-Sale Intrusions pattern). This catch-all Crimeware
pattern represents malware infections within organizations that are not associated with more
specialized classification patterns such as Cyber-Espionage or Point-of-Sale Intrusions.

Crimeware represents malware infections within organizations that are
not associated with more specialized classification patterns.

Like speeches by a politician, Crimeware incidents in our corpus are large in number and short
on details, as these everyday incidents are less likely to receive a full forensic investigation or
rise to the level of law enforcement involvement. They are also predominantly opportunistic and
financially motivated in nature.

Not much changed in the way of details for the Crimeware pattern since its debut last year, but
there were some notable differences worth a mention. First, malware used to launch DoS attacks
jumped from #8 to #2 in threat action variety, with command-and-control (C2) continuing to
defend its lead position. This isn’t surprising, as the rest of the malware threat actions rely on a
robust command and control structure to function. (NOTE: There’s more on DoS in the Denial-ofService Attacks pattern section).

When there is confirmed data breaches, bank records and credentials traded places for the top
spot, though we suspect credentials may be under-represented given that it’s common practice
for criminals to use keyloggers to steal credentials, which are ultimately used to gain banking
information. One last item of interest here is that trade secrets were compromised in several
cases in this pattern, even without espionage as the motive (they would have been in the CyberEspionage pattern and not here), which shows that even onesie-twosie malware can put very
sensitive corporate data at risk.

Our "Before and Beyond the Breach" featurette on malware confirms the volume and variety
findings in this pattern on the threat side of the equation and also demonstrates that tools are
available to enable organizations to do a relatively good job at discovering crimeware incidents.
Quantifying the malware incident details is another matter.
We suggest not only capturing and tracking your own malware incidents (ie, COUNT ALL THE
THINGS!) but also spending the time necessary to get into the weeds to uncover what actions
malicious programs were intent on carrying out in your environment.
If you’re relegating the task of handling this run-of-the-mill malcode solely to your help desk in
a set-it-and-forget-it process, we suggest you rethink that strategy, as you may be able to learn
more from these incidents than you think.

Aristotle once said that the avarice of mankind is insatiable. This philosophical insight is no
less true for cybercriminals, since we can only assume that they were so distressed by last
year’s DBIR findings (TLDR: ideology > greed) that this year, organized crime became the most
frequently seen threat actor for Web App Attacks, with financial gain being the most common of
the primary motives for attacking.


This year, organized crime became the most frequently seen threat
actor for Web App Attacks.
A long time ago in a DBIR far, far away, we began to see high-profile instances of hackers
targeting web servers just to set up an attack on a different target, a tactic known as a Strategic
Web Compromise. We began to track this type of attack last year (so, it shows up in this year’s
data) and we’re seeing that secondary attacks make up nearly two-thirds of Web App Attacks.
Virtually every attack in this data set (98%) was opportunistic in nature, all aimed at easy marks.

One interesting sub-pattern distinguishes Financial Services from the rest. End-user devices were
a factor in 82% of incidents and nearly a tenth of them involve some human element (phishing/
social). A look through the details of these incidents shows a common sequence of “phish customer
≥ get credentials ≥ abuse web application ≥ empty bank/bitcoin account.”
Pulling back from a single industry view, we find that most of the attacks make use of stolen
credentials, which is a story we’ve been telling since 1 AD.  Over 95% of these incidents involve
harvesting creds from customer devices, then logging into web applications with them.
Cross-site scripting and SQL injection (SQLi) haven’t disappeared from the list but still seem less
favored than simply using actual credentials. Unfortunately, the specific incidents are scant on
details, but with so many credential lists available for sale or already in the wild, why should a
criminal actually earn his/her keep through SQLi when a simple login will suffice?

If you have a web presence (e-commerce or otherwise) you should be tracking user behavior and
using some form of fraud detection to get an early warning on suspicious behavior. Load balancer
logs, web application logs, and database transaction logs can all help identify malicious activity
before your last bit of sensitive data is fully exfiltrated.
Get a complete inventory of every component of your web presence (honestly, it’s not that hard)
and ensure they are all in a regular patch cycle. Three-quarters of web app compromises are
opportunistic, so this falls squarely under “the cost of doing business.”
To combat Web App Attacks head-on, we recommend strengthening authentication. The use of
two-factor authentication for web applications—even by customers—will go a long way toward
keeping your organization from being used and abused.

Distributed denial-of-service (DDoS) attacks got worse again this year with our reporting
partners logging double the number of incidents from last year (in other shocking news:
water is wet). However, we also noticed an interesting pattern that might have some practical
implications for defenders. Essentially, we saw some indication that there may be two distinct
tiers—or clusters—of DDoS attacks based on bandwidth, velocity, and duration.
Before we get to that, we need to first tie up a thread that started in the Crimeware pattern.
This year, we saw a significant jump in the DoS threat action variety associated with malware.
These incidents came mostly from our computer emergency response team (CERT) partners
(with additional ones coming from Arbor and Akamai), and involved the repurposing of servers/
devices to be used in amplification/reflection attacks. These attacks rely on improperly secured
services, such as Network Time Protocol (NTP), Domain Name System (DNS), and Simple Service
Discovery Protocol (SSDP), which make it possible for attackers to spoof source IP addresses,
send out a bazillion tiny request packets, and have the services inundate an unwitting target
with the equivalent number of much larger payload replies. NTP topped the list49 with max attack
bandwidth hitting 325 Gbps, with SSDP jumping on the DoS boat for a 134 Gbps cruise.

We saw some indication that there may be two distinct tiers—or
clusters—of DDoS attacks based on bandwidth, velocity, and duration.
Stepping back to the broader series of attacks, let start by looking at one subset of the DDoS
data that comprises about a thousand of the “worst of the worst” DDoS incidents last year.
Instead of a single most common measure, bandwidth has two clusters around 15 and 59 Gbps,
while velocity has clusters around 3 and 15 million packets per second. Data about attack
duration similarly suggest clusters around one- and two-day average durations. When we saw this
pattern emerge from several distinct subsets of DDoS incidents from different contributors, we
decided it was worth highlighting.

What is
actually going on here? Are we seeing two tiers of DDoS actors, maybe ideologically motivated
and criminal? Or, are we seeing two tiers of DDoS-for-hire criminal product tiers? We’ll need
better data, especially around actors, to support any solid conclusions.

Last year, it was hard to give much advice about DDoS beyond just saying “plan ahead.” We hope
this data might bring additional solid numbers to those planning conversations. Even without full
knowledge of the underlying details of the criminal machinations, we think there are also some
practical takeaways. We’ll begin with service providers, a term that includes anyone who runs
their own UDP-based services and even those with home routers: Secure your services (which
means knowing where your services are and how they’re configured). Block access to known
botnet C2 servers50 and patch your systems to help stop malware from turning your nodes into
hapless automatons of doom. For larger providers, anti-spoofing filters at the Internet edge can
also help prevent reflection/amplification techniques.

A message for service
providers: Secure your
services. Block access to
known botnet C2 servers
and patch your systems.

To understand how your organization would react to a DDoS attack, conduct regular drills/
exercises to see where you need to shore up processes and, perhaps, add technology or external
mitigation services to help maintain or restore services. This year’s data also has us wondering
whether it means there might be room for less expensive, medium-sized mitigations that would
protect against many if not all DDoS attacks.

We were almost at a loss for words for this section and, if you were hoping this would finally be the
year for a spike in stolen mainframes, we’re afraid we must let you down (again). As was the case with
our previous reports, people are people; so, why should it be that we expect perfection when it comes
to the physical security of their corporate devices? Also (predictably), folks still steal things.


The data is heavily biased towards U.S. industries (99.8%) that operate under mandatory disclosure
regulations, with the Public sector dominating the field. (Healthcare was also well represented.)
Despite valiant efforts by our crack team, all the king’s data scientists couldn’t find a chart or data
visualization to put together that was actionable to you, our beloved readers and defenders. In the
end, every industry loses things, and almost all theft was opportunistic in nature.

Like last year, most of the theft occurred within the victim’s work
area—55% of incidents.
There are no new tactics being used by the adversaries in this pattern to steal equipment.
Like last year, most of the theft occurred within the victim’s work area (55% of incidents), but
employee-owned vehicles (22% of incidents) are also a common location for thefts to occur.
While we are not spending a significant amount of prime DBIR real estate discussing this
further, this pattern is not to be taken lightly. The impact to an organization can be significant
(if not equal to other data-loss events), depending on the sensitivity of the data resident on
the asset(s) involved and the controls that have and have not been implemented to protect the
confidentiality52 of and recoverability of the data.
HOW CAN I LEARN MORE?
Work with your procurement department to know who has what, and track the volume and variety
of devices lost each week/month to see if there’s a pattern of behavior you need to identify and
prepare for. Make it super easy for folks to report lost or stolen devices, perhaps going so far as
to incentivize your workforce to report all incidents within a certain number of hours (15% of
incidents in this category still take days to discover).
Full-disk encryption, locking down USB ports, password protection, and the ability to remote
wipe continue to be the recommended countermeasures, as it’s much better to be ahead of these
incidents than be behind the eight-ball. 53 Protecting the data and documenting the steps you have
taken to do so is likely the best you can do to avoid a painful post-incident series of events.

15% OF INCIDENTS STILL
TAKE DAYS TO
DISCOVER. INCENTIVIZE
YOUR WORKFORCE TO
REPORT ALL INCIDENTS
WITHIN A CERTAIN
NUMBER OF HOURS.

“Unencrypted,” “not encrypted,” and “without encryption” were present in the
OSINT four times more than “was encrypted,” “encrypted passwords,” and similar phrases.
“Should I encrypt my laptops and thumb drives?” Calibrated magic risk-ball says: “Without a doubt.”

There you are, sipping Mai Tais on the beach, enjoying a well-deserved respite after installing all
those shiny, new, advanced threat-mitigation devices at your perimeter, confident in your ability
to detect those pesky and insidious external attackers bent on stealing your corporate secrets.
You fire up your BlackBerry® only to be met with an e-mail subject line from a vendor that sends
shivers down your back: “What are you doing to combat the insider threat?!” Looks like it’s time to
get off the beach chair and back to work.

The Insider Misuse pattern shines a light on those in whom an organization has already placed
trust—they are inside the perimeter defenses and given access to sensitive and valuable data,
with the expectation that they will use it only for the intended purpose. Sadly, that’s not always
the way things work.
As with prior years, the top action (55% of incidents) was privilege abuse—which is the defining
characteristic of the internal actor breach. We see individuals abusing the access they have been
entrusted with by their organization in virtually every industry. And it’s all about grabbing some
easy Benjamins for these mendacious malefactors, with financial gain and convenience being the
primary motivators (40% of incidents), whether they plan to monetize stolen data by selling it to
others (such as with financial data) or by directly competing with their former employer. Coming
in a not-so-distant second is the motive of convenience (basically using an unapproved workaround to speed things up or make it easier for the end user), and while this is not something that
is intended to harm the organization, it certainly often has the same result.
This year, we saw more incidents involving the end user than ever before.

Finally, you know all those SOX, PCI, and internal audit-mandated controls that seem to single
out the dastardly system administrator? Well, either all of those controls are doing their job
and working perfectly, or you might want to consider rebalancing where you’re focusing your
potentially outdated control frameworks.


Catching insider abuse is not easy. You need to have a good handle on all the dependencies
(IT and otherwise) in your critical processes. Begin by identifying those core areas and then
look for activities to track or places where you need to insert additional auditing and fraud detection capabilities, so you can get ahead of the attackers.
In many of the incidents we reviewed, the insider abuse was discovered during forensic examination
of user devices after individuals left a company. Setting up a similar process in your organization
can at least tell you what happened. Though it might be too late at that point for damage control,
you may be able to apply some lessons learned to shore up gaps in your processes.

By focusing on the attributes and behaviors of these entities (e.g., your users and resources)
instead of coarse, simplistic threshold anomalies, you can compute risk scores down to the
user- or system-level rather than getting lost in a sea of event data, and narrow the gap
between insider abuse and successful detection.
For example, most developers on the same project have resource-access patterns that include
the same code repositories. Looked at as a whole, this forms clusters of access. When a
developer accesses a repository outside of his cluster, it creates a long, obvious relationship
that probably wouldn’t occur normally. When the developer then transfers an unusual volume
of data at an unlikely time, Interset uses machine learning to infer that she was up to no good,
even though any one of the indicators on its own could have been a false positive.

A company’s competitive edge in the market often derives from the quality of its
confidential and proprietary information.
In England, there are extremely powerful civil injunctions available that allow the aggrieved
party, without prior notice to the alleged data thief (similar to a search warrant in a criminal
law context), to search a defendant’s premises for hard-copy evidence and take copies of all
of their electronic devices on the relevant premises, including computers, phones, all e-mail
accounts (web or otherwise), clouds, and any other devices and data-holding platforms.
The injunctions also require deletion of the relevant stolen material after the devices have
been copied under the court order. They also provide for the non-use of the relevant data so
that even if the defendant fails in the deletion process, they are not allowed to use the stolen
data. If they do, they are in breach of the court order. Any breach of these types of court
orders can lead to a finding of contempt of court and consequently fines and imprisonment.
In all of the cases we have run using these nuclear remedies, we have essentially won the case
(either by settlement or trial) and retrieved the stolen data. And if the other side had the
capacity to pay the legal costs of the case, then those costs were paid in full or in part.
Similar remedies are available in British Commonwealth counties such as Australia, Canada,
Hong Kong, South Africa, and New Zealand. Certain E.U. countries also have similar remedies,
but mainly in an IP context. These remedies are not available in the United States and are
solely the domain of the law enforcement organizations.

Stephen Dedalus, a character in James Joyce’s Ulysses, says, “Mistakes are the portals of
discovery.” In the case of the DBIR, they are also the gateways to breaches. The globe spins,
people continue to make mistakes, and organizations suffer losses across the C-I-A triad as a
result. While the industries represented in this year’s data set mirror prior reports—largely due
to disclosure regulations for Public and Healthcare (just like the Physical Theft/Loss pattern)—a
new incident type has clawed its way into the top 10 error varieties.
As with years past, errors made by internal staff, especially system administrators who were
the prime actors in over 60% of incidents, represent a significant volume of breaches and
records, even with our strict definition of what an “error” is.

As with last year, due to government reporting requirements, the number of Public sector
breaches dwarfed all other data by an order of magnitude, and in the interest of trying to tease
out useful information to the broader set of industries, we removed the Public data from the
corpus for the rest of this analysis. Suffice it to say that government agencies send out heaps of
mailings that many times take a wrong turn at Albuquerque.
With the chaff filtered out, the new incident pattern we alluded to earlier made it into the top
10 this year. One-quarter of the errors last year were capacity planning issues. How is this a
VERISizable error, you ask? Say you’re the system administrator for a soon-to-be released online
multiplayer video game. Your company sold 10 million pre-orders, but requisitioned your five
online game servers at the local flea market. The chances of them holding up to the demand are,
well, not good. Service interruptions or performance degradation when infrastructure designed
for a normal day receives exponentially more traffic is not a surprising outcome.
Another example is what we’re calling the Law of Unintended Content Popularity. The Hacker
News/Reddit/Fark/Slashdot effect has been around for a long time, and availability losses due to
self-inflicted DoS or overloads of legitimate page visits are the end result.

According to One World Labs (OWL), an enterprise security assessment and consulting firm,
their team of threat intelligence analysts encounter publicly accessible FTP servers on a
daily basis. As part of the company’s Deep Web research process, which maps their clients’
digital and online footprint, OWL analysts are “tripping over” company and individual FTP
sites requiring no authentication. Even worse, many of these sites contain large volumes of
intellectual property and personally identifiable information (PII). OWL considers unsecured
FTP servers one of the greatest risks to company and individual data integrity.
Depending on the FTP servers' configuration, most can be accessed by web browsers, which
makes them a flexible and attractive vehicle for companies and individuals to remotely
access documents. Companies and individuals use FTP servers for a variety of reasons.
Some companies use FTP servers to share project documents between team members
working at different client locations. Users frequently use FTP servers to back up home
computers, and often unbeknownst to their employers, their work computers as well.
Examples of material found on a regular basis by OWL analysts in the course of their normal
duties include: Usernames and passwords for various accounts and enterprise hardware; Company documents marked “Proprietary” or “Confidential”; Proprietary software files; Partnering agreements; Individual tax documents; Individual medical records; Individual military service records;
OWL emphasizes the ease with which all of this data can be located. In many cases, a simple
Google® search can reveal millions of results from unsecured FTP servers. They note that
most of these issues could be remediated by the FTP owner simply requiring a username and
password to access the server and by disabling the anonymous login feature.
The inherent difficulty for OWL when finding this extremely sensitive material is the lack of
a defined and trusted process to notify the affected party, with whom there may be no
previously existing relationship. Past attempts to warn companies and individuals of their
data exposure were often met with skepticism, and in some cases, hostility. OWL
underscores the need for the information security industry to establish a process to educate
and warn parties of the dangers of unsecured FTP servers.
Understand where goofs, gaffes, fat fingers, etc., can affect sensitive
data. Track how often incidents related to human error occur. Measure effectiveness of current
and future controls, and establish an acceptable level of risk you are willing to live with, because
human fallacy is with us to stay.
Finally, learn from your mistakes. Was the root cause a combination of autocomplete in the “To:”
field and similarly named e-mail aliases? Did the staff member not have the understanding that
loan applications don’t go in the regular trash? Was the process to publish updates to the web
server built by Rube Goldberg and prone to misconfiguration? Those answers to your real-world
events will guide your specific countermeasures better than an industry report can.

The variety of data taken provides some explanation for the diversity. Secrets, credentials,
internal, and system data are taken, whereas in other patterns the primary goals were personal
information, Health records, and banking data. It seems these modern-day cyber Slugworths are
more concerned with the secret formula behind the Everlasting Gobstopper than they are your
Twitter password.

Before we point you in the direction of data you should be collecting and analyzing, the reality is
that if a determined, state-sponsored adversary wants your data, they’re going to get it unless
another state-sponsored entity helps you defend it.
Having said that, if you’ve got your own Gobstoppers to protect, start collecting data. Now.
Seriously. Put this report down and go set up your syslog servers. We’ll wait.
You back? Good. Now, specifically, start amassing e-mail transaction logs (in general), records
of attachments, and records of links in e-mails. Log all DNS web-proxy requests and invest in
solutions that will help you ingest and analyze this data both on the fly and forensically. Even if
you don’t manage to detect or deter these adversaries, you will at least have a much easier time
figuring out what they did after the fact.

Log all DNS requests
and log all web-proxy
requests, and invest in
solutions that will help
you ingest and analyze
this data.
On the other hand, we aren’t going to write pages and pages
of eloquent prose, only to end with, “Well, all that sure was depressing. kthxbai.”
We started by conducting a mapping exercise of the top 2015 threat action varieties to CSC
sub-controls. Not perfect, but starting with the most common attack methods, and finding the
controls that are designed to counteract said methods, is still a worthwhile effort, and the latest
iteration is available online.
The introduction of the incident classification patterns last year allowed us to make industryspecific recommendations based on the likelihood that your industry would be affected by a
particular pattern. Upon review of this year’s data, the changes were not statistically significant
in either the relationship between the industries and patterns, or within the attack methods
used to warrant a redo. If this is your first go-around with the DBIR, last year’s report is eagerly
awaiting you and would appreciate a click or two now that it’s no longer the new kid in town.
This year, we decided to focus our efforts on the incidents where we had the most detailed data.

The second reason that made this exercise a challenge was running into environments with
numerous gaps in their baseline security practices. Victims that have a web server vulnerable
to SQL injection, an open admin application login page, a flat network, and (to top it all off) no
logging to speak of make it very difficult to figure out which of these potential doors was kicked
in. In these cases, no attempt was made to hone in on a single control. In these circumstances, it
might even make sense to rebuild the entire organization’s security strategy from the ground up.

Without a really, really good understanding of the business culture and
all of the variables, a true root cause is hard to pin down.
The third reason was touched on above. In many of the cases, no information was available to find
the best control to disrupt the attack. A classic example is evidence of malware that did something
bad. Merely rubber-stamping “Get AV” is a very myopic thing to suggest in this exercise. Did they
have AV? Was it kept up to date? Did their vendor have a signature for that particular variant on
the day the infection occurred? How did the infection occur? Was the user baited into opening an
attachment? If so, should the e-mail attachment filtering have blocked it there?
I think you get the point, and it brings us to our first and most critical recommendation. Do this
stuff in your organization if you aren’t already. Learn from incidents and near misses, something
we have been preaching for years. Make use of the publicly available VERIS framework or collect
data in another structured fashion. Perform 5 Why–type assessments, as you are the person best
situated to do so. Use this report as a tool and source of information and in a supplementary role
to your own knowledge of your business and security practices.

NO. THERE IS TOO MUCH. LET US SUM UP.
We gathered up all the nuggets of mitigation wisdom from our reviews and tallied up the
percentage of incidents where a CSC control could be applied as the recommended strategy.

The results of this process actually reinforce things we’ve said in the past: Don’t sleep on basic,
boring security practices. Stop rolling your eyes. If you feel you have met minimum-security
standards and continue to validate this level of information, then bully for you! It is, however, still
apparent that not all organizations are getting the essentials right.

As the light of the new year dawned in 2015, the primary focus of the Verizon Cyber Intelligence
Center was discerning actionable intelligence surrounding the Retail vertical data breaches
at Target and Neiman-Marcus, which took place in late 2014. Risks to payment systems would
prove to be a recurring trend throughout the year. Reports of a breach at Target, stemming from
the loss of credentials at one of their vendors, would grow into a larger theme for many other
breaches during the remainder of 2014. January’s largest breach impacted 4.5 million users of
Snapchat, whose user names and phone numbers were compromised.

February kicked off with Kaspersky’s discovery of a zero-day attack using an Adobe® Flash® Player
vulnerability. Two weeks later, FireEye and Websense reported on Operation Snowman, which used
another zero-day, this one in Internet Explorer® (IE), on the websites of the Veterans of Foreign
Wars (vfw.org) and Groupement des industries françaises aéronautiques et spatiales (gifas.asso.
fr). Operation GreedyWonk used yet another Adobe Flash zero-day against the websites of two
national security/international relations organizations. As many as 5.6 million people who pledged
money through Kickstarter were the victims of the month’s largest reported breach.

The second zero-day IE vulnerability in as many months was discovered after going through
March’s patch-Tuesday bulletins; Symantec revealed it was used in a watering hole attack. GData
and BAE alerted us to the Uroburos/Turla/Snake campaign that would be in new collections every
month for the rest of 2014. Symantec attributed 2013’s biggest breaches to the Cyclosa threat
actor. Korean telecommunications company KT reported the first of 2014’s megabreaches to
affect that country, after the account information of 12 million customers was compromised.

If only Heartbleed had been an April fool’s joke. Alas, it became the first of three tumultuous
vulnerabilities in open-source software (OSS) we responded to last year. It is a vulnerability
in OpenSSL that enabled an attacker to steal 64 Kb of plaintext memory from a vulnerable
application. DLR, the German Space Center, Michaels Stores, and digital storage company LaCie
vied for the biggest breaches of April.

After skipping a month, zero-day attacks returned in May with FireEye’s report of Operation
Clandestine Fox and another unpatched IE vulnerability leading to an out-of-cycle Microsoft®
security bulletin only four days after the first report. Adobe also demonstrated agility when it
was compelled to patch another Flash Player zero-day used in watering hole attacks reported by
Kaspersky. The breach affecting the most users in 2014 was reported by eBay after attackers
used compromised credentials to access their database of 145 million customers.

The good guys collected their biggest win of 2014 in June with the disruption of the operation
behind the Gameover Zeus botnet and Cryptolocker ransomware. Later in the month, Microsoft
disrupted the NJrat/NJworm infrastructure. But a new banking Trojan, Dyre aka Dyreza, made
its appearance, trying to steal some of the spotlight from Zeus. A data breach at PF Chang’s was
probably June’s most high-profile breach after BAE’s report of a hedge fund breach on CNBC was
revealed to be vacuous.

In July, the Cyber Intelligence Center collected a bounty of detailed reports on sophisticated
threat actors and their attacks. Attacks on the Energy vertical by “Energetic Bear” were reported
by F-Secure, Symantec, CrowdStrike, RSA, FireEye, and Palo Alto Networks. “Pitty Tiger” was
outed by Airbus and McAfee. SWITCH.ch and Trend Micro reported Operation Emmental, a
complex attack on 34 banks using spear phishing and malware to defeat SMS-based two-factor
authentication. Samsung suffered a US$38 million loss from physical risks when a plant in Brazil
was robbed. Australian e-tailer Catch of the Day revealed the other large breach in July, but
offered no explanation as to why it was reporting a PII/PFI breach that occurred in 2011.

Early in August, we learned of Backoff POS malware that uses brute-force attacks on
remote access to payment systems. Cybervor, a small Russian crew’s collection of 1.2 billion
compromised credentials, seemed almost too fantastic to take seriously until it was tied to one
of the year’s most high-profile data breaches. Three significant beaches were reported in

UPS announced a POS malware breach at 51 of its stores, followed by unfounded speculation
Backoff was the cause. Community Health Systems disclosed a data breach involving the PII, but
not PHI or PFI of 4.5 million patients. And JP Morgan Chase reported it was responding to a data
breach that we later learned was discovered after following Cybervor bread crumbs.

September kicked off with the breach of hundreds of celebrity iCloud® accounts after their
credentials were compromised. The Shellshock bug in Bash was 2014’s second tumultuous OSS
vulnerability event, quickly eclipsing Heartbleed due to many more successful attacks. The
next high-profile breach report was caused by POS malware at Home Depot, affecting
56 million of its customers.

Zero-day attacks returned with a vengeance in October when Quedagh or Sandworm spun off
from BlackEnergy, attacking a new Windows® OLE vulnerability, and then CrowdStrike added
a new Kernel Mode driver attack distributing PlugX RAT. Adobe patched Flash Player, but the
notice that attacks were in the wild was delayed until November. October occasioned the third
huge OSS bug, POODLE, but we assessed that it was more smoke than spark. A gap in strong
authentication and compromised credentials was identified as the causes for the JP Morgan data
breach. The most high-profile breach was of unclassified White House networks, attributed to
Russian threat actors.

Flaws in Microsoft crypto implementations were the subject of many collections in November
after the Patch Tuesday SChannel security bulletin and an out-of-cycle bulletin for Kerberos
that could not have come at a worse time for the Retail vertical; contrary to popular predictions,
neither emerged as another Heartbleed. Adobe patched a Flash Player zero-day discovered in the
Angler exploit kit, along with one of last month’s zero-days. It seemed like intelligence about the
Regin espionage platform would bring the month to a close, until the data breach at Sony Pictures
Entertainment (SPE) rocketed to the top of the list of high-profile data breaches.

Adobe updated Flash for the fifth zero-day of the year. Another Cyber-Espionage campaign,
“Inception Framework,” was reported by Blue Coat and Kaspersky. December 2014 in the Cyber
Intelligence Center was very similar to December 2013—just swap in SPE for Target. We were
intensely focused on processing everything SPE-related to discern some actionable intelligence.
Trend Micro tied malware used to attack the Korea Hydro and Nuclear Power Co. to the SPE
breach. So raise a glass to turnings of the season—like last year, 2014 ended with focus
around a high-profile breach.

Denial-of-service
attacks, system
compromises, and other
incidents: Our report
on data breaches now
includes more than
data breaches.

All technologies have their benefits and liabilities.
Digital technologies are transforming the way in which
people interact with each other and employ machines,
and how machines connect to machines. Enormous
amounts of data are flowing and being stored; our world
increasingly depends on the internet and digitization to
be able to function. Vulnerability to theft of these data
has become one of the major drawbacks of financial and
other commercial transactions. The protection of data
and the secure functioning of the critical infrastructure
– such as energy, food and water resources, transport
and communications – depend on digital technologies
functioning safely and securely. Individuals’ privacy in
regard to, for example, medical records and insurance data
is still being breached to detrimental effect. This report,
while considering such situations, focuses on a far more
dangerous category of cyber attack – when a facility’s
industrial control systems are disrupted or even captured
and harnessed by saboteurs acting either inside or outside
the facilities where these systems are located.

The study found that the nuclear industry
is beginning – but struggling – to come to grips with this
new, insidious threat. The cyber risk to nuclear facilities
requires constant evaluation and response, particularly as
the industry increases its reliance on digital systems and as
cyber criminal activity continues its relentless rise.

Nuclear energy has been seen at different times from
differing perspectives as both a blessing and a curse. The
concerns about the health risks of ionizing radiation have
meant that the nuclear industry has developed a vast array
of safety and security measures to prevent the catastrophic
release of radiation, and to respond quickly and effectively
should such an event occur. However, no technology is
immune to accident, misjudgment or deliberate sabotage.
The 2011 nuclear disaster at Fukushima Daiichi as a result
of the overwhelming Tōhoku earthquake and tsunami is a
recent reminder of what can happen when basic prevention
protocols and upgrades are not followed through and –
perhaps more significantly – when the improbable is recast
as impossible and the duty to plan for the overwhelmingly
catastrophic is neglected. Yet the role of nuclear power
production in the energy portfolio of many countries
remains significant and in some regions is growing.

It is our intention that the findings of the research and the
proposals put forward in this report for dealing with the
cyber threat to the civil nuclear energy should be considered
in the spirit of assistance and engagement. The nuclear
industry is fortunate in having established regulatory
systems and international guidance. The Nuclear Security
Summit process, with the next meeting due in 2016, and
the role of the IAEA in addressing nuclear security and
nuclear safety in concert are mechanisms to ensure that this
important issue will start to receive more attention. The
nuclear industry, regulatory bodies, security establishments,
governments and international organizations need to engage
with cyber security experts and academics, on a sustainable
basis, to formulate robust policy responses through
coordinated plans of action to deal with the technical,
managerial and cultural shortfalls identified in this report.

The vulnerability of critical infrastructure has been the
subject of some study over recent years, but since the
revelation of the digital worm Stuxnet and the impact
it is understood to have had on the functioning of the
equipment in Iran’s nuclear program, many experts have
been concerned that similar attempts to interfere with the
physical workings of a nuclear power plant could prove to
be a severe risk. Indeed, as this report notes, there have
been a number of reported incidents of cyber interference
in nuclear power plants and – assuming that the nuclear
industry behaves in similar ways to other industries – we
ought to assume that these examples represent the visible
part of a much more serious problem.

Finally, as the report notes in conclusion, many of
the findings of this research are applicable to other
industries and sectors. Across societies, the wider critical
infrastructure – including power grids, transport networks,
maritime shipping and space-based communications assets
– is similarly vulnerable to cyber attack, with different but
potentially equally dire consequences. We hope that this
report will speak to those responsible for the safety and
security of this critical infrastructure and help to create
a culture of pragmatic dialogue between industry and
cyber experts, for the common good.

Recent high-profile cyber attacks, including the deployment
of the sophisticated 2010 Stuxnet worm, have raised
new concerns about the cyber security vulnerabilities of
nuclear facilities. As cyber criminals, states and terrorist
groups increase their online activities, the fear of a serious
cyber attack is ever present. This is of particular concern
because of the risk – even if remote – of a release of ionizing
radiation as a result of such an attack. Moreover, even a
small-scale cyber security incident at a nuclear facility
would be likely to have a disproportionate effect on public
opinion and the future of the civil nuclear industry.
Notwithstanding important recent steps taken by the
International Atomic Energy Agency (IAEA) to improve
cyber security across the sector, the nuclear energy
industry currently has less experience in this field than
other sectors. This is partly due to the nuclear industry’s
regulatory requirements, which have meant that digital
systems have been adopted later than in other types of
critical infrastructure. In addition, the industry’s longstanding focus on physical protection and safety has meant
that while these aspects of risk response are now relatively
robust, less attention has been paid to developing cyber
security readiness. As a result, exploiting weaknesses
in digital technology could be the most attractive route
for those seeking to attack nuclear facilities without fear
of interdiction.
The cyber security risk is growing as nuclear facilities
become increasingly reliant on digital systems and make
increasing use of commercial ‘off-the-shelf’ software,
which offers considerable cost savings but increases
vulnerability to hacking attacks. The trend to digitization,
when combined with a lack of executive-level awareness of
the risks involved, also means that nuclear plant personnel
may not realize the full extent of this cyber vulnerability
and are thus inadequately prepared to deal with potential
attacks. There is a pervading myth that nuclear facilities
are ‘air gapped’ – or completely isolated from the public
internet – and that this protects them from cyber attack.
Yet not only can air gaps be breached with nothing more
than a flash drive (as in the case of Stuxnet), but the
commercial benefits of internet connectivity mean that
nuclear facilities may now have virtual private networks
and other connections installed, sometimes undocumented
or forgotten by contractors and other legitimate thirdparty operators.
Meanwhile, hacking is becoming ever easier to conduct,
and more widespread: automatic cyber attack packages
targeted at known and discovered vulnerabilities are
widely available for purchase; advanced techniques
used by Stuxnet are now known and being copied; and
search engines can readily identify critical infrastructure
components that are connected to the internet.

By drawing on indepth interviews with 30 industry practitioners, as well as
policy-makers and academics, and convening three expert
roundtables, the project sought to assess the major cyber
security challenges facing the wider nuclear industry; to
identify international policy measures that could help to
enhance cyber security in the sector; and to help increase
knowledge of current concerns in this area. This report
examines the major cyber threats to civil nuclear facilities,
focusing in particular on those that could have an impact
on industrial control systems, and suggests some potential
solutions to these challenges.

The infrequency of cyber security incident
disclosure at nuclear facilities makes it difficult to
assess the true extent of the problem and may lead
nuclear industry personnel to believe that there are
few incidents. Moreover, limited collaboration with
other industries or information-sharing means that
the nuclear industry tends not to learn from other
industries that are more advanced in this field.
A paucity of regulatory standards, as well as limited
communication between cyber security companies
and vendors, are also of concern.
This suggests that the industry’s risk assessment
may be inadequate; as a consequence, there is often
insufficient spending on cyber security.
Developing countries may be particularly at risk,
because they have even fewer resources available to
invest in cyber security.
Cultural challenges
Nuclear plant personnel, who are operational
technology engineers, and cyber security
personnel, who are information technology
engineers, frequently have difficulty
communicating, which can lead to friction. In many
cases the problem is exacerbated by the off-site
location of cyber security personnel.
Nuclear plant personnel often lack an
understanding of key cyber security procedures,

Finding that the procedures documents produced by
cyber security personnel do not communicate this
information in language that is clear to them.
Cyber security training at nuclear facilities is
often insufficient. In particular, there is a lack of
integrated cyber security drills between nuclear
plant personnel and cyber security personnel.
Reactive rather than proactive approaches to
cyber security contribute to the possibility that a
nuclear facility might not know of a cyber attack until
it is already substantially under way.
This suggests that nuclear plants may lack
preparedness for a large-scale cyber security
emergency, particularly if one were to occur outside
normal working hours.

Many industrial control systems are ‘insecure
by design’, since cyber security measures were not
designed in from the beginning.
Standard IT solutions such as patching are difficult
to implement at nuclear facilities, mainly owing to
concern that patches could break a system and because
of the commercial need to reduce plant downtime.
Supply chain vulnerabilities mean that equipment
used at a nuclear facility risks compromise at any stage.

The cyber security threat requires an organizational
response by the civil nuclear sector, which includes, by
necessity, knowledgeable leadership at the highest levels,
and dynamic contributions by management, staff and the
wider community of stakeholders, including members of
the security and safety communities. The nuclear sector as
a whole, taking account of recommendations and guidance
issued by the IAEA, should take a strategic approach
that will: Develop a more robust ambition to match or
overtake its opponents in cyberspace and thereby
take the initiative, focusing its resources on critical
elements of the nuclear fuel cycle; Fund the promotion and fostering of cyber
security within the industry, aiming to encourage a
sectoral-level approach, from the highest levels down
to the individual; Establish an international cyber security risk
management strategy designed to maintain momentum and agility, incorporating the necessary mechanisms for in-depth preparation to meet cyber
security challenges, however these may arise, and a
flexible and coordinated response; Develop coordinated plans of action to address
the technical shortfalls identified, such as in patch
management, and make the necessary investments; Include all stakeholders in the organizational
response. This will require knowledgeable leadership
at the highest levels, the free flow of information and
dynamic contributions by management, staff and the
wider community of stakeholders, including members
of the security and safety communities.
Promote an environment that enables the
appropriate balance between regulated and
self-determined actions to avoid any tendency
for overall stagnation.

Develop guidelines to measure cyber security risk
in the nuclear industry, including an integrated
risk assessment that takes both security and safety
measures into account. This will help improve
understanding of the risk among CEOs and company
boards and make cyber security in the nuclear sector
more commercially attractive.
Promote cyber insurance, which will require strong
risk assessments, as an effective way to drive the
process of implementing change.
Handling the ‘human factor’
Engage in robust dialogue with engineers and
contractors to raise awareness of the cyber
security risk, including the dangers of setting up
unauthorized internet connections.
Establish rules where these are not already
in place – such as banning personal devices
from control rooms and requiring nuclear plant
personnel to change the default passwords on
equipment – and enforce these rules through a
combination of independent verification methods
and technical measures, for example by blocking
off USB ports.

Encourage nuclear facilities to share threat
information anonymously (such as by revealing
‘indicators of compromise’) in order to promote
greater disclosure, since the reluctance to disclose
cyber attacks stems partly from concerns for
damage to reputation.
Promote industry conferences and other measures
to enhance interpersonal relationships in order
to encourage informal sharing initiatives, even if
governments are dissuaded by national security
concerns from sharing threat information at the
international level.
Governments should lead the establishment of
national Computer Emergency Response Teams
(CERTs) specialized in industrial control systems,
particularly since they recognize that information sharing at a national level is key.
The regulator should reassure owner-operators
that they will not be penalized for any information
that they share, provided they show good faith.
Developing further international policy measures
Encourage all countries that have not yet done
so to adopt an effective regulatory approach to
cyber security at nuclear facilities. Since a large
number of countries follow IAEA guidance, allocating
more resources to the IAEA to enable it to develop
recommendations on responding to cyber security
threats could generate significant benefit.

Improve the frequency and quality of cyber
security training at nuclear facilities, potentially
involving accreditation of training programs by
the IAEA, and hold integrated scenario-led drills
between nuclear plant personnel and cyber security
personnel to hone skills and develop common
understandings and practices.
Promote the further creation of more cross disciplinary university programs aimed
at training cyber security specialists in the
nuclear industry.
Foster partnerships between vendors and cyber
security companies to enable the development of
more robust cyber security products.
Enhancing security – including the need for ‘security
by design’
Promote the importance of ‘security by design’,
so that future generations of industrial control
systems incorporate security measures during
the initial conception phase. This may mean
avoiding superfluous digital features as well as
incorporating authentication and encryption
technologies.
Ensure that sufficient redundancy is retained in
digitlized systems.
Promote the use of ‘whitelisting’, which
restricts the unprecedented flexibility of digitized
industrial control systems and also reduces the need
to patch systems.

Provide technical and funding assistance to
developing countries in order to improve cyber
security at their nuclear facilities.

Implement intrusion detection systems such
as network monitoring of traffic for anomalous
behaviour across the entire control system network,
not just on the network perimeter.

Encourage the further adoption of secure
optical data diodes.

Establish integrated projects between nuclear
plant personnel and cyber security personnel, such
as the preparation of cyber security training materials
and undertaking of joint vulnerability analyses.
This would also encourage IT personnel to visit the
nuclear facility in person on a regular basis to aid
mutual understanding.

Ensure the integrity of the supply chain.

Recent high-profile cyber attacks on nuclear facilities
have raised new concerns about their cyber security
vulnerabilities. This is of particular import because of
the potential – even if remote – for the release of ionizing
radiation as a result of a cyber attack. Given the sensitivities
surrounding the nuclear industry, even a small-scale cyber
security incident at a nuclear facility would be likely to have
a disproportionate effect on public opinion and the future
of the industry itself.
Meanwhile, cyber criminal activity is becoming ever easier
to conduct, and more widespread: automatic cyber attack
packages targeted at known and discovered vulnerabilities
are widely available for purchase, and search engines
can readily identify nuclear facilities and other critical
infrastructure that are connected to the internet. As states
and terrorist groups expand their online activities, the fear
of a serious cyber attack is ever present as well.
At the same time, nuclear facilities are increasingly
making use of digital systems, commercial off-the-shelf
software and internet connectivity – all of which provide
efficiency and cost-saving benefits but also make facilities
more susceptible to cyber attack. As these changes are
currently under way, personnel at nuclear facilities may not
realize the full extent of their cyber vulnerability. Some still
cling to the myth that nuclear facilities are ‘air gapped’ – or
completely isolated from the public internet – and that this
protects them from cyber attack. Yet not only can air gaps be
breached with nothing more than a flash drive but a number
of nuclear facilities have virtual private networks (VPN) or
undocumented or forgotten connections, some installed
by contractors.
The nuclear industry as a whole is currently struggling
to adapt to these changes. Notwithstanding important
recent steps taken by the International Atomic Energy
Agency (IAEA), the industry’s long-standing focus on
safety and physical protection has meant that while these
systems are now relatively robust, less attention has been
paid to upgrading cyber security. In addition, its relatively
late adoption of digital technologies means that it has
less experience than other sectors in this area. As a result,
exploiting weaknesses in digital technology may be a
particularly attractive route for those seeking to attack
nuclear facilities.
Other characteristics of the sector, such as the associated
national security sensitivities, make disclosure of cyber
security incidents that have occurred less likely, leading
nuclear industry personnel to believe that cyber attacks
are less of a threat than is actually the case. It also means
that the sector’s limited collaboration with others leaves
it unable to learn from those with greater cyber security
readiness. Furthermore, the rapid evolution of the threat
means that regulatory standards are currently inadequate.

As a result there is insufficient spending on cyber security,
including a lack of funding for agencies poised to deal with
the challenge.
All this suggests that the industry’s threat assessment or risk
calculation may be inappropriate, and that it is not investing
as much as it should in cyber security. Consequently the
cost–security equation may be out of balance. Developing
countries may be particularly at risk, because they have
even fewer resources available.
There are also significant issues in the culture of the
industry that contribute to the challenge. The different
priorities and ways of thinking of nuclear plant personnel,
who are operational technology (OT) engineers, and cyber
security personnel, who are information technology (IT)
engineers, frequently lead to misunderstandings. The
problem is exacerbated by the fact that cyber security
personnel are often located at a considerable distance
from nuclear facilities and rarely visit.
Furthermore, the level and quality of cyber security
training at nuclear facilities are insufficient: in addition to
a lack of cyber drills, nuclear personnel may have a poor
understanding of key procedures, in part as a consequence
of the cultural divide, since the training material is written
by IT engineers. Thus nuclear plants may lack preparedness
for a large-scale cyber security emergency, particularly one
that occurs after normal working hours.
There are numerous technical challenges too. Having
been designed in the 1960s when the idea that a malicious
actor would try to attack them was inconceivable, many
industrial control systems lack basic security measures such
as authentication and encryption, making them ‘insecure
by design’. Moreover, the flexibility of code means that
any attacker who can get past network perimeter defences
can make logic changes that are very difficult to spot.
And standard cyber security solutions used in home or
office IT environments, such as patching, are much more
difficult to implement in nuclear facilities. Supply chain
contamination is also a concern.
Growing recognition of the rapidly changing cyber security
scene led the International Security Department at Chatham
House to undertake an 18-month project exploring the
potential impact on and implications for the civil nuclear
sector. The project sought to assess the major cyber security
challenges and risks posed to nuclear facilities and nuclear
power plants in particular; identify international policy
measures that could help to enhance cyber security at
nuclear facilities; and increase knowledge and awareness
among both industry practitioners and policy-makers of
cyber security concerns in the nuclear sector. This report
focuses on the major cyber threats to nuclear facilities,
in particular on those that could affect industrial control
systems, and suggests potential responses and solutions.

The research took a fourfold approach: a literature review;
interviews with industry practitioners, policy-makers and
academics; a series of expert roundtable workshops at
Chatham House; and soliciting feedback from industry
experts at international conferences.
Literature review. Since the current literature on the cyber
security threats to nuclear facilities is relatively limited, the
project drew on a wide range of sources, including academic
publications, industry reports and news articles. The related
literature on the cyber security risks to critical infrastructure
was also consulted (see Select Bibliography).
Interviews. Interviews were conducted with 30 practitioners
(each referred to in the text as a numbered source) working
on cyber security and on nuclear issues in fields ranging
from industry to government, international organizations
and academia. Since the project’s remit was international,
interviews were conducted with experts from several
different countries, including the United States, United
Kingdom, Canada, France, Germany, Japan, Ukraine and
Russia, as well as representatives of major international
organizations, including the IAEA and the European
Network and Information Security Agency (ENISA). They
included both industrial control systems experts and IT
experts working in the nuclear field; a former manager at
two US nuclear plants; a former security manager at the

This chapter examines known cyber security incidents at
nuclear facilities and their consequences, and describes the
various threat actors ranging from hacktivists to states. It also
details the possible impacts of a cyber attack, which cover the
spectrum from the theft of commercial data to the release
of ionizing radiation.

Recent high-profile cyber attacks on nuclear facilities
have raised new concerns about the vulnerability of
nuclear power plants. In 2010, the emergence of the
Stuxnet worm heralded the advent of a new era in cyber
warfare. In a cyber attack on the Natanz nuclear enrichment
facility and Bushehr nuclear power plant in Iran, the
Stuxnet worm caused the partial destruction of around
1,000 centrifuges. This was the most highly
sophisticated publicly known cyber attack on a nuclear
facility to date, demonstrating an unprecedented level
of technical capabilities. On a lesser scale, South Korea’s
state-run nuclear operator was the subject of a cyber
attack in December 2014 which saw the theft of sensitive
information, including the blueprints of at least two nuclear
reactors and electrical flow charts.

As early as 1992, a technician at the Ignalina nuclear power
plant in Lithuania intentionally introduced a virus into the
industrial control system. He claimed this was in order to
highlight the cyber security vulnerabilities of such plants,
although this did not stop the police from arresting him. It also
illustrates the dangers of the insider threat – in this case little
harm was caused, but someone with malicious intent could have
provoked a serious incident. The hacking of a computer at the Ignalina nuclear
power plant in Lithuania could have resulted in a disaster
similar to that in Chernobyl. Slammer spread rapidly to computers
across the internet by exploiting a vulnerability in the Microsoft
SQL 2000 database server software. The worm scans and
sends itself to random IP addresses; if it reaches a machine that is running Microsoft SQL 2000, it infects that machine
and begins scanning and sending itself anew.
Slammer found its way to Davis-Besse by first infecting a
consultant’s network. From there it infected the corporate
network of First Energy Nuclear, which operates the plant. First
Energy Nuclear’s corporate network was connected directly
to a supervisory control and data acquisition (SCADA) system
at Davis-Besse so that it could remotely monitor the plant,
without any type of firewall. Once on the corporate network,
Slammer could thus make the jump onto the SCADA system.
It then generated a large amount of traffic that overwhelmed
the system. The safety parameter display system (SPDS), which
collects and displays data about the reactor core from the
coolant systems, temperature sensors and radiation detectors,
was unavailable for almost five hours.
Fortunately, Davis-Besse’s reactor was not in operation at the
time, but the same scenario could have occurred if it had been.
A patch for the Microsoft SQL 2000 vulnerability, which had
been released six months earlier, would have prevented infection
by Slammer, but neither the corporate network nor the SCADA
system had been patched.

In August 2006, the Browns Ferry nuclear plant in Alabama
experienced a malfunction of both the reactor recirculation
pumps (which use variable-frequency drives to control motor
speed and are needed to cool the reactor) and the condensate
demineralizer controller (a type of programmable logic
controller or PLC). Both of these devices contain microprocessors
that send and receive data over an ethernet network, but this
makes them susceptible to failure if they receive too much traffic.
(Ethernet functions by sending data to every device on the
network; the network devices then have to examine each packet
to determine if the packet is destined for them or if they can
ignore it.) At Browns Ferry, it seems that the network produced
excess traffic that caused the reactor recirculation pumps
and condensate demineralizer controller to fail. The plant’s
Unit 3 then had to be manually shut down in order to avoid
a meltdown.
Although this was not a cyber attack, the incident reveals
the impact that the failure of just one or two devices can
have on a plant. It also suggests that if a hacker were to
cause a recirculation pump to fail, it could seriously disrupt
plant operations. Such an attack mounted in combination
with infection by a worm like Slammer could disable not
just the recirculation pumps but also the sensors that warn
plant personnel of a problem – which would pose a serious
threat.

In March 2008, the Hatch nuclear power plant in Georgia
experienced a shutdown as an unintended consequence of a
contractor update. An engineer from Southern Company, the
contractor that manages the plant’s technology operations,
installed an update to a computer on the plant’s business network.
The computer was connected to one of the plant’s industrial
control system networks, and the update was designed to
synchronize data between the two. As a result, when the engineer
restarted the computer he had updated, the synchronization reset
the control system’s data to zero for a brief moment. However, the
plant’s safety system incorrectly interpreted the temporary zero
value of the water level to mean that there was insufficient water
to cool the reactor core, putting the plant’s Unit 2 into automatic
shutdown for 48 hours.
This demonstrates that nuclear owner-operators often do not
understand the full ramifications of connecting their business
networks to a plant’s industrial control systems. Although
in this instance the update’s unforeseen consequences did
not put the plant in danger (although it did trigger a costly
shutdown), it shows how a hacker might make a change to
a plant’s business network that, either unintentionally or
intentionally, could have a significant impact on industrial
control systems.

First exposed publicly in June 2010, the Stuxnet computer
worm infected both the Natanz nuclear facility and the Bushehr
nuclear power plant in Iran, partially destroying around 1,000
centrifuges at Natanz. It is believed to have been designed by the
US and Israeli governments and specifically targeted to disrupt
Iran’s uranium enrichment programme (Anderson, 2012).
The worm most likely spread initially when infected USB
flash drives were introduced into these facilities, which thus
became infected despite being ‘air gapped’ (i.e. fully separate
from the public internet).
Stuxnet infects computers that run the Microsoft Windows
operating system, taking advantage of vulnerabilities in the
system that allow it to obtain system-level access. (The worm
also makes use of falsified certificates so that the files it installs
appear to come from a legitimate company, thus deceiving
antivirus software.)
Once it has infected a machine, Stuxnet checks to see if that
computer is attached to a Siemens Step 7 SCADA system, as used
by Iranian nuclear facilities. If the computer is not attached to
such a system, then no payload is activated. Instead, Stuxnet
continues to replicate itself on other computers. One way it does
this is by taking advantage of another set of vulnerabilities in
print spoolers to spread to networks with shared printers. And
of course it continues to spread through USB flash drives.
If the computer is attached to such a Siemens system, then
Stuxnet’s payload is activated and it reprogrammes the system’s
PLCs, which control centrifuges used to enrich nuclear fuel, so
that they spin too fast and eventually break apart. At the same
time, it also sends false feedback to make it appear as if the
system is running properly (Falliere et al., 2011).
Stuxnet was aimed at preventing the acquisition of a nuclear
weapons programme, not causing an explosion or inflicting
civilian casualties, but its unprecedented capabilities show
the destructive potential of such technologies if used for more
nefarious purposes, and have heralded a new era in cyber attacks
as other countries race to develop offensive cyber capabilities.
Unnamed Russian nuclear power plant – Stuxnet (circa 2010)
Stuxnet is also believed to have infected a Russian nuclear power
plant during ‘the Stuxnet time’, around 2010. This incident was
revealed by Eugene Kaspersky, founder and CEO of Kaspersky
Lab, during a question-and-answer session after a 2013 talk. He
reported that a friend who was working at the nuclear plant at
the time told him that the plant’s internal network – which was
air gapped – was ‘badly infected by Stuxnet’. The plant has not
been identified.

As Kaspersky pointed out, this incident shows the unintended
consequences that state-sponsored malware can have. Even
though Stuxnet was narrowly targeted, it still infected at least one
other plant. He added: ‘Unfortunately, it’s very possible that other
nations which are not in a conflict will be victims of cyber attacks
on critical infrastructure’ (Kaspersky, 2013). This incident also
confirms that an air gap is no guarantee of protection.
Korea Hydro and Nuclear Power Co. commercial
network (2014)
In December 2014, hackers infiltrated and stole data from the
commercial network of Korea Hydro and Nuclear Power Co.,
which operates 23 of South Korea’s nuclear reactors (Cho, 2014).
The hackers gained access by sending phishing emails to the
owner-operator’s employees, some of whom clicked on the
links and downloaded the malware. The hackers obtained the
blueprints and manuals of two reactors, most likely belonging
to the Gori and Wolseong nuclear power plants, as well as
electricity flow charts, personal data belonging to some 10,000
of the company’s employees, and radiation exposure estimates

The primary set of threat actors that pose a cyber risk to
nuclear facilities can be divided into four broad categories:
hacktivists; cyber criminals; states (governments and
militaries); and non-state armed groups (terrorists).
Hacktivists such as radical fringe anti-nuclear power
groups might carry out a cyber attack on a nuclear facility
to raise awareness of vulnerabilities. Their goal is sabotage
or disruption, so such attacks would be likely to involve
defacements of websites or low-level attacks on the business
network intended to embarrass an operator rather than
cause a dangerous incident.
Cyber criminal groups are becoming increasingly
skilled. Organized criminal groups might steal confidential
information belonging to a nuclear facility and then
blackmail the facility into paying a ransom to prevent it
from being released. Their primary aim is monetary profit.
The threat from state actors ranging from intelligence
agencies to militaries and state-sponsored groups is on
the rise (McConnell et al., 2014). These types of attackers
tend to instigate long-term campaigns aimed at infiltrating
the critical infrastructure of other countries (Source
25). Currently the activities of states occur more in the
area of cyber espionage than cyber conflict. According
to Source 1: ‘At present, the motivations are primarily
commercial, aimed at the theft of sensitive, confidential for inhabitants in the surrounding area. The data were leaked
over Twitter from an account purported to belong to the head of
an anti-nuclear group in Hawaii; the hackers also warned Korea
Hydro and Nuclear Power Co. to shut down three reactors or face
‘destruction’. The owner-operator ignored the ultimatum, which
turned out to be an empty threat (Kim and Cho, 2014).
Further blueprints and test data were leaked over Twitter in
March 2015, with the hackers demanding money in order not
to release more data and intimating that other countries had
expressed interest in purchasing the data. Rather than responding,
South Korea issued a statement officially blaming North Korea
for the attack, citing as evidence that IP addresses used in the
phishing attacks were linked to the regime; North Korea has
strenuously denied the accusations (Park and Cho, 2015).
The incident illustrates the rise in extortion in the nuclear
industry. Those interviewed for the project have reported
that such incidents, while not often publicly known, are
relatively frequent.

proprietary data that will give the country an advantage.’
Yet in the longer term, the unintended escalation of cyber
skirmishes into cyber conflict is a concern. These same
infiltration campaigns are also aimed at acquiring cyber
capabilities against the critical infrastructure of other
states, including nuclear plants, in the event of a conflict. In such a case, the intent of an attack might be
to endanger human or environmental safety or, at the very
least, to create widespread confusion and fear among an
adversary’s population.
Terrorists or non-state armed groups are a growing
challenge. Some radical extremist groups have already
acquired significant capability in the use of social media
and, with sufficient financial resources, could develop the
capability to carry out a cyber attack on a nuclear plant or
employ a ‘hack for hire’ company to do this (BBC, 2011).
For example, ISIS (Islamic State of Iraq and Syria), with its
sophisticated use of Facebook and websites for recruiting
purposes, could potentially pose such a threat. According to
Source 10:
Radical extremism is also a serious risk, so we can consider it at
least equal [to a] governmental hack attack. If an attacker really
wants to penetrate or infiltrate the network, it is a question of time
and money.

Such groups might wish to build up a picture to support a
later coordinated attack intended to sabotage the plant or
to remove nuclear material. Or they might wish to use cyber
means in order to cause physical destruction.

There are a number of ways in which cyber attacks might
affect nuclear facilities. Some of the most important targets
are detailed below.
The most basic attacks will target business networks –
the corporate networks belonging to the owner-operators
of nuclear facilities that contain the information needed to
manage the business dimension of the plant. Most attacks
on these networks will be aimed at the theft of confidential
corporate data that can be used to garner financial
benefit. Others might be carried out for reconnaissance
purposes, to steal operational information that can be
used to conduct a more harmful attack at a later date.
Or, as business networks are typically connected to the
nuclear facility, some attacks on business networks could
serve as a route for attacks on the facility’s industrial
control systems.

A cyber attack that took one or more
nuclear facilities offline could, in a very
short time, remove a significant base
component to the grid, causing instability.
More sophisticated attacks on nuclear plants involve the
targeting of industrial control systems themselves and
have the potential to be the most harmful. Within the plant
itself, the industrial control systems are the most important,
notably SCADA systems. While highly complex, these can be
thought of as having just three parts. The first consists of the
computers that control and monitor plant operations, and
that send signals which physically control the second part.
This comprises the field devices, such as programmable
logic controllers, which control the sensors, motors and
other physical components of the plant. The third part
consists of the human–machine interface (HMI) computers
which display user-friendly data on operations and often
run using Windows programs.
Some possible attack scenarios might include the following.
A cyber attack on a nuclear plant could cause a
widespread loss of power. Nuclear reactors using water
in their primary cooling circuit are designed to give a high
level of protection to that water, but the water supply that
cools the turbines which in turn generate the electricity is not so well protected. Without that water supply, the
turbine could be tripped and electricity generation halted,
with a serious impact on the power grid. In countries that
rely on nuclear energy, power provided by nuclear plants
is considered to be the ‘base load’, or a steady and constant
source of supply. Other sources of power generation, for
example gas-fired electricity generation, can be more
responsive to demand and so can be adjusted to meet
peaks in demand and to reduce supply when there is a
lower requirement for power. Thus a cyber attack that took
one or more nuclear facilities offline could, in a very short
time, remove a significant base component to the grid,
causing instability. According to Source 27:
In the US, it’s very easy to have this ripple effect because if those
plants go off the grid quickly enough, it’s a pretty significant
percentage of the grid’s base load that all of a sudden disappears,
which causes the entire grid to become burdened. If you did that to
a reasonable number of those larger substations, you could cause
a significant grid event.

The consequences of a loss of power could be severe.
In theory, a cyber attack on a nuclear plant could bring
about an uncontrolled release of ionizing radiation.
An adversary with sufficient technical knowledge and
adequate resources could mount an attack on a nuclear
power plant that could trigger the release of significant
quantities of ionizing radiation. All nuclear power plants
need offsite power to operate safely and all have a standby
generator system which is designed to be activated when
a loss of mains power occurs. Attacks on the offsite power
supply and the on-site backup system could create some
of the effects that occurred following the 2011 earthquake
and tsunami at Fukushima Daiichi, although multiple
failures of the many safety features at modern nuclear
power plants would also need to occur at the same
time as that loss of offsite power and the disruption of
standby generators.
The risk of simultaneous attacks is also a concern. It is
possible that different types of attacks could be launched
simultaneously against a nuclear plant: for example, a
cyber attack might be planned to occur concurrently with
a physical, perhaps armed, intrusion on the same plant.
Alternatively, there could be a concerted simultaneous
cyber attack on a nuclear facility and on other types of
critical infrastructure such as regional water systems,
the electrical grid or banking systems.

The changing nature of the cyber security threat appears to have
prompted some countries to begin overtly and rapidly preparing
defensive and offensive capabilities in the event of a future
conflict that includes cyber attacks between states (McConnell
et al., 2014). If such a conflict were to occur, nuclear power
generation plants could well be prime targets.
Potential attackers are likely to be states, but it is possible
that non-state armed groups with sufficient financial and other
resources may turn to cyber attacks. Cyber defense requires
significant financial and intellectual investment, and states
that lack such resources or are reluctant to commit them may
become increasingly vulnerable to attack.
Attackers would face two challenges. First, their cyber attack
would have to be tailor-made for the specific target plant, which
would require knowing exactly which software programs run
the control systems. Obtaining this information is difficult and
time-consuming. It would involve reconnaissance of the plant
in advance, stealing passwords or obtaining insider intelligence.

Second, they would need a customized test-bed – which would
be very expensive – to trial such a weapon. At present only the
most advanced states have this capability.
A state-sponsored cyber attack on a nuclear facility of another
state would attract widespread international condemnation and
invite reprisal. This would deter most states – although not all.
The difficulty of attribution means that the perpetrator might
not be identified.
It is not like you are having tanks coming over the hill, or
you’ve got soldiers in uniforms flying flags; the reality is we’re
seeing a real increase in the use of false flag operations. Attacks
that appear to be coming from somewhere are actually coming
from somewhere else. (Source 26)

In the midst of a wider, physical interstate conflict, however,
cyber attacks against a range of critical infrastructure including
nuclear power plants would have to be considered possible, and
even probable. Numerous countries are rapidly acquiring cyber
capabilities to attack critical infrastructure, and nuclear plants
could become targets of choice in an all-out attack.

This chapter describes the growth of the cyber security
challenge in the nuclear industry. There are ever more tools
and services that make it easier and cheaper for hackers to
attack industrial control systems, including at nuclear facilities:
search engines can readily identify critical infrastructure that
is connected to the internet, techniques from Stuxnet are
being copied, and automatic cyber attack packages targeted
at known and discovered vulnerabilities are widely available
for purchase. In parallel, there is a rise in factors that make
nuclear facilities more vulnerable to cyber attack, with
facilities increasingly adopting digital systems, making use
of commercial off-the-shelf software, and connecting to the
internet. All of these offer considerable cost savings but are
easier to hack.
Because of this rapid evolution, nuclear facility personnel
do not necessarily understand the extent of facilities’
vulnerability to cyber security threats. Many still cling to
the myth that nuclear facilities are ‘air gapped’ – and that
this protects them from cyber attack. Furthermore, nuclear
personnel may not always realize that nuclear facilities may
have internet connectivity: VPN connections are increasingly
used, and there are sometimes undocumented or forgotten
connections installed by contractors and other legitimate
third-party operators without malicious intention.

A number of factors have made it easier and cheaper
than ever for hackers to attack critical infrastructure,
including nuclear facilities. The growth in specialized
search engines for internet-connected industrial
systems is one such element. For example, the search
engine Shodan, which allows users to search for and find
SCADA systems that are connected to the internet, has
grown in popularity. According to Source 25:
We did research in which we used Shodan and found all of the
nuclear plants in France that are connected to the internet. If a user
knows what he is looking for, he could easily find this information.

Specifically, Shodan’s geolocation capability can display the
location of the identified SCADA systems on a map. Taking
this with known facts such as the location of nuclear plants
in France, it is entirely possible to correlate the two datasets
and to determine which of those identified SCADA systems
are at nuclear facilities. The basic version of the search
engine is free to use, while more extended searches are not onerous. Another search engine, ERIPP, is very similar
to Shodan but concentrates on critical infrastructure.
Once the user has identified the internet-connected
systems at a nuclear facility, it may be possible to take
advantage of default passwords to gain access. Some
nuclear facilities do not change the default passwords
on their equipment, yet those used by companies such
as Honeywell and Siemens are widely shared on hacker
websites. ‘You know that for company X, the default
password is always, say, 1234, so you can get in that way,’
comments Source 25. Thus hackers can often gain access
more easily than managers of nuclear facilities expect.
Another element is that cybercriminals are now able to
copy the advanced techniques used by Stuxnet. Stuxnet’s
tactics, which could only have been developed by a team
from a highly advanced state, are now known to less skilled
hackers who would not have had the capability to develop
such sophisticated malware on their own. Once Stuxnet’s
existence became publicly known, hackers around the
world took inspiration from the way it functioned and
incorporated some of its features into malware to suit their
own purposes. The same techniques could be adopted to
launch attacks on other nuclear facilities (Simonite, 2012).
Moreover, the increased availability of automated
exploit toolkits is making it easier for hackers to attack
industrial control systems (Zetter, 2014). For example, open
source toolkits such as the Metasploit Framework – which
is free to use – enable users to use and execute any exploit
combined with any payload in order to test a system for
vulnerabilities.2 The framework was originally designed to
automate the process of penetration testing, but hackers can
now use these same exploits to attack a system by simply
replacing the payload with a malicious one. In the past, they
had to develop their own tools, so only a small number of
highly skilled hackers were able to attack industrial control
systems. Now automated exploit toolkits not only make it
easier for less skilled hackers to engage in such attacks, but
also automate the process.
Furthermore, an increasing number of companies are
selling zero-day vulnerabilities3 and exploits that take
advantage of these. Rather than reporting the vulnerabilities
to the software vendors so that they can be patched, they are
selling them to governments and to other paying customers
instead. As an example, ReVuln, a company based in Malta,
specializes in selling zero-day vulnerabilities for SCADA
systems. This type of activity is currently not illegal because
such companies are operating in unregulated ‘grey markets’.

An exploit is a software tool that takes advantage of a vulnerability in a computer system; the payload is the malicious code that it installs.
Zero-day vulnerabilities are gaps in computer security that are unknown to anyone except the researcher who found them; that is, they have been known about
for zero days.

At the same time, several factors are increasing the
vulnerability of nuclear facilities to cyber attack. One is
the increasing use of digital systems, which are more
susceptible to cyber attack. Many nuclear plants were built
in the 1960s, 1970s and 1980s, and are primarily legacy
analogue systems comprised of hardware and software
designed during those decades. Although much of this
older equipment lacks important cyber security features,
the systems have, for many years, provided a certain form
of ‘protection by antiquity’. In the years before the arrival
of microprocessors, systems were hardwired, which means
that a computer’s logical functions were carried out by
circuits that were permanently built into devices, instead
of by programmable code. They had so little flexibility that
any attacker wanting to change a device’s function would
have had to go to the device and make a physical change
to the circuit. And many of these older systems predate
networking, so network-based attacks are not possible.
As older equipment in existing facilities reaches the end of its
working life and needs replacement, comparable equipment
is no longer manufactured or available, and so it is gradually
being replaced with newer hardware (and software) that has
more digital features. Now, the use of programmable code
means that an attacker can simply change the code in order
to change the function of a device; digital systems allow an
unprecedented amount of flexibility, which makes them more
susceptible to cyber attack.
With the advent of the microprocessor, there are so many degrees
of freedom you can do anything you like. Before that, systems were
hardwired. What they did was built into the design and so there
was not much flexibility. That means that there was very little scope
for subverting them or for them doing the wrong thing. (Source 5)

Furthermore, these new digital systems have been
conceived without adequate security protection, making
them ‘insecure by design’ (this concept is discussed further
in Chapter 6). Thus new industrial control systems and newbuild nuclear power plants dependent on these digitized
technologies are more susceptible to cyber attacks that
exploit these weaknesses.
There is more and more automation coming into the nuclear
industry because of obsolescence. So there are more and more
cyber-sensitive systems being installed. The problem is these
systems often haven’t been adequately designed. (Source 8)
Whatever technology is currently available will provide the raw
materials for what gets put around these reactors, and there are
some fundamental problems with the way that the digital system
that we have got is engineered. It is not actually a sound basis on
which to build safety-critical systems. (Source 5)

In addition, there appears to be some reduction in the level
of redundancy (the addition of extra critical components
or functions to provide backup should a component fail)

currently existing in nuclear facilities. A key redundancy
requirement is for fail-safes, which ensure that if a system
should fail, it does so in a safe manner. As nuclear facilities
gradually convert from analogue to digital, fail-safes are
losing part of their efficacy: since the digital systems are not
independent, there is no longer a genuine redundancy.
Fail-safes used to be hardwired, analogue, completely set aside
from anything else; you wanted to make absolutely sure they
would work. But with the microprocessor, it is now cheaper to
incorporate both control and safety in the same device. We are
losing redundancy.

Another factor is the increasing use of commercial
off-the-shelf systems that are easier to hack. The nuclear
plants built between the 1960s and 1980s run highly
customized SCADA systems. The large number of vendors
meant that systems, computer languages and proprietary
protocols varied widely from plant to plant. This provided
‘protection by obscurity’. Attacking such individualized
systems is difficult: hackers would first need to acquire
specific knowledge of a SCADA system’s particular
characteristics, which might require insider information;
then they would have to identify vulnerabilities in order
to write and deliver exploits to take advantage of these.
And they would have to do this for each plant they
wanted to attack.
Since the 1990s, facilities have been increasingly
integrating their SCADA systems with computer networks
built from commercial operating systems such as Windows
or Linux, manufactured by a small number of vendors. This offers cost savings and greater
efficiency, but the growing use of these operating systems
in a large number of industries across the world means that
hackers are already familiar with their vulnerabilities and
previously written exploits that they can use. Hackers are
thus able to attack nuclear plants with far less effort and
a much greater chance of success.
Furthermore, the use of ‘air gaps’ is declining at
nuclear facilities, which opens up new vulnerabilities
for cyber attack. Traditionally, being air gapped, or fully
isolated from the public internet, has formed the mainstay
of nuclear facilities’ defense against cyber attacks. This
literal gap of air between the nuclear plant and the public
internet provided a form of ‘protection by isolation’.
Yet in recent years many nuclear facilities have gradually
developed some form of internet connectivity. This is in
large part because legitimate third parties located offsite (and often some considerable distance away) need
access to data generated at the plant. Owner-operators are
increasingly opting to use the internet to transfer such data
because it is the most efficient way of doing so; they find it
too slow and cumbersome to download the data onto a USB
drive which is then sent to those who need it.

In most cases, third parties only need to download data from
the plant. However, there may also be instances in which
they wish to upload data – for example, vendors may wish
to undertake software updates remotely. Since each facility
has several suppliers, this means that a large number of
actors may need to connect to nuclear facilities.
While you see the notion of the air gap in the literature. It used
to be true but in practice it is less and less the case. Today there are
many third-party vendors that want remote access to do updates
and monitoring.
Most equipment manufacturers can remotely monitor a device
for problems. And all of the plants have several suppliers, so that’s
a lot of people who are connecting.

Infection via known connections to the internet
Nuclear facilities that allow third-party remote access
may open up several new avenues by which hackers can
gain access. The owner-operator’s commercial network
can serve as a route of infection. Owner-operators are
increasingly creating direct links between their corporate
business networks and facilities’ industrial control system
networks. In many cases, the plants will employ optical data
diodes, which allow unidirectional communication (i.e.
allow data to flow outwards but not inwards) by beaming a
laser through a fibre optic cable from inside the plant to an
external receiver. The receiver detects the light and converts
it into data form; it has no ability to transmit data back,
making the system nearly impossible to breach (except
perhaps by a highly advanced state actor).
In other instances, however, these links may not be
adequately protected and a hacker may be able to use the
corporate business network to gain access to the nuclear
facility’s industrial control systems. For example, some nuclear facilities may only be using a firewall (which
controls incoming or outgoing traffic according to a set
of rules), configured so that it only allows traffic to flow
outwards, to protect the industrial control system network.
Yet it would be relatively straightforward for a hacker to
modify the firewall settings and gain access.
As standards vary from country to country, so will the technologies.
The companies that are aware of the need to do the right thing
implement data diodes. But not everybody implements data
diodes, which means that there is room for interpretation with
the regulations as you go from country to country.

Virtual private networks can also provide one possible
route of infection; some plants are permitting vendors
to access facilities remotely through a VPN connection,
which allows individuals to connect to a private network
over the internet via a secure encrypted tunnel. If the VPN
is insecure, however, it can be a source of vulnerability,
making it possible for malware to find its way onto the
industrial control network.
For example, if VPN access is allowed to the digital reactor
protection system, which is the system that shuts down the
reactor in the event of a safety concern, a hacker could gain
access to and compromise the reactor protection system,
triggering a plant shutdown – or, worse, preventing a plant
from shutting down in response to a safety alert.
There are some countries that allow remote access for the vendors
to the digital reactor protection systems. And if a hacker knows
that, he has an entry point.

VPNs can also be an avenue for unintentional infection of a
facility. As noted in Box 1 above, at the Davis-Besse nuclear
plant in 2003, an engineer working for a subcontractor
connected from his home laptop via a VPN to his company,
and that company had a site-to-site VPN with the nuclear
plant. His home laptop was infected with malware, infecting
the facility and causing a monitoring system to crash.
Fortunately, Davis-Besse was shut down at the time, but as
this is a standard way of providing remote access, the same
scenario could be repeated elsewhere (Kesler, 2011).
Infection via undocumented connections to the internet
Often, nuclear facilities will have undocumented
connections to the internet (i.e. connections of which the
plant managers or owner-operators are unaware); these too
can provide potential pathways through which malware can
infect a nuclear facility.

Instead of using VPNs, nuclear facilities could build a private cable connection to third parties, but this would be prohibitively expensive, particularly given the
number of third parties that would want access. And even a private cable connection could be tapped, although this would possibly require state-actor capabilities and
physical access to the cable.

For this reason, network diagrams of nuclear facilities that map out existing connections are frequently incorrect; there are often a number of additional connections that have not been documented.

In some cases, contractors or employees might set up
rogue or unauthorized connections. For
example, even though wireless connections are generally
strictly forbidden at nuclear facilities, a contractor might
– for reasons of convenience – install a wireless network
in the office without informing systems administrators. If
that wireless network is not adequately protected, a hacker
could access (or malware might infect) an industrial control
system through the office network wireless.

In some instances, contractors or employees might
inadvertently install equipment that has internet
connectivity. For example, when a part wears out in a facility,
a contractor might replace it with a new part that could have
exactly the same serial number as the old part, leading the
contractor to believe that it is exactly the same. Yet the vendor
might have added a Wi-Fi or GPS functionality that provides
a mode of access for a hacker (or malware).

A related concern is that contractors or employees
might install temporary internet connections and then
forget about them. For example, a contractor providing
maintenance might add a router and use it only once for
a specific purpose; then if it not removed once the task
is finished, its installation could easily be forgotten and
that internet connection could provide an avenue for
infection.

The maintenance team can be part of the problem. How do we
prevent somebody from putting in a wireless access point and
plugging it in? It’s basically to make their lives easier.

Even when nuclear facilities are air gapped, there are still
a number of possible routes of infection; while an air gap
does reduce a facility’s vulnerability, it does not provide
complete protection. For example, malware can infect a
nuclear facility when a USB drive or other removable
media device is plugged into the plant network. If the
removable media device contains malware, it can spread
to the plant itself. This was the most likely route by which
the Stuxnet worm infected the Iranian nuclear facilities at
Natanz and Bushehr, which were both air gapped.

As discussed previously, given that nuclear facilities will
always have a need to either download data (e.g. to get data
off the plant) or, perhaps less frequently, to upload data
(e.g. to perform a software upgrade), this will require the
use of removable media devices in the case of air gapped
facilities – thus breaching the air gap.
Information still needs to flow inbound periodically, whether you
use a USB or something else, and that’s where problems can occur.

The DragonFly cyber espionage campaign – also known as
Havex or Energetic Bear – which targeted US and European
energy companies (although there are no reports of nuclear
facilities specifically having been infected) provides an
example of how malware can be introduced via software
updates. The attack infected the software updates of SCADA
equipment manufacturers with a Trojan Horse malware
program. When energy companies installed these software
updates, the malware spread to facilities’ industrial control
systems, giving the hackers backdoor access.

There appears to be some element of denial. Some nuclear
facility personnel may view cyber conflict as occurring
between a small number of advanced states rather than as a
threat that concerns them.
In other instances, nuclear plant personnel might, for
reasons of convenience, bring their own USB drives or other
removable devices into a plant and use them to transfer
data, thereby providing an opportunity for malware to cross
the air gap.

Furthermore, many in the industry are also skeptical about
the potential for a release of ionizing radiation to occur as
a result of a cyber attack; a number of those interviewed
asserted that it just would not be possible.

Air gaps work in theory, but not in practice. All it takes is a
USB drive: people walk into the plant room, plug the USB into
the system, and the malware is on there. All of a sudden it has
jumped the air gap.
If you allow in a USB key, which breaches the air gap, you’ve now
got a connection that nobody really considered. And since there
is [often] no security software running on any system machines,
malware is free to do whatever it wants.

Despite the demise of the air gap within many nuclear
facilities today, a number of nuclear plant personnel and
even owner-operators of facilities may not necessarily
realize that their nuclear facilities have internet
connectivity – or fully understand its implications.
We have this tired cliché that industrial control systems aren’t
connected to the internet. But search engines like Shodan have
proved that they are.
Many nuclear operators say that their facilities are not connected
to the internet, so there is no risk.

Differences in terminology may contribute to this
confusion. While the ‘true’ definition of an air gap is a
literal gap of air between a nuclear facility and the public

Some degree of complacency on the part of nuclear
plant personnel – such as engineers or contractors setting
up rogue or unauthorized connections, as described above
– may be due to their not being fully cognizant of the cyber
security risks. Some of it is also attributable to human
nature, which often seeks out shortcuts. Some further
examples of poor IT practices at certain nuclear facilities
are described below.
Many infection problems stem from the use of
personal devices at nuclear facilities, including directly
connecting personal computers to industrial control
systems. Everyone is suffering from BYOD within
industrial environments. In some countries it is common
practice to bring personal computers into nuclear facilities,
where they provide an avenue for virus infection. Source
6 describes how in some US facilities, engineers regularly
bring in their own personal computers in order to run tests
and plug them directly into the computer interface of the
PLC. For example, in order to assess how a controller is
working, an engineer might dock his or her laptop into it
and download data on how the device is functioning. If the
engineer’s personal computer is infected with malware,
this will infect the PLC in the process.

Many computer control systems have PLCs. You can introduce
viruses or other malware into a PLC – and we have. Engineers are
usually the worst offenders. Often, they will bring their own laptops
in, and want to take data off a machine. Lots of times they have
introduced viruses in the PLCs when doing tests.

Such actions can have severe consequences. However, in the process he introduced a virus into
the controller that caused the turbine to overspeed. This,
in turn, can cause the reactor to overheat. In this instance,
it triggered the fail-safe – but one could imagine a cyber
attack in which the fail-safe was compromised.

In some countries it is common practice
to bring personal computers into nuclear
facilities, where they provide an avenue for
virus infection.
Compounding the problem, in some instances
engineers may leave their personal computers
unattended in the control room, where they are liable
to being accidentally infected or to infect other devices.
Source 6 comments: ‘Sometimes engineers will leave their
computers sitting in control rooms, I’ve seen them.’ Since
running a series of tests can take as long as 10 hours,
engineers will exit the control room for certain periods of
time but will leave their computers there unattended. (Of
course, they will unhook them and shut them down.) But
there might be 70 or 80 other people in the control room.
‘I’ve actually seen guys come in, start them up and sign
into their email,’. This of course opens up a
pathway for that computer to be infected via a malicious
email. In other instances, engineers might have taken data
off an industrial control system device and need to upload
it to a computer. They will plug in the zip drive that they
used to take the data on. At that point, however, if the
computer is infected it could infect the USB drive that
is used to take data.
The failure to change default passwords is another
challenge at nuclear facilities. In some instances, nuclear
facilities fail to take basic ‘good IT hygiene’ security
measures, such as changing the factory default passwords
on equipment. Manufacturers typically use a simple default
password, which is intended to be replaced. The use of default vendor login details is everywhere,
including in nuclear. You just put these in and you can get
access to the networks.

While some of these risky situations arise from a lack
of rules banning a particular practice, in other instances
rules are in place against such an action but the challenge
instead is their insufficient enforcement. For example, the
use of smartphones is typically not authorized at nuclear
facilities. Engineers might plug their personal smartphones directly
into a control system computer in order to charge them;
given that these devices lack antivirus software, they are
particularly vulnerable.
If you look at most corporations’ policies, they would forbid the
introduction of personal mobile devices like smartphones in secure
environments. But the problem is they don’t enforce it.

Overall, it appears that while nuclear facility operators
are extremely rigorous about enforcing rules that
pertain to physical safety and security, they may be less
so when it comes to rules that concern cyber security.
Operators are really rigid about obeying and enforcing rules. But
as rigid as we are about procedures, a lot of the time we are not as
rigid about cyber security. One thing operators don’t do religiously
is have somebody from the IT department check for viruses. And
the work requests usually do not require the engineer to run a virus
scan on the machine prior to connecting it, either.
On a standard issue, if we have a procedure that says valves x and
y need to be open, we usually send two people to do that and then
a third person to check. When it comes to cyber, they’ll make sure
that the computer is hooked up to the right hub, but they don’t have
anybody check to make sure that the computer you’re hooking up is
the one we bought for it and not your own, or that you didn’t plug it
in anyplace else. They tell you that they do but they don’t. (Source 6)

In other instances, the danger might not arise from
inadvertent infection but from deliberately malicious
motives. The ‘insider threat’ has long been recognized
within the nuclear industry as a threat to safety and security
and is also a concern from the cyber security perspective.
An insider would have the opportunity to insert a USB
drive or other removable device into a facility to introduce
an infection. Russian agents have
infiltrated nuclear plants in Ukraine:
I think they have an agent in each plant; it is a priority for them
to have people in Ukrainian nuclear plants.

The low wages in Ukraine make workers particularly
vulnerable to recruitment by Russian agents, as does the
fact that part of the population has loyalties to Russia.
Many government clerks or public employees just want their salary,
and they don’t want change – the old generation is used to taking
bribes, and of course there is a very big lobby for Russian interests.

This chapter examines some of the specific challenges
currently facing the nuclear industry as it begins to grapple
with the cyber security threat. Notwithstanding important
recent steps taken by the IAEA to improve cyber security, the
nuclear industry currently has less experience in this field than
other sectors. This is partly due to the industry’s regulatory
requirements, which have meant that digital systems were
adopted later than in other types of critical infrastructure.
Moreover, the industry’s long-standing focus on physical
protection and safety has meant that while these systems are
now relatively robust, less attention has been paid to upgrading
cyber security. As a result those seeking to attack nuclear
facilities can more easily exploit the weaknesses in digital
technology outlined in the previous chapter.
There is increasing evidence that the industry’s threat
assessment or risk calculation is inadequate, and that it is not
investing as much as it should in cyber security, leading to
an unbalanced cost–security equation. Developing countries
may be particularly at risk, because they have even fewer
resources available.

with formulating and implementing the agency’s nuclear
safety and security program. The department’s activities
are aimed at protecting people and the environment from
radiation exposure, and it responds to the safety- and
security-related needs of its member states.
However, because cyber security concerns only came to the
fore in recent years, the industry has a natural tendency to
consider the cyber threat to be relatively low compared
with other safety and physical security threats.
Within nuclear, safety will always win because, when safety goes
wrong, someone gets injured or killed, and with that you get
litigation and cost. With security, if you get a security breach, there
is a very slim possibility you might get reprimanded or sacked, but it
is very unlikely. That is why safety is the prime concern and always
will be.

In the aftermath of the 9/11 terrorist attacks on the United
States, the nuclear industry invested heavily in physical
protection of facilities and materials, with the IAEA promoting
major improvements in physical security. In this area, the
industry has now reached a high level of security (referred to
as ‘gates, guards and guns’). However, this very robustness
may in itself make the cyber route a particularly attractive
alternative for those seeking to cause damage, as it is now
seen as the ‘soft underbelly’ of the industry.

The nuclear industry is concerned with both safety and
security, and lessons learned from major incidents involving
the release of radioactive material have created a culture in
which safety is paramount. Although in certain cases safety
and security can overlap, there are significant differences
between them.
Safety can be broadly viewed as protection against
accidental or unintentional incidents. In the nuclear
context, the term only includes those incidents that might
result in a release of ionizing radiation. The IAEA thus
defines safety as the necessary measures to protect people
and the environment from accidents that could involve
undue radiation hazards.
Security involves protection against malicious or intentional
acts. Traditionally, this was rooted in theft protection
and physical breach of facilities, although the context
has evolved to be much broader, and now includes cyber
security. The IAEA defines it as the prevention or response
to theft, sabotage, unauthorized access or other malicious
acts involving nuclear material or their associated facilities.

The nuclear industry’s late adoption of digital systems
has resulted in a lower level of cyber security experience
than in other industries. There were several reasons for
this delay. The very high costs of running nuclear power
plants mean that equipment used in nuclear facilities
tends to be kept in service for at least 20–30 years, while
those in other, less costly industries might be replaced
every 15 years. Furthermore, the nuclear industry is one
of the most heavily regulated in the world, and initial
regulatory restrictions prevented the adoption of digital
systems. It is only just beginning to address its relative lack
of experience of the cyber security challenges associated
with digital systems.
The nuclear industry worldwide is far behind many other
industries when it comes to cyber security. Since the nuclear
industry was one of the last to start implementing cyber systems
due to regulatory reasons, they have among the least amount of
experience and expertise. (Source 8)
There is a safety policy and when you come to any industrial
object, you find a lot of leaflets, banners, everything about
safety, safety, safety first. But as to cyber security, nobody cares
about that. … Remember that all the software or IT systems
were designed in order to comply with safety regulations but

While all industries are reluctant to disclose cyber
security incidents out of concern for the negative impact
on their reputation, the problem is even more pronounced
within the nuclear industry. The national security sensitivities
surrounding the nuclear sector have fostered an industry
culture that is inward-looking and closed, with information
typically shared on a ‘need to know’ basis. Limited incident
disclosure makes it difficult to assess the true extent of the
problem, as nuclear industry personnel might take it to mean
that there are very few incidents, reinforcing their belief
that cyber security is not a real threat. It also means that
the nuclear industry cannot learn from incidents that have
already occurred and enhance its cyber defenses. Given that
a certain cyber attack technique attempted on one plant is
likely to be attempted on others, the lack of disclosure means
that nuclear facilities do not have warning that they are
likely to be targeted by that technique.
All of the examples in an IAEA training course that I give are the
same two or three instances from 2003 and 2008 at a few plants.
It looks like we are picking on these plants, but it is just that they
are the only ones that have ever disclosed breaches and, therefore,
that we can talk about. I am sure that there have been many more.

While only a few cyber attacks on nuclear facilities have
been made public, one estimate puts the number
of major incidents that have affected industrial control
systems as high as 50 (this is in addition to frequent routine
attacks on business networks):
What people keep saying is ‘wait until something big happens, then
we’ll take it seriously’. But the problem is that we have already had
a lot of very big things happen. There have probably been about 50
actual control systems cyber incidents in the nuclear industry so far,
but only two or three have been made public.

Moreover, some incidents at nuclear facilities are not
correctly identified as having been caused by a cyber attack.
In some cases, the facility may know there has been a
malfunction but not be able to determine the cause. The lack
of adequate cyber forensics or logging for industrial control
system networks makes it even more difficult to determine
the cause of an incident. In other cases, a nuclear facility
might not know it has experienced an attack: a hacker could
have gained access to the business network or even industrial
control system network, but leave no trace. He or she might

even be able to introduce a logic bomb, or malicious code
that goes undetected until certain conditions are met.
When things start to go wrong, you might not know it is a cyber
attack.

Even when incidents are correctly identified, most
countries have few legal requirements to disclose cyber
security incidents at nuclear facilities. Since many incidents
have never violated an IT security policy, either because
they are not strictly an IT issue or because they did not
cause an outage, there is no obligation for the facilities
to disclose them to the relevant authorities.
The nuclear industry’s emphasis on safety at the expense
of security may also go some way towards explaining
why there is even less disclosure in the nuclear industry
than elsewhere: a cyber security breach does not receive
widespread attention unless it causes a safety problem.
When it comes to breaches, if it were a nuclear safety issue, it would
be public for sure. But because it is a nuclear security issue, no one
talks about it.

Limited collaboration and information-sharing
Another consequence of the industry’s ‘need to know’
mindset is that it is reluctant to collaborate and share
information with other industries in order to address
cyber security challenges. Unfortunately, this means that it
is unable to learn from industries that have more experience
in dealing with these problems.
The nuclear industry has always been insular, and the feeling is
that when it comes to nuclear, they know best. When it comes to
cyber, they do not, period. The bulk of the expertise, the bulk of the
experience, the bulk of everything else comes from outside nuclear,
but they refuse to use it.

Indeed, the cyber security challenges involving industrial
control systems are not industry-specific: the same systems
are used in a wide variety of industries, including closely
related industries such as the energy and utilities sectors, and
thus the vulnerabilities are common to all.
The information about a problem, for example with a Siemens
system, like what happened with Stuxnet, affects every single
industry everywhere that uses that same Siemens system. The
fact that Stuxnet went after centrifuges … was just the internal
programming: what made it cyber vulnerable made it cyber
vulnerable to every nuclear plant, fossil plant, water plant, railroad,
you name it. This is the exact same Siemens controller, the exact
same hard-coded default password that is in every single industry
worldwide.

The lack of information-sharing also means that the industry
cannot benefit from the use of collective data to identify
patterns that can aid attribution (which, in turn, can reveal
valuable clues about the intent of the attacker).

At the international level, too, countries are cautious about
sharing threat information with each other, as governments
are reluctant to share this type of information with one
another out of concern that it could be used against them.

Another challenge is that there is insufficient spending on
cyber security within the nuclear industry as a whole. Since
owner-operators and vendors tend to focus on profitability
and return on investment, they may not be spending enough
on cyber security. This, too, is a likely consequence of not
thinking that the cyber security challenge is ‘real’: since
the probability of a major attack is considered to be low,
such expenditure is not considered a priority.
Owner-operators of nuclear facilities are under constant
pressure to reduce the high costs of running the plant.

For this reason, owner-operators are increasingly
purchasing commercial off-the-shelf products from
vendors because they are cheaper than bespoke products,
despite being more vulnerable to cyber attack. In fact,
there is a fundamental tension between cyber security
and business efficiency. For example, increasing the
interoperability of control systems makes them more
efficient. But it also makes them more vulnerable since
many people have similar devices; someone who can
hack into one may be able to hack into all.
Given the other issues they must contend with –
safety-related concerns, for example, or physical security
challenges – it is not surprising that owner-operators
typically do not see cyber security as a spending priority.
The owner-operators are not terribly interested in spending money
on [cyber] security, such as upgrading or replacing existing process
control systems. They have got enough things that they want to do
without including [cyber] security. It is not something that gets you
more production, more efficiency.

Vendors too are not investing sufficiently in cyber security
and are making no efforts to design it in from the beginning
in future generations of products. Much of this lack of
enthusiasm is a problem related to return on investment.
A vicious circle is set up: since the owner-operators are
not demanding greater protection in this area, vendors do
not see a need to spend money to provide it. If there was

demand for it, vendors would view it as profitable and
would want to build it in.
The owner-operators aren’t pushing the vendors for greater cyber
security. This allows the owner-operators to say, ‘Well, we can’t buy
it; the vendors don’t make it.’ And the vendors can say, ‘Well, there
is no market demand.’ And so everyone is happy doing nothing.

Furthermore, many vendors do not view developing patches
as a priority. It is a costly process, and, again, since many
owner-operators do not install patches, vendors do not
always think it worth their time to develop them. When
researchers find zero-day vulnerabilities in their products,
some vendors do not engage with them to develop and issue
patches for them before researchers make them public.
Some of the vendors don’t recognize that cyber security is even
an issue. They’re not even responsive to researchers who find
vulnerabilities in their products. You would think they would want
to develop a patch and get it out to their customers before the
researcher tells people about it.

In fact, a profit-driven model may not be suitable for
ensuring cyber security in the nuclear industry; the
cost–security equation may be out of balance.
The fundamental dilemma is between either building
everything bespoke out of components that one can trust, or using
the components that are commercially available at a sensible price.
The question is, can you trust a ‘profit and return on investment’ motivated environment to deal with a truly difficult security
problem?

The agencies charged with providing support to the
nuclear sector are often under-resourced, making it all the
more challenging for them to allocate sufficient funding to
cyber security. This insufficiency of funding for agencies
manifests itself at both the national and international levels.
On a national scale, nuclear regulators often lack resources.
At the international level, the IAEA – the world’s central
intergovernmental forum on nuclear issues – has a team
that has been working to issue important guidance on cyber
security, but it is small relative to the size of the task.

Another challenge is the limited communication between
vendors and cyber security companies – yet
communication is essential if the latter are to provide
adequate protection. Industrial control systems rely on
operating systems and communication protocols between
devices that are often very poorly documented or else

proprietary to each major vendor, such as Siemens or
Honeywell. This is in part because such technologies
are considered a national security issue. In other cases
these operating systems or communication protocols
are obsolete, so only the vendor may still know how it
functions. However, if cyber security companies do not
know how an operating system or communication protocol
has been designed, it is much more difficult to protect it.

If you don’t know how a system has been designed, it is hard to
protect it. We need vendors to tell us how their products work, so
that we can figure out how our products can work in accordance
with that.

Given that governments are just beginning to grapple with
the emerging cyber risk, there is currently an insufficiency
of regulatory standards. Only a small number of countries
have issued standards on cyber security at nuclear facilities.
In Canada we are one of the first countries that produced a national
standard on the cyber security of nuclear facilities, with new
national standard N290.7. There are only a handful of national
standards.

When countries do issue guidance, the cyber security
measures that they recommend may not be rigorous
enough. In the United States, the guidance issued by the
Nuclear Regulatory Commission (NRC) is not sufficient to
protect against the cyber security threat. Even
if a nuclear facility were to implement all of the measures
in the ‘Reg Guide’ – a guide that helps interpret regulations
and gives guidance on how to comply with them – a number
of major cyber security vulnerabilities would remain.
Two years ago I was involved in doing a third-party review of what
I consider the most comprehensive cyber assessment done of any
commercial facility worldwide, and it was a nuclear plant. We found
major cyber security vulnerabilities that weren’t being addressed in
the Reg Guide.

Today, governments are increasingly moving away from
mandatory requirements in favor of recommendations –
a trend that is mirrored at nuclear facilities as well. This
change results in part from lobbying pressure by the nuclear
industry, which seeks to avoid the high costs of complying
with such regulation. For example, the Nuclear Energy
Institute, a lobbying group which represents the nuclear
industry’s interests to the US government, put in a request
in August 2014 to reduce the number of systems in nuclear
plants that would have to be included.
In any event, there also appears to be pressure for less
regulation to help alleviate regulators’ already stretched
workloads and ease the strain on their financial resources:

There is a push on individual responsibility – that is, having
individual facilities determine what they are supposed to be
doing to protect their assets – because the regulator cannot look
after everybody all the time. However, I would like to see the
days again where you have to have this or that in place because it
is a lot easier, rather than leaving it to me to figure out what the
threat is.

Some of the circumstances discussed – including the
historical prioritization of safety and physical security
(to the detriment of cyber security) as well as the
industry’s reluctance to disclose cyber incidents and
share information – suggest that the nuclear industry’s
cyber security risk assessment may be inaccurate and
a worrying underestimate. This is particularly the case
when combined with the previous chapter’s description
of misconceptions among a number of nuclear plant
personnel – notably, that they may be convinced that
nuclear facilities are air gapped, that this air gap protects
them, and that a cyber attack could not result in the
release of radioactive material.

Developing and economically stressed countries
Developing countries and others with struggling
economies – whether as a result of underdevelopment,
underinvestment, economic crises or conflict – may be
particularly at risk. If a non-state armed group, for
example, wished to cause widespread international
concern, nuclear facilities in more vulnerable countries
might be a preferred target.
Not all countries have the same level of knowledge when it comes
to cyber security. Some countries are just starting to learn about
the cyber security challenge.

The problem with developing countries is that – just like with SMEs
[small and medium-sized enterprises] – they tend not to have the
resources to have security-focused IT staff and large organizations
of people that can perform network monitoring functions and so on.

In Ukraine, personnel working in nuclear plants are not highly
trained in IT. Moreover, they have low salaries so their motivation
is very low.

The lack of regulation is particularly pronounced in
countries that are under economic and conflict stress.
For example, in Ukraine there is almost no regulation
involving the protection of critical infrastructure.

The United States has a CERT for industrial control systems. It
may not be enough, but there are some processes, people, and
organizational structures in place at least. In Ukraine, we still do
not have even a basic structure or management commitment to
protect against cyber threats.

The problem is exacerbated by lack of government
knowledge on protection against cyber threats.
We have an Information Security Authority that is trying to start
protecting critical infrastructure, but they know very little. Then
there is the security service of Ukraine, but they are not very
good with IT security. Neither is the commission on the safety
of nuclear plants.

The challenges in Ukraine and other countries,
including developing countries, may be aggravated by
the lack of English-language skills among their personnel,
which often makes it more difficult for them to access
the latest information available on cyber security. Source
11 comments:
Most people in Ukraine’s Information Security Authority … are very
bad at speaking English. Only [a few] persons in the CERT teams
know English; most do not. And so they produce some documents
that are very far from modern practice.

Their different ways of thinking
result in different priorities that are incompatible and can
lead to frictions. One such consequence is that nuclear
plant personnel often do not understand the cyber security
procedures. Additionally, the procedures are not always
clearly written, so that nuclear plant personnel may not
know whom to call in the event of a cyber security incident,
and may therefore not interpret the recommendations
or requirements in the way intended by the IT engineers.
These communication problems are exacerbated by limited
interaction, as those responsible for cyber security are
not based on-site. Furthermore, cyber security training at
nuclear facilities is often inadequate, and the lack of drills
means that nuclear plant personnel have no opportunity
to practice these procedures.
Another concern expressed by those interviewed is that
security at nuclear facilities is reactive rather than proactive.
While this might work in other areas, in terms of cyber security,
personnel at nuclear facilities might not become aware of a
cyber attack until it is already substantially under way. The
combination of factors discussed above suggests that nuclear
plants may lack preparedness for a large-scale cyber security
emergency, in particular if one were to occur after normal
working hours.

Conflicting priorities and cultural divides
Nuclear plant personnel, who are primarily OT
engineers, and cyber security personnel, who are
considered IT engineers, often have conflicting
priorities. The OT discipline concerns itself primarily with
the operations of a plant – such as the industrial control
systems, including the remote management of pumps and
valves – whereas IT is primarily concerned with computers
and networks. Each group has different priorities and
ways of thinking. In many cases, these different frames
of reference will clash, leading to conflict between the
two camps. They may often not even realize that their
approaches are different and that this will inevitably
lead to clashes.
Safety versus cyber security. Historically the main priority
of OT engineers has been to ensure the safe and efficient
running of the plant; but for cyber security personnel (or IT
engineers), security has been the priority.
Source 5 describes a recent IAEA meeting in which
the OT engineers (also termed ‘safety engineers’)

and the IT engineers (also called ‘security engineers’)
were approaching the discussions from such different
perspectives that they could not understand each other:
The safety engineers wanted the security engineers to add security
to a system, but were telling the security engineers, ‘You can’t touch
the rest of the tests. We have done 19 tests. You’re the last test, test
20.’ They were bent on making sure that the security engineers did
not invalidate any of the previous safety tests. They were essentially
saying, ‘Just make sure it is right for us and don’t violate any of the
previous tests.’

In reality, it is simply not possible to treat security as a
bolt-on extra to safety in this scenario, because the IT
engineers cannot introduce security without risking a
change that would invalidate the safety case. For example,
a valve controller may have a detailed safety case that has
been approved by the plant, but with little or no security
to protect the device from interference. If the plant decides
to add security to this valve controller, doing so may
invalidate some of the safety tests that have already been
done, or there might even be unexpected incompatibilities
between the security system and the safety system. The
system might then behave in such a way that it would
no longer be safe. This would be especially true if the
nuclear plant wanted to connect the valve controller
to the network, in order to gain easier access to data
generated by the equipment.
The reason that the security engineers don’t understand is that it
is not practical, not possible; you cannot defend a system without
altering its state. And so when the safety engineers say, ‘You can’t
alter the state,’ the security engineers say, ‘In that case we can’t
defend it.

Availability versus security. OT engineers prioritize
maintaining availability (in other words, keeping the plant
running continuously), while cyber security personnel (or
IT engineers), as discussed above, regard security as their
key focus.
Yet it is not always possible to promote availability and
security at the same time. For example, ‘patching’ a system
against a known vulnerability might mean that the system
will be unavailable during installation and testing. Rather than reducing the system’s availability,
the OT engineers will often prefer not to patch. From an IT
engineer’s perspective, patching is a way to improve security
against the growing number of cyber security threats. On
a larger scale, IT engineers could be in a position where,
to maintain the security of a facility’s systems, they might
require a shutdown of the plant in order to eliminate a cyber
security threat – which directly conflicts with the needs of
OT engineers.
On one side, nuclear wants availability as key priority. Cyber wants
security as key priority. And often they can’t cohabit well. That’s the
real fight.
Unintentional (accidental) versus intentional (malicious).
OT engineers are primarily concerned with preventing
accidents and other unintentional acts. This concern
derives directly from their focus on safety. By contrast,
cyber security personnel (or IT engineers) tend to focus on
preventing intentional acts which might harm the plant,
namely malicious attacks (although they are also concerned
about unintentional events).
OT engineers’ long-standing focus on safety and guarding
against accidents means that they have developed rigorous
methods of statistical analysis. They approach problems by
doing a causal fault analysis, which allows them to look at
everything that could theoretically go wrong, the probabilities
that all possible events might occur, and what the underlying
causes could be. This approach is so central to their culture
that they expect the IT engineers to show them the same
kind of causal analysis. But IT engineers are not trained to
approach problems in this way. The need to consider the
intentional threat means that there are simply too many
potential, unpredictable events for such an analysis to be
undertaken. For example, attackers could make use of zeroday vulnerabilities and other attack technologies that have
never been seen before, and new threat actors could emerge.

Frictions between nuclear plant (OT) personnel
and cyber security (IT) personnel
Given the conflicting goals and mindsets of nuclear plant
personnel and cyber security personnel, it is not surprising
that at times some degree of animosity manifests itself
between the two. Interviews with personnel from both
camps have provided useful anecdotes that further illustrate
these frictions and explain some of the underlying causes.
Source 8 emphasized that OT engineers’ general dislike of
IT engineers is a major part of the cyber security challenge:
The problem is as much cultural and sociological as it is technical.
One of the biggest problems we have is that – as in any industry –
the operations people dislike IT.

I can understand why nuclear plant managers don’t like us, because
they think we are painful. We come in at the end of a procedure that
works [and say that all of these cyber security measures must be
added]. We add in cyber security in order to protect them, but from
their perspective they don’t see the benefit.

Part of the problem can be attributed to the belief among
some nuclear plant personnel that cyber security does
not pose a real threat; they thus tend to regard the cyber
security measures imposed on them by IT engineers as a
nuisance, rather than as an important contribution to the
security of the plant.

I’ve never been convinced that if we ever implemented the [cyber
emergency] procedure, the guy was even qualified. Certainly not
qualified to the extent I was, where I had to go through schools. He
might be the biggest computer wizard in the world, he had no idea
how a nuclear plant worked.

Without this fundamental understanding, in his view, IT
engineers cannot understand why stabilizing the reactor
is so essential. As a result, many IT engineers would be
unhappy if nuclear plant personnel prevented them from
working on an IT problem because the nuclear plant
personnel first needed to stabilize the reactor; the IT
engineers would not understand why it should take priority.
The extent of the mistrust is such that Source 6 expressed
doubts about whether he could rely on the IT engineers
in the event of an emergency. He suggested that the IT
engineers do not have enough of a work ethic, commenting
that ‘They want to get the job done as fast as possible so that
they can go home. They are not 24/7 workers like we are’ –
the implication being that IT engineers were less likely to be
available in an incident occurring outside standard working
hours. Unlike nuclear plant personnel, who are accustomed
to receiving urgent calls in the middle of the night, the
cyber security personnel tasked with responding tend to be
corporate middle managers who are not normally required
to deal with out-of-hours calls, and it was felt they might not
fully appreciate their critical nature.
The same source observed that IT engineers often
wish to know the full extent of a problem before making
a decision, or in some cases need to seek permission from
the appropriate authority in their management structure
before taking action – meaning they might not be able to
make decisions quickly enough in the event of an incident.
Moreover, unlike nuclear plant personnel, cyber security
personnel do not have the requirements on fitness for duty
(including working-hour limitations and rules governing
alcohol consumption before reporting for a shift), so an
OT engineer would not know if an IT engineer responding to
a cyber incident had been up all night or was unwell.
Source 6 recounted how in the nuclear plants in which he
had worked, the IT engineers developed cyber security
procedure documents for the nuclear plant personnel that
directed them to stop what they were doing in the event of
a cyber incident, to touch nothing, and to call in the cyber
security personnel. They did not explain to the nuclear
plant personnel the nature of the cyber security risks, how
to deal with them, or the rationale behind the procedures:

The interviews also revealed that nuclear plant personnel
often do not understand the cyber security procedures,
including those to follow in a cyber-related emergency.
Even the most experienced nuclear plant personnel
reported difficulty in understanding the procedures as
communicated in the documentation.
The procedures are confusing as hell … I didn’t really understand
the procedures. What I knew is that if a cyber incident happened,
the first step was that I was supposed to tell the operators to stop
what they are doing and not touch any critical control systems.
And then the second step, after informing security, was that I
was supposed to call whomever the cyber person on call was.

Often, this is because the procedures are not clearly
written. Nuclear plant personnel report finding the cyber
security procedures so hard to understand that they do not
always know whom to call in the event of a cyber security
incident. In one case, while the procedures documents
provided a flow chart of who among the cyber security
personnel should be called in such an event, the chart was
unclear.

In fact, the difficulty understanding the procedures is not
limited to OT engineers. The physical security personnel
at nuclear plants, who must implement IT requirements at
times, also have difficulty understanding cyber security
procedures.

Given that the procedures documents were written by
IT engineers, with their very different approach and
ways of thinking, this is hardly surprising. The nuclear
plant personnel’s difficulties in understanding the
documents are a clear manifestation of the cultural
divide. As Source 8 explains, ‘One reason the guidelines
are unclear is that they were written from an IT
security perspective.’
The consequence of this is that the interpretation of
recommendations or requirements by nuclear plant
personnel may be very different from that intended
by the IT engineers. OT and IT engineers literally often
take different meanings from the same phrase. For
example, for OT engineers a ‘denial of service’ might
mean that a 10,000 horsepower main coolant pump in a
nuclear plant has shut down. For IT engineers, a ‘denial
of service’ occurs when a malicious flood of data makes
a computing resource unavailable.
As another example, although the cyber security
procedures instruct nuclear plant personnel not to touch
any ‘critical control systems’ in the event of an incident,
it does not detail which system or systems should be
regarded as critical. Nuclear plant personnel are thus
expected to use their discretion, and their conclusions may
be very different from those envisaged by the authors of
the cyber security procedures.
Similarly, physical security personnel at nuclear plants
might interpret the phrase ‘intrusion detection system’
as a gate monitor or a card reader. To an IT engineer,
an ‘intrusion detection system’ monitors a network for
suspicious traffic.
The security professional and the IT professional will have a
different interpretation of what exactly IT security compliance
means; a security professional and an IT professional may
have different views on what a control actually is because the
documents are badly written. (Source 7)

Another consequence of the cultural divide is that
personnel at nuclear facilities often have difficulty
determining what their critical cyber assets are. For
example, in one plant, one of the most critical controllers
– the pumps that are used to bring water back into the
plant after a loss of feed water event – had its push button
located in the highly secure control room but its PLC was in
a building that required key card access but was not a vital
area. In the United States, there have been some promising
recent initiatives to encourage nuclear plant personnel to
work with cyber security personnel in order to agree on
which assets are cyber critical and need to be prioritized for
protection, but more such efforts are needed.

These communication problems between nuclear plant
personnel and cyber security personnel are magnified
by the limited interaction between the two. A significant
part of the challenge is that those responsible for cyber
security at nuclear facilities are not based on-site and
in fact are often located some distance away; there are
thus limited opportunities for the nuclear plant and cyber
security personnel to interact in person. Moreover, among
the latter, responsibility is often highly dispersed. Thus for
most nuclear plant personnel their main contact with the IT
engineers is when they come out to the plant on occasion
to make repairs. However, these would be unlikely to be the
same people who would be responding to a cyber incident.

Another concern is that cyber security at nuclear facilities
is reactive rather than proactive; in other words, the focus
is on reacting and responding to incidents as they arise,
rather than proactively seeking to prevent attacks. In general,
defences at nuclear facilities (e.g. physical security) rely on
receiving warnings of an imminent attack. For example, if a
plane were heading towards a nuclear facility located in the
United States, the Federal Aviation Administration would
call the facility to alert personnel there.

No plants in the country have cyber expertise on site. I think that
it’s all corporate people and that they are not even around. I had
no idea who they were. I just knew that they worked in an office
that was maybe 100 miles away.

It appears that the level and quality of cyber security
training at nuclear facilities are often low compared
with the mandatory training for nuclear personnel in
other areas. In particular, some organizations undertaking
the training may not have sufficient expertise to do so.

Many companies propose training sessions, but not all of them are
equally rigorous. The right people are not always doing the training.
Many companies and foundations take norms and say that they can
train people, without there being any accreditation process.

As such, the
training also did not address what was happening from a
cyber security perspective or how to coordinate with the
cyber security personnel. This inadequacy of training is very
likely to stem from the nuclear industry’s perception that
cyber threats are not a high risk.
In particular, the lack of drills is a problem since there is no
opportunity for nuclear personnel to practice cyber security
incident procedures. By contrast, nuclear facilities have
regular drills for other scenarios, including integrated drills
with the physical security personnel to prepare for the event
of an attempted invasion.
The inadequacy of the training is such that when nuclear
plant personnel are tested on the cyber security procedures,
they may not understand the questions properly and often
fail, whereas they obtain high scores in the frequent tests
they must take on other procedures.

Many procedures for reacting to events at nuclear facilities are
based on warnings of either an imminent threat or of an event that
has occurred … It’s all reactive, based on somebody in the plant
seeing that something has happened.
We’re reactive to a large extent, something happens in the industry
and we learn from it. I can assure you that what happened in South
Korea back in December [2014] is going to cause a lot of changes in
the way operators and states think of cyber security.

When it comes to a cyber attack, however, there are no
such warning mechanisms in place. In fact, as discussed
above, a nuclear facility might not know of a cyber attack
until it is already substantially under way. For example,
a hacker could introduce a logic bomb that lies dormant
until it is activated to cause physical damage. In the case
of the Natanz and Bushehr nuclear facilities in Iran, the
nuclear plant personnel knew that their centrifuges were
breaking apart. However, it was only months later that they
realized that the Stuxnet worm was the cause. In addition,
the ease with which malicious code can be hidden makes
implementing such a warning system more difficult than in
other domains, and in some cases it may be impossible.

The combination of factors discussed above suggests that
nuclear plants lack preparedness for a large-scale cyber
security emergency, and there would be considerable
problems in trying to coordinate an adequate response.
A large-scale cyber security emergency occurring
at night could be particularly dangerous. The most
confusing time for a system to go out of service is during
this time. When we have to call people in the middle of the night for other
issues that are just as important, like a pump breaking, the response
can be slow. If you’re calling people at 1 am, it takes them a few
minutes to wake up. And say you have 10 people who need to
be on the call. By the time you get everyone to dial in, it can
take over an hour.

This chapter assesses some of the technical challenges
involved in providing cyber security at nuclear facilities.
Above all, early designs of nuclear facilities – before cyber
attacks were a concern – means that they are insecure by
design, lacking basic safeguards including authentication
and encryption. This means that cyber security at nuclear
facilities depends in large part on the successful defense of the
network perimeter – all the more so because the flexibility of
code means that any attacker who can get past the perimeter
defenses would be able to make logic changes in the code
that are almost impossible to observe. Furthermore, some
cyber security techniques such as patching that are standard
in home or office IT environments are difficult to implement
within nuclear facilities. Lastly, it is extremely difficult to
guarantee the integrity of the supply chain.

A major challenge for the nuclear industry, as for most
critical infrastructure, is that cyber security measures
were not designed into industrial control systems
from the beginning. The control systems in most nuclear
facilities were developed in the 1960s or 1970s when
computing was in its infancy and designers gave no thought
to the possibility that an actor with a malicious agenda
might deliberately try to attack a computer system using
electronic means. Against this background, systems were
not designed and built with protection against cyber attack
in mind, and ‘retrofitting’ cyber security measures to
these original systems now is technically challenging and
expensive.
A couple of minor tweaks in how you think about a system right
at the very beginning can have huge implications for its security.
If security wasn’t built in at conception, it is difficult to bolt on
after the fact. Actually, it is going to require a redesign.

One example of the ‘insecure by design’ nature of
industrial control systems is the lack of authentication
and verification. That is, field devices do not require
authentication that a command sent to them is a valid
command, or verification that it comes from a legitimate
source. They are designed to do what they are told
without  question. This means that any attacker who is able
to gain access can just send a command to the device and
it will comply. As a result, industrial control systems are
particularly vulnerable to man-in-the-middle attacks that
alter the communication between two devices:
The field devices accept the message immediately, without asking.
The receiving device does not have to authenticate. Control systems
are thus very fragile due to man-in-the-middle attacks.

Furthermore, the flexibility of code means that an
attacker can change the logic, or the set of programming
instructions, for a piece of equipment in order to cause it to
behave differently. This was exploited by the Stuxnet worm.
Logic changes are difficult to detect and are therefore a major
concern. While it would be technically feasible to examine
the code to determine whether any lines had been changed,
in practical terms the task would be immense because a
typical system could contain billions of lines of code.
This difficulty is exacerbated by the lack of cyber forensics for
control systems. For example, they do not generally have log
files that maintain records of which parts of the system have
been accessed, who accessed them, which information was
viewed, and at what date and time. Without a log, it is much
more difficult for cyber specialists to determine whether a
hacker has gained access or changed anything in the code.
A major implication of the existence of ‘insecure by design’
systems at a nuclear facility is that such systems rely entirely
on network perimeter defense to protect them from attack. If
a hacker is able to breach the network perimeter, then the lack
of authentication and the flexibility of code provide a number
of opportunities to inflict significant damage on the facility.
It is almost impossible to protect the system once someone gains
access to it. That means that right now, we’re entirely reliant on the
perimeter to stop hackers.
The unique aspects of industrial environments (and
particularly nuclear facilities) mean that standard cyber
security measures used in everyday home or office IT
environments are not necessarily applicable. Cyber
security experts urge home and office users to install
patches that will address vulnerabilities discovered in
software. Yet patching at nuclear plants presents unique
challenges, and is therefore infrequently used.
Patching is really challenging, and the reality is that very few
people are actually installing any patches.

First, unlike in everyday home and office IT environments,
patches are less likely to be available for the systems being
used. Since these are predominantly legacy systems kept
in service for at least 20–30 years, unlike those in home
or office environments, many are no longer supported by
the vendor. A number of facilities have very old MS-DOS
or Windows NT operating systems, for which Microsoft no
longer issues patches (or at least, not at a reasonable cost).
In some instances the vendor may no longer be in business.

Furthermore, patches risk breaking the system that they
are trying to protect. A patch may not be compatible
with other software or hardware on a system, thereby
causing the entire system to malfunction, or it might have
unintended and unforeseen effects. Since the utmost
priority is maintaining the availability of the plant, and a
patch which does not perform as expected could take an
entire plant offline, some operators consider the risk of
patching to be too high. Source 3 noted: ‘I’ve seen patches
break systems, where they actually disable the system.’ In
home and office environments the consequences are much
less severe and can usually be corrected fairly quickly.

Since the utmost priority is maintaining the
availability of the plant, and a patch which
does not perform as expected could take an
entire plant offline, some operators consider
the risk of patching to be too high.
Even if a patch has been approved for software that runs on
a vendor’s equipment, this does not necessarily guarantee
that it is safe to install. The mere presence of one additional
piece of software, such as a plug-in, running on a system
in a nuclear facility can create an incompatibility with the
patch and break the system. The vendor will have tested
that the patch is safe in several standard cases, but cannot
possibly test every combination of software that a nuclear
facility might be running.
Just because your automation vendor has certified a patch, you
don’t know whether, because you’ve got that system with some
other plug-in, it’s going to have a negative impact. (Source 26)

The unique characteristics of industrial environments
like nuclear facilities mean that even patching a
facility’s commercial network could have significant
consequences. It might be reasonable to assume that a
facility’s commercial network is an ‘everyday’ office IT
environment and that a patching problem there would
only affect that network. Yet its interconnectedness with
the industrial control systems means that a problem with
a patch could affect both systems. As noted in Box 1, at
the Hatch nuclear plant in Georgia in 2008, a patch was
applied to the business network in order to synchronize it
with the industrial control system network. Unfortunately,
it introduced incorrect data onto an industrial control
system, triggering an automatic plant shutdown.

Owing to the risk of a patch breaking a system, nuclear
facilities, again unlike everyday home and office IT
environments, must test patches extensively and intensively
before they can install them. Patches that affect key systems
cannot be applied and tested on the system directly without
the risk of taking an entire plant offline. Instead, nuclear
facilities often need to set up a costly partial or complete
duplicate system to serve as a test bed.
Having a duplicate system is enormously expensive. And even then,
you’ll never literally have two identical nuclear reactors. Yet, to have
absolutely accurate testing, you would need literally the exact same
thing twice. (Source 9)

Even if a patch is available and has been tested, finding a
time window in which to apply it is often difficult. Nuclear
facilities operate 24 hours a day, but the plant would need
to be shut down in order to apply patches, especially if they
affect key systems. Some systems provide such essential
capability for the running of the facility that even taking
them temporarily out of service would compromise the
plant’s safe operation. Nuclear power plants might typically
shut down for maintenance every two years, so installing
a patch may not be possible until a scheduled shutdown
occurs. Again, this is in contrast to everyday home and
office IT environments, where patches can easily be
installed during downtime.
You have to be assured that you have even got a change
window. Now, if you have a change window, then potentially the
organizations themselves have to take a break from operations,
and you are talking about a 24/7 operation.
Operators are not going to be willing to shut a unit down for three
days to install a patch for a vulnerability that somebody might or
might not exploit.

Since patching changes the configuration of a system,
in a nuclear plant it also makes it harder to monitor the
system for unusual behavior that might indicate infection
by malware. Among nuclear operators, the instinct is to
avoid making changes to a system so that the operator
can acquire a deep understanding of how that system
works; the moment a patch is installed, however, the
system has changed and the operator no longer has the
same depth of understanding of its behavior. Patching
would thus considerably reduce the effectiveness of
monitoring techniques, which look for behavioral
anomalies. Yet again, this is in marked contrast to
changes to the configuration of systems in everyday

Creating a test-bed for a nuclear facility is particularly complex because of the prevalence of legacy systems. Many of the components used at nuclear facilities are
no longer manufactured, so operators must try to purchase them on markets for old equipment. Moreover, the equipment must be absolutely identical in order to test
a patch properly, since just one difference in a component could cause the duplicate system to react in an entirely different manner. For example, if a computer in a
nuclear facility is running Windows 98, then an operator must obtain a Windows 98 computer that has exactly the same graphics card, network card and other elements
for the test bed. In procuring components for a test bed, for either legacy or new equipment, part substitutions made by the device manufacturers can present real
problems. For example, if an operator buys a personal computer in January and then purchases exactly the same model in March, it is possible that the manufacturer
could have changed a small number of components in those three months: even if the two computers are seemingly the same model from the same manufacturer, they
may not be identical. Yet even small differences such as these could cause the duplicate system to react in a different manner during testing.

The default position is that, as you develop and field test a system,
that’s the way it stays. Industrial operators do that because it works.
Every change you make introduces uncertainties and always will.

Finally, patching is a never-ending cycle with new
vulnerabilities always being discovered and with them the
requirement for new patches.
You could spend all this time patching your systems and,
tomorrow, they will be just as outdated as they were before you
patched.

This challenge is magnified by the large number of
systems that need to be patched in a nuclear facility. For
patching to be effective, an operator could be faced with the
requirement to patch every single device in that facility on
a regular basis, but there will always be a significant period
after the discovery of a vulnerability when a system will be
known to be vulnerable while the vendor develops a patch,
which then has to be tested. This process could at best take
weeks or months, but in many cases it could take years.
In order to limit your exposure, you need to patch everything. You
need to patch your switches, you need to patch your firewalls, you
need to patch embedded devices.

It seems, therefore, that each nuclear facility must carefully
assess the advantages and disadvantages of patching in
each instance. Many appear to have decided that the risks
outweigh the benefits and choose not to patch.

Supply chain vulnerabilities are a growing concern since
the equipment used at a nuclear facility (and in critical
infrastructure more generally) could be compromised at any stage. Backdoor access or exploits could be introduced,
for instance, at the vendor’s facility, when the equipment
is being designed and assembled, or at the locales of
any of the subcontractors. For reasons of cost efficiency,
vendors are likely to make use of sub-components from
other sources, including those produced in other countries.
Even the transportation phase is liable to tampering. The
Snowden revelations provided evidence that the United
States’ National Security Agency (NSA) intercepted routers
and other network devices being shipped overseas and
implanted backdoors, or means of obtaining unauthorized
remote access to computer systems.
We really have no way to defend against supply chain risks
in a cyber warfare situation: a computer or system could be
compromised in transit or at the place of manufacture.

Although supply chain threats are at present primarily
confined to a small number of state actors seeking to
prepare the terrain for cyber conflict scenarios, it is possible
that terrorist groups or even hackers could adopt such
tactics as well.
Of course, intelligence agencies across the world are
concerned by these vulnerabilities – particularly in the wake
of the Snowden revelations – and a number of countries
are increasingly seeking to nationalize their supply chains.
However, the reality of globalization is that very few
countries are capable of producing all the required parts of
a nuclear plant themselves.
The US would like to do that [produce all its own components], but
I don’t think the US can do it anymore. I don’t think anybody’s in a
position to do this.

For instance, just one computer used at a nuclear facility is
comprised of thousands of parts. Among these, it is almost
inevitable that there might be, say, a tiny chip made in
Taiwan, or some other foreign sub-component.

Meeting the challenges described in the previous chapters
will require a blend of policy and technical measures. This
chapter proposes a series of solutions centered around several
key themes. There is above all a need for improved risk
assessment guidelines on cyber security at nuclear facilities,
which will provide a solid economic underpinning for
investment. The ‘human factor’ can best be handled through
a combination of better communication about the risks of
poor ‘cyber hygiene’ and stronger enforcement measures.
Improving disclosure and information-sharing could be
achieved by encouraging anonymous sharing, fostering
personal contacts at international conferences, and the
establishment of industrial CERTs. There is also a need for
regulatory standards and more funding for agencies like the
IAEA. The cultural divide might be bridged by measures such
as encouraging IT engineers to visit nuclear plants, cross disciplinary educational programs, and improving cyber
security training.
Technical measures such as avoiding the use of nonessential digital features, implementing whitelisting
(authorization) technologies, network monitoring, and
encouraging the adoption of data diodes can all enhance cyber
security. Countries can mitigate supply chain risk by reducing
their dependency on foreign components.

Since the insurance industry requires solid risk
assessments, promoting the further development and
adoption of cyber insurance in the nuclear sector might
also be beneficial in helping develop these guidelines to
measure cyber risk; cyber insurance may therefore be an
important tool to enhance cyber security. The French
government has been conducting a major study on this
question. An early conclusion is that to succeed (and to find
the right level of underwriters’ exposure when measured
against the cyber security risk), a key need is the accurate
calculation of that risk based on metrics agreed between
insurers and the insured.
What underwriters need is an understanding of the risk and that
really comes down to, do organizations have the right people in the
right places, with the right authorities, to make the right decisions and
have the right policy and operational structures in place? (Source 9)

Insurance may also make cyber security more commercially
attractive and drive the process of implementing appropriate
measures, by providing the necessary financial incentives (in
the form of lower premiums) to persuade owner-operators to
invest in them.
If an insurance company tells an owner-operator that their
insurance premium would be very high because they don’t have
adequate cyber security measures, the owner-operator might just
conclude, ‘if I spend $100,000 on cyber security measures, I can
save $200,000 on the insurance premium’.

Given that many in the nuclear industry do not believe that
cyber security poses a real risk to nuclear facilities, a first
step is to raise awareness of the challenge. One way to do
so would be through the development of guidelines on
ways of measuring cyber security risks in the nuclear
industry. Since at present there is no risk assessment
methodology that would permit a nuclear facility to perform
a combined safety risk and security risk assessment (only a
safety risk assessment and a separate security assessment,
which includes cyber security risk), such guidelines include
the need for a combined risk assessment methodology
for safety and security. Developing a methodology will
require reflection within the industry, perhaps led by the
IAEA’s Interface Group, which was formed to address
conflicting priorities between safety and security.
A greater understanding of the risk will also help to tackle
the challenge of insufficient spending on cyber security
in the industry. In addition to raising awareness of the
need to invest in cyber security, it will make cyber
security more commercially attractive and provide a
clear economic rationale for CEOs and corporate boards
to increase expenditure on it.

Given that part of the challenge stems from the ‘human
factor’ – such as engineers or contractors who set up rogue
or unauthorized connections or those who plug their home
laptops directly into nuclear facility networks – raising
awareness among the personnel involved of the inherent
dangers in doing so will be key.
There is also a need for nuclear facilities to establish
rules where they are not in place already. For
instance, in countries or facilities where personal devices
are not already expressly forbidden within nuclear facilities,
engineers should be required to hand in any personal devices
such as laptops when they enter the facility; the devices
should only be returned to the engineers when they depart.
Engineers should be required to turn in any personal laptops that
they bring to the plant.
If you are going to do any testing and have any kind of device of
your own, you should have to turn it in and we will issue it back to
you when you bring our laptop back.

There is also a need for rules requiring nuclear plant
personnel to change the default passwords on equipment
to secure passwords; this should apply to both existing
equipment and to any new equipment installed.

In order to ensure that engineers actually follow such
policies, enforcement is key. In particular, independent
verification methods, in which multiple personnel check
compliance with procedures, should be rigorously followed
for cyber security issues. Source 6 suggested that if a
device has been signed out, an assigned person should
independently check that it is the correct device before it
is hooked up to a nuclear plant; a person should also be
assigned to run a virus scan on the device.

In communicating valuable
information about prevalent attacks – including the types of
vulnerabilities exploited by hackers, attack pathways used
to gain access, and systems targeted – sharing indicators of
compromise would provide others with an early warning
of such an attack. This would enable them to put defensive
countermeasures in place, perhaps by increasing monitoring
or by deciding to patch systems that are identified as
particularly vulnerable.

Technical means can also be used to help enforce
compliance. For example, given that nuclear plant personnel
may plug USB devices into the nuclear facility computers
even though this is not allowed, owner-operators may want
to glue USB ports.

People working in nuclear plants might be more willing to put
up with glued ports than they would in a standard IT environment.
Glued ports within a plant room probably do not impact productivity;
they just make it hard for someone to charge his iPhone. On the other
hand, glued USB ports in an IT environment would definitely impact
the effectiveness of employees.

Another option is to ensure that USB devices are checked for
malware and cleaned before they are allowed into nuclear
facilities. One company has developed a technology to do so.
There is company down in the south of France that has developed
technology that provides USB cleaning devices. So we’re not saying
don’t bring your USB to work, but can we at least plug that USB into
a special device that will examine all of the data that’s on it, it will
execute the files that are executable and make sure that there’s no
malicious software on them before the person plugs that USB stick
directly into a critical asset.

Promoting disclosure and information-sharing
Since the industry reluctance to share information about
cyber attacks that have occurred stems partly from concern
about potential damage to reputation, encouraging nuclear
facilities to share threat information anonymously would
promote greater disclosure. Anonymity could be achieved
by asking facilities to share ‘indicators of compromise’,
which are traces left on a network or system that indicate a
malicious actor has been in the system. These might include
phishing emails, the IP addresses from which an attack was
launched, or the malware code itself. In sharing indicators
of compromise, nuclear facilities do not have to reveal their
identity, nor what the impact of the attack has been.
Sharing indicators of compromise can help the whole industry
and improve security. We could anonymously share indicators
of compromise without knowing who it was that was breached.

Given that nuclear facilities tend to focus on reacting to
attacks as they unfold, another benefit of sharing indicators
of compromise is that it would encourage a proactive

The airline industry … has set up a platform in which pilots and
other industry personnel can anonymously report incidents
(for example, if two aircraft come too close to each other); this
approach has helped increase disclosure and enhance the safety
of the industry.

Such mechanisms could be copied and adapted in the
nuclear industry and in the industrial sector more generally.
Fostering personal contacts, which are central for the
trust-building required for information-sharing, is also
key for promoting the exchange of information at both
national and international levels. People may not trust
other companies – or governments, for that matter – but
they do trust other individuals with whom they have
developed strong personal relationships; they are therefore
prepared to take the risk of sharing information with them.
International conferences can be an important avenue for
building these relationships, and more such initiatives in the
nuclear industry (and critical infrastructure more broadly)
should be encouraged.
Personal contacts are always best for information-sharing;
these trusted environments work best where they co-align
common interests of countries or organizations and also
personal relationships.
Conferences where people meet with each other are very important,
because when people personally know one another they will not
want to attack each other in a cyber warfare scenario. There are not
that many nuclear plants in the world, so this should be possible to
implement; there needs to be a sense of community.

Although governments are concerned that sharing threat
information with other governments could jeopardize
national security and thus are reluctant to collaborate at the
international level, they recognize that at the national level
such sharing is a key priority for defense. Governments can
therefore play a key role in encouraging information-sharing
within their own countries by leading the establishment
of national Computer Emergency Response Teams
specialized in industrial control systems.
The unique characteristics of industrial control systems
mean that CERTs specifically dedicated to industrial control
systems will be more effective. In fact, the United States
has achieved success with its Industrial Control Systems

CERT (or ICS-CERT) established in 2009, which operates
in addition to the national CERT (referred to as US-CERT).
Of course, for countries that have yet to establish national
CERTs, doing so is a first priority and ICS can be handled as
a division within these as a first step.

the nuclear sector should learn from its experiences as the
cyber security culture develops over time and corresponding
capabilities are developed.

Regulators need to understand that in order
to foster a more proactive cyber security
culture in the nuclear sector, they should
be content to stay remote from some of the
necessary dialogue between stakeholders.

A number of policy measures would be beneficial as
well. Given that only a small number of nations have
implemented regulations regarding cyber security at
nuclear facilities, the remaining countries should be
encouraged to adopt regulatory standards. Since a
large number of countries follow IAEA guidance, the
agency’s further development of its work on cyber
security at nuclear facilities will prove beneficial. This
can be encouraged by allocating more resources to the
IAEA (and other agencies) to enable them to deal more
effectively with cyber security threats.

Some measure of government-backed international sharing
can also take place between close allies. One avenue for this
is the national CERTs; encouraging greater information sharing between national CERTs could prove beneficial.
At present, there is only limited information-sharing between
CERTs on an informal, ad hoc basis. Even
though some governments will take more information than
they contribute, this will still strengthen cyber security. Many
in the industry feel that any information-sharing, however
limited, is still better than the current minimal situation.
It would be helpful to have greater information-sharing between
the CERTs. Of course, most countries will want to take information
but not give it. But if you allow countries to give what they want and
take what they want, it’s not ideal, but it’s much better than what we
have today because today we don’t have anything.

Furthermore, given that owner-operators can be wary of
disclosing cyber security breaches or incidents in case they
are held liable, creating an environment where they feel
they can speak candidly without fear of repercussions is
key to increasing the level of reporting to ICS-CERTs (or
CERTs more generally). The regulator should reassure
owner-operators that they will not be penalized for any
information they share – provided they show good faith –
and that, if they disclose a cyber security problem or incident
that arose because they violated the code, they will not be
prosecuted. According to Source 20: ‘To enable information sharing, you need to develop a culture where whatever you
say will not be used against you.’
Regulators thus need to understand that in order to foster
a more proactive cyber security culture in the nuclear
sector, they should be content to stay remote from some of
the necessary dialogue between stakeholders; that their
prime focus is on outcomes, rather than on the mechanics
of delivering a minimum level of security. They also need
to be aware of the difficulties of security in the electronic
medium, and take a pragmatic approach to enforcement.
Every system, whether it is air gapped, patched or otherwise
protected, is liable to intrusion; as long as the root cause of
a particular breach is not negligence or purposeful violation
of rules, then regulators should only be concerned that


Particular attention should be dedicated to helping
developing countries improve their cyber security readiness
in the nuclear sector, given their greater vulnerability. These
countries are likely to require funding assistance as well to
enable them to achieve this.

In order to overcome the communication barriers
between nuclear plant personnel (OT engineers) and cyber
security personnel (IT engineers), fostering face-to-face
communication between the two groups will be essential.
For example, it is important that the cyber security personnel
physically visit nuclear facilities on a regular basis. As
cost-saving measures, they will be tempted to use methods
of remote collaboration, but face-to-face contact is key to
promoting mutual understanding between the two cultures.
In particular, encouraging nuclear plant personnel and
cyber security personnel to work together on integrated
projects would allow them to gain greater appreciation of
each other’s ways of thinking. This might involve working
together on joint vulnerability analyses or risk assessments,
for example. It would also help raise general awareness of
cyber security risks among nuclear plant personnel.
Actually getting the IT guys to work in the plant, to sit with the
engineers and work with them, to deploy stuff in OT environments is
how you ensure that IT and OT understand each other.
You need an IT security professional to talk to the on-site security
professional so that they can understand the same language.

It will be important to improve cyber security training
at nuclear facilities. Given that one problem identified
is that some of the training may be conducted by groups
without sufficient qualifications, there may be a need
for accreditation of training programs.

Given that most industrial control systems were designed
without considering cyber security requirements – and
that, as noted above, it is difficult to ‘add on’ cyber security
at a later date – it is essential that the designers of future
generations of control systems take cyber security into
account during the initial conception phase. For example,
ICS should avoid the inclusion of non-essential digital
features that could introduce cyber security weaknesses;
otherwise, removing such features will require partial or
complete redesign. In practical terms this may mean that
particularly important functions should not be digitized.
A couple of minor tweaks in how you think about a system right
at the very beginning can have huge implications for the security.
If a certain function is particularly important, you might make the
decision that you don’t even want a computer involved.

Additionally, given that the growing uptake of digital
systems is leading to a reduction in redundancy, it is
important for nuclear facilities to realize this and to ensure
that sufficient redundancy is retained. This may involve,
for example, making certain that there are manual backups
for critical systems in the event of a failure.

Encouraging the greater adoption of authentication and
encryption technologies in future generations of ICS will
also be key, since their lack contributes to making SCADA
systems ‘insecure by design’. Adding authentication when
sending and receiving communications or commands means
that the different parts of a SCADA system have to prove
their identity to each other – and that the communication or
command being transmitted is legitimate. It makes it harder to
carry out cyber attacks that send an unauthorized command
to a device that automatically accepts it, or that falsify
communications (as happened with Stuxnet, for example).
And adding encryption to authentication would also make the
contents of the communications or commands unintelligible
to hackers, providing an even greater level of security. Source
29 confirms: ‘The solutions to the ‘insecurity by design’
challenge will involve encryption and authentication.’
Given that the unprecedented flexibility of the current
generation of ICS also makes them ‘insecure by design’,
it will be vital to restrict their malleability. While the
specific nature of industrial environments means they face
particular cyber security challenges that do not exist in
everyday home or office IT environments – such as patching
difficulties – these special characteristics also permit unique
cyber security solutions that would not be possible in the
latter. Promoting the adoption of ‘whitelisting’, for
example, could therefore be an important way to bolster
cyber security at nuclear facilities. As an information
exchange protocol that only permits actions or traffic if
they are on an authorized list known to be safe, whitelisting
contrasts with traditional ‘blacklisting’ methods of cyber
defense, a model under which all actions or traffic are
permitted unless they are on a blocked list.
Whitelisting can be done both at the device level and at
the network level. At a device level, the methodology
involves authorizing the device to carry out only a narrow
set of actions that are necessary for its role. The computer
would only be allowed to run certain types of pre-approved
executable files, rather than, as now, any executable files on
a USB key. This would reduce the risk of infection carried
across an air gap by insertion of a USB device (which was
the likely pathway used by Stuxnet).
Whitelisting at a network level involves only authorizing
traffic between specific points that are needed for its
activities. For example, instead of allowing a computer to
talk to all of the computers on the network, whitelisting
would only allow it to talk to a small number of other
previously identified computers with which it needs
to communicate.
Industrial environments are particularly suited to
whitelisting because they are predominantly static in
functionality, making it possible to determine exactly
what actions or traffic should be authorized. Most everyday home or office IT environments are in constant
flux. At the device level, users regularly download new
software on their computers – either new applications or
software updates to existing applications. At the network
level, computers are regularly added to or removed from
parent networks. These computers also generate a lot of
unfamiliar traffic, as they visit new websites and receive
and send numerous emails to and from new people all over
the world. This results in a high level of unpredictability.
By contrast, the industrial world is relatively fixed. At the
device level, patching is rare (particularly in the nuclear
environment), so device configurations change little. At the
network level, industrial control systems primarily involve
computers talking to computers; thus the communications
and commands that different parts of such systems must
exchange with other parts should follow relatively stable
patterns. This predictability makes it possible to determine
what actions and communications should be authorized
in industrial environments.
Within an industrial control system environment, especially
a nuclear environment, actually being able to secure these
environments is infinitely easier, not harder, than it would be for
an IT environment.

Whitelisting can also provide a solution to the
patching challenges experienced by nuclear facilities:
by restricting the functionality of a device or network,
it becomes less important to patch systems, and this in
turn facilitates whitelisting.
If you compare the effort of doing whitelisting with the effort of
patching and vulnerability management, they are not even vaguely
related in scope.

In order to implement whitelisting, if the programmable
logic controllers are modern, purchased within the last 10
years or so, and as long as they are digital, only a firmware
upgrade to them or a new ethernet card would be needed.
The financial expense of an upgrade would be manageable.
In fact, the largest share of the cost would be the additional
testing and planning needed to make the upgrade safely.
If a system is older, perhaps 20–30 years old, then
whitelisting may not be possible. In this case, other options
that can add security include active management, the
deployment of intrusion protection systems, and intrusion
detection systems which monitor the electronic traffic
within a nuclear facility for anomalous behavior. Some
of these are discussed further below.
Intrusion detection systems such as network
monitoring, which involves examining the traffic within
a nuclear facility for anomalous behavior, would enable
nuclear facilities to take a more proactive approach to cyber
security. When the system detects unusual traffic that does
not fit the established pattern, it alerts the owner-operator.

For many facilities (nuclear and otherwise), the first
step in network monitoring is to map the expected traffic
between devices in order to establish a standard baseline.
Many nuclear facilities have yet to do this, and others
may not have undertaken the mapping at a sufficiently
detailed level.
It is vital that operators identify the devices they have, identify
how they communicate with each other, and put in place technical
systems that will immediately alert the operators as soon as any of
that ever changes. This is typically not done in industrial settings.
It is done to a greater degree, but far from ubiquitously, in nuclear.

Because people are not thinking about security, they are not doing
the data flows at the level that is needed for security. The way data
flows are currently documented is, for example, that this computer
sends data every 10 minutes over to that computer. But what we
need to know is the communication between IP addresses or ports
and the data format. For example, if a computer is trying to access
an IP address outside my company on port 80, that would be a red
flag because it is indicative of a backdoor access Trojan sending
data back to a command and control server.

The use of virtualization – the creation of a virtual
version of a device, operating system or network – may be
a useful process in helping understand the data flows and
serve as an effective way to map out those connections. By
virtualizing the entire network, it is possible to learn about
the data flows without the degree of risk involved in actual
experimentation.
We can use virtual environments to learn about the data flows
without having to experiment with our real network and worrying
that we are going to mess it up.

Furthermore, monitoring needs to be done on the
entire industrial control system network, not just on the
perimeter. Since personnel at nuclear facilities (and, in fact,
critical infrastructure more generally) too often concentrate
only on perimeter defense, allowing malware to operate
undetected if it is able to get past the perimeter, they need to
recognize that they must monitor all networks.
Most people focus all of their security on prevention and they do
very little for detection and containment. Network monitoring tends
to be on the perimeter and very little [on] any form of network
monitoring within the control system. So people need to monitor
all their networks, not only the perimeters.

In addition, encouraging the adoption of secure optical
data diodes where not already implemented would
significantly enhance cyber security. This is key given that
there are some nuclear facilities that may have only a
firewall to protect the industrial control system network.
With regard to supply chain challenges, the globalization
of manufacturing means that resolving vulnerability
remains difficult. However, some countries are taking
important steps towards the nationalization of their
supply chains (in the nuclear sector and beyond).

The best option for countries that lack the required
extensive national industry is to reduce their supply
chain vulnerability to the maximum extent possible.
Russia, for example, views the nationalization of its supply
chain as a priority, including in the nuclear sector. Given the
difficulty of manufacturing all of its products domestically,
in the short term Russia is seeking to reduce its dependency
on components manufactured in countries that it
considers ‘less friendly’; instead, it is substituting them with
components from China, which it considers a ‘more friendly’ country at present. Russia views this as an intermediate step
while it continues to build up its own national industry. In
the long term, it hopes to be able to replace the majority of
components with Russian products.
Throughout all of last year, there was a big discussion in Russia
about the need to urgently replace foreign components and
hardware with Russian ones. This attitude extends to all spheres
and sectors of the Russian economy.

Of course, for financial reasons it will be important for
nuclear facilities to identify the most crucial parts of the plant
from a cyber security perspective (notably, their critical cyber
assets) in order to grant those the highest levels of protection.
Prioritization of
the cyber risks is therefore key.

This chapter sets out a series of proposals for the development
of a response regime in the civil nuclear industry. This
regime would be aimed at mitigating cyber security problems
identified above, and at addressing others. It would be based
on an organizational methodology that is scalable and flexible,
and able to act with confidence and authority, but that would
be driven by the overriding need to keep providing nuclear sourced energy rather than by the sometimes commercially
restrictive requirements of the security profession.

In cyber security, organization is a prerequisite for
everything. Technological responses on their own
have failed. So have data-centric responses. Without
organization, communication and cooperative actions
involving stakeholders and individuals will always be
inefficient and ineffective. Without organization, a strategic
cyber response does not work.
Acting coherently, stakeholders involved in a future civil
nuclear cyber security regime should have as their goals to
turn the components of cyberspace that are key to achieving
strategic sectoral aims into a self-governing eco-system,
instead of, as now, an ungoverned environment made up of
disparate components, each engaged in tactical battles with
a variety of threats. The comprehension involved must also
reach beyond simply cyber security into physical security,
personnel security and safety. In addition, meaningful and
persistent dialogue between IT and OT stakeholders must
be incorporated as a fundamental necessity.
Cyber security is a multi-dimensional concept that cannot
readily be accommodated within traditional security policymaking. In the nuclear sector, both safety and physical security
measures have developed incrementally and in tandem over
time, but the rapidity in the development of cyber dependency
creates dissonance within a security regime. Three essential
components (physical, virtual and personnel) are evolving
at different rates, in terms of both threat manifestation
and countervailing capability development. This leads to a
twisting complexity in the management of overall security
(with additional complications of insider threats posing
problems across all areas of risk).
This environment must continue at all times to establish the
appropriate balance between regulated and self-determined
actions to avoid any tendency towards overall stagnation,
which is a condition attractive to organized groups and
individuals aiming to challenge the welfare of the nuclear
energy supply chain.

The various illicit uses of cyberspace amount to a system level challenge to the civil nuclear sector. As it is currently
configured, however, the sector does not act and respond as
a coherent eco-system where cyber security is concerned.
This is despite fifty years’ experience of developing a safety related (and more recently a security-related) culture.
Stakeholders in the nuclear cyber domain remain largely
segregated, despite having a satisfactory set of enabling
computer security policy documents that act as a potential
operational glue. As a result, agencies within the sector
may well fail to see that they are affected by another
stakeholder’s cyber security, or, more often, by the lack of
it. This is a matter of communication, both horizontally
between nuclear energy producers, but also vertically
throughout the entire supply chain.

At a simple level, the priorities for a cyber security
regime are nothing but traditional: deterrence, prevention,
detection and response. But it is how these activities are
coordinated that will set the tone of the nuclear sector’s
cyber security culture, closely allied by absolute necessity
to the safety culture already at the core of the industry.
Hitherto challenges in the cyber domain, no matter in
which industrial sector, have been managed generally by
a patchwork of technological responses. The nuclear sector
is no different from any other. However, there is ample
evidence to suggest that the main challenge, affecting the
entire sector, requires an equally far-reaching response
mechanism to achieve higher overall levels of security,
thus providing confidence in the use and maintenance
of electronic control and information systems. Without
appropriate controls in behaviour, the potential for
technology to deliver future efficiencies in the nuclear
energy life-cycle will become limited; the threat picture
begins to build, but awareness on its own does not act
as a catalyst for the technological design of operational
and defensive systems, which fail to keep pace with the
real world.
In order to attempt to correct this imbalance, cyber
security policy within the sector needs to be extended to
fuse two approaches: the largely reactive and bottom-up
concerns with computer and network security, along with
information security and assurance; and the top-down
approach driven by the needs of sector-level responsibility
to deliver nuclear-sourced energy safely and at commercial
prices. If this organizational transformation is achievable,
it should be possible to shape future cyber security policy
to align with strategic perspectives – primarily the needs
of the nuclear business, but also progressive strategies on governance and regulation, cost-effectiveness and,
particularly, inclusiveness.
The term cyber security and other related expressions
are widely used as though their meaning were clear and
incontrovertible, but the primary research for this report
confirms that there is no consistency in approach to cyber
issues across the international stage, and the nuclear
sector is no exception. The lack of an international cyber
lexicon continues to hinder multilateral responses in all
sectors, particularly when applied across linguistic barriers.
Even within individual states, the interpretation of ‘cyber’
can mask a range of inconsistencies and unanswered
questions, posing a serious difficulty for policy-makers and
those tasked with ensuring security. In several languages,
for example, there is only a single word for both safety
and security.

Even within individual states, the
interpretation of ‘cyber’ can mask a
range of inconsistencies and unanswered
questions, posing a serious difficulty for
policy-makers and those tasked with
ensuring security.
One way to achieve alignment across and within all
stakeholder groups might be to put one set of stakeholders
(such as the technical cadre) at the centre of the problem
and then organize the response around it. However, the
foundations of a more integrated and robust regime in any
sector require a common idea of cyber security – as regards
both the problem and responses to it. At the top of the IAEA
policy tree there is some very sound advice already being
developed, but at the centre of the downstream problem is
the lack of a common baseline in building a potential cyber
response. This makes development of a unified approach to
cyber issues all the more challenging. Creating a common
lexicon for cyber security, and hence a common threat
picture, as well as acknowledging differences in national
cultures in terms of risk management, security vetting
and operational responses, are all issues for further and
immediate consideration.

The threat that the nuclear sector faces in cyberspace is
fast-changing, sophisticated and potent, suggesting that
a response mechanism needs to be equally powerful and
agile. However, meeting an unresponsive and arguably
obsolete regulatory requirement being enacted in a highly
regulated environment (which is the cultural norm in
the nuclear industry) would only be counter-productive.
Such an approach would quickly strangle the vitality of a

potential response based on the mature culture that the
nuclear sector enjoys in the matter of safety. Instead, there
needs to be a well-judged and informed balance in policy,
regulation and communication. A potential solution would
be to support regulatory authorities with accredited specialist
cyber security expertise that can act with appropriate agility
and speed.
A strategy that shifts the risk of cyber-related harm to
proactive rather than reactive measures would serve to
deter cyber adversaries by increasing the degree of difficulty
they will encounter if attacking the sector. This approach
would deflect threats to easier targets, while also ensuring
that any determined adversaries would have to invest more
to achieve their aims. A nuclear-cyber security regime needs
to be put in place to make the sector hostile to saboteurs,
while maintaining the delicate balance between prescriptive
regulation and the empowerment of knowledgeable people
to take appropriate mitigating action where necessary.
This approach would need to be high on vision, doctrine
and knowledge, and moderate on control. The development
of cyberspace with its embedded insecurities will always
outpace any internationalized hierarchical structure
designed for policy development rather than operational
response. Such a regime would allow the fullest of freedoms
to those who have a role to play in countering risks to the
security of the sector, while also contributing to a broader
approach to cyber security through a policy of inclusiveness.
It will rely to a much greater extent on creating a shared
awareness of cyberspace, its threats and operating methods,
as well as the spectrum of available security capabilities,
including collective protection, to mitigate risk.

Such a security regime would have to incorporate a
technical response to address the issues described earlier
(such as patching or air gapping). This would fit into an
organizational approach to risk reduction, which can be
bought into action rapidly and uniformly to raise the level of
security to address critical vulnerabilities across the sector.
Thus an appropriate overall response would comprise an
eco-system in which the activities of different responding
agencies and bodies complement one another and are
mutually reinforcing, rather than conflicting; this would
include a very close cooperation with the safety cadre and
the physical security teams on sites. An approach to cyber
security that draws in a wide range of people from across
the sector and also further afield (national regulatory
bodies, for example), scarcely lends itself to centralized
control. Cyber security operations at nuclear facilities
therefore need to be self-informed, self-governing and
spontaneous, but to act within an agreed framework that is
coordinated more centrally, most probably by the IAEA.

There is limited evidence of cyber attacks on the civil
nuclear sector, most likely owing to lack of disclosure (as
is common in other reputation-sensitive sectors such as
finance, but also perhaps from a lack of discovery. Current
responses to the exploitation of cyberspace by adversaries
characteristically lack both agility and organization, making
it difficult to improve security systematically and efficiently.
Organized threats require organized responses by the
whole sector, which necessarily includes leadership at the
highest level (supported by up-to-date advisory bodies)
with energetic and knowledgeable inputs from the various
internal and external stakeholders. Technological capability
involved in protection, detection and response capability
needs proportionate investment, gearing to risk registers
and recognition of the guidance and recommendations for
cyber security that the IAEA is developing on behalf of its
member states.

The governance of cyber security in the sector has to
be able to promote debate around two key factors. First,
responses need to be managed in a way that creates a norm
that supports the use of information and communications
technology (ICT) to ensure safe and efficient nuclear
energy production, while increasing the difficulties
for threat actors.
Second, cyber security management must have a
collective dimension, involving all the key stakeholders
and organizations. Clearly, where vulnerabilities remain
in infrastructure protection or information assurance,
these are likely to be discovered (whether by accident
or design) and exploited by the ill-disposed. A collective
approach would enable cyber security to become a self-taught, dynamic process based on common operating
principles to counter evolving threats, and benefiting from
a doctrinal loop based on the ‘Boyd cycle’ of continual
process improvement (observe, orientate, decide, act). If
each stakeholder were to be given the opportunity to learn
from the experience of others, the overall level of cyber
security across a chosen sector should increase (and has
been demonstrated to do so in the UK financial services
through concepts such as the virtual task force).
Effective and durable responses in cyberspace therefore
require a shared awareness, an appetite for collaboration
and the development of an instinct for risk, which might
alternatively be described as a culture of cyber security. But
achievement of this relies once again on developing a truly
knowledgeable leadership at the very top of the eco-system,
and then within its subsections too.

To achieve absolute security in cyberspace would require
all threats, their toolsets and their attack paths to be
identified and isolated, and certain interactions to be
interdicted before they became critically dangerous.
However, taking into account the complexity of the
internet, the rapidity with which malware is developed
and the unpredictability of the human component of
the environment, such perfect security is a fantasy –
and perhaps not even a desirable one at that, given
the constraints that would place on the industrial
processes involved.
Thus the requirement for a civil nuclear cyber security
regime must be to manage and mitigate rather than
eliminate threats from cyberspace and to assess these
threats relative to vulnerabilities, the likelihood of an attack
and the potential impacts if an attack is successful. Cyber
security therefore becomes a matter of risk management,
within an environment in which the key element of the
responsive entity is the development of pace and agility.

A technological approach alone will not be sufficient to
resolve the complexity of the security space. An approach
to cyber security which is entirely or largely technological
will lack depth and deny the defender the ability to
develop an interlinked series of layers of security, each
representing another hurdle for an attacker to overcome.
Understanding the intersection between the technical,
human, organizational and regulatory aspects goes to the
heart of solving, or even merely mitigating, the problem
of cyber security in any sector, let alone the politically
sensitive theatre of nuclear energy production
Given the technological sophistication of the cyber medium,
the pace of change and the way in which user demand on the
internet catalyses high degrees of innovation, security within
ICT infrastructures could be seen as just too great a security
problem for analysts, industrialists and policy-makers.
This condition exists nationally, internationally and within
industrial sectors themselves, with complex sets of regulatory
authorities trying to make their respective marks on the
structure of cyber security within their own ambits. But
this does not necessarily have to be the case in the nuclear
sector, where there are fewer stakeholders, they are generally
acquainted with one another, and the culture of ‘collectivism’
is more clearly accepted (principally through the traditional
lens of ‘safety’). This fundamental principle in the delivery of
nuclear safety has the potential to extend to a complementary
development of cyber security in the sector, given the
appropriate push by regulators and the development of
a workable model of the response required.

Within this difficult concept of cyber security, some
capabilities can be identified as simply common sense,
aligned to general principles in risk management systems
where most resources are expended on the most critical
vulnerabilities. However, the philosophy of taking proactive
action to mitigate risks when they are identified rather
than focusing on responses when they occur – often called
‘left-shifting’ risk management (Jonas 2011) – also points
usefully to the less resource-intensive (and less expensive)
preventive activities of education, training and exercising.
The key features of the response will be agility and
initiative; and taking both an actor-neutral and a riskbased approach.
Agility and initiative. As described earlier, the range of cyber
threats is so broad and the spectrum of threat actors so
diverse that a ‘line in the sand’ cyber defense philosophy
will mean two things. First, the agile and intelligent (and
well-resourced) cyber adversary, who is unencumbered
by long-winded business processes, will enjoy a good deal
of initiative in the contest, and will not have to compete
particularly vigorously to gain or maintain the initiative.
Second (as is borne out by our interviews for this project),
the response to cyber threats will tend to be reactive
rather than anticipatory, with reaction only occurring
when an attack hits the firewall or is detected inside it (if
it is detected at all). In other words, the point at which
response mechanisms of the sector begin to address
cyber threats is when those threats are fully developed
and at their most powerful. Before such an event occurs,
attacks may even have been rehearsed on other sectors,
nationally or internationally, particularly those containing
a preponderance of industrial control systems. The nuclear
sector’s cyber security response should therefore seek to
be as agile as possible and should focus on unbalancing
an opponent by winning and maintaining the initiative,
and where possible activity should be intelligence-led. The
intelligence component, which includes horizon scanning,
research and development (R&D), and information from
other actors in the sector, thus helps to determine the
triggers which invoke the response mechanisms.
An actor-neutral approach. An ‘actor-neutral’ approach in
which capability is developed irrespective of particular
(and known) threat actors would be preferable, given that
these are so diverse and can change quite quickly. What
is important is the knowledge of what an adversary (any
adversary) could do, and to have the policies, procedures
and equipment necessary to meet (or anticipate) that
challenge, whatever its origin and whenever it occurs.
A risk-based approach. It would not be reasonable to expect
to eliminate all cyber threats permanently, nor would it be

possible to filter out all criminal or hostile use (actual or
potential) of the global ICT infrastructure. However, a risk based approach to cyber-security will: Indicate that legitimate use of ICT should not be
assumed to be free of plausible adverse consequences; Enable cyber security to be assessed on the basis of
proportionality: perceived benefits can be set against
possible penalties, and benefits can therefore be
prioritized; Encourage agility and adaptability: as cyber security
challenges evolve, priorities can be recalibrated; Allow cyber security policy to be framed at an overall
or system level, with risks and dangers in one sector
being offset by benefits and advantages in another.

A cyber security regime
In transforming cyber security management in the
direction proposed in this report, it becomes reasonable
and useful to describe these efforts as aspects of a sector level cyber security ‘regime’. Such a regime will define
a methodology to organize efforts through the national
and international development of enabling policy,
while acknowledging that successful cyber operations
will need to remain delegated to power plants via their
commercial parents.

A successful and durable regime is one
that functions intelligently and responsively
within its area of concern, remaining
absolutely current with the threat picture,
concomitant risks and the arsenal of
available countermeasures.
Any management system that remains centrally driven
or over-prescriptive would risk reducing pace and agility
in the response, leading to ever-widening capability
gaps between threats and responses and thus higher
risks. A successful and durable regime is one that
functions intelligently and responsively within its area
of concern, remaining absolutely current with the threat
picture, concomitant risks and the arsenal of available
countermeasures. The regime method offers the most
suitable basis for a sectoral cyber security strategy because
it can include and empower (not direct, as to do so would
cause resistance and impose delays) a wide variety of
actors, agencies and stakeholders. It can also be sufficiently
agile (yet without losing focus) to meet a rapidly evolving
and transforming security challenge.

Perhaps the greatest cyber security issue facing the nuclear
industry is that many in the sector do not fully understand
the risk, and therefore a key first step is to develop guidelines
to assess and measure this risk as accurately as possible. This
will help CEOs and company boards to understand what is at
stake, and also provide them with a clear economic rationale
to invest in cyber security. The development of cyber
insurance, with its strong reliance on risk metrics, may be an
important tool for promoting the development of cyber risk
guidelines. In tackling the challenges related to the ‘human
factor’, it will also be important to raise awareness among
both engineers and contractors of the risks involved in
setting up unauthorized connections or plugging in personal
USBs at nuclear facilities. Measures that promote disclosure
and information-sharing can also play an important role
in enhancing cyber security, as can regulatory standards
and other policy measures, improved communication
to bridge cultural divides and the implementation of
technical solutions.
The nuclear industry as a whole needs to develop a
more robust ambition to take the initiative in cyberspace
and to fund the promotion and fostering of a culture of
cyber security, determining investment priorities and
ensuring that sufficient and sustained funding is allocated
to effective responses to the challenge. It also needs to
establish an international cyber security risk management
strategy and encourage the free flow of information

between all stakeholders. This will require the industry to
develop appropriate mechanisms and coordinated plans
of action to address the technical shortfalls identified, as
well as to find the right balance between regulation and
personal responsibility.
The report has also highlighted some important areas for
future research. Given that developing countries have been
found to be particularly vulnerable, their specific needs
should be assessed so that resources can be allocated more
efficiently to combating the particular risks identified.
The apparent lack of preparedness for a large-scale cyber
security emergency, particularly one that occurs outside
normal working hours, also suggests that scenario-based
planning studies and exercises would lead to a better
understanding of how a situation might unfold in a crisis –
and to the development of effective response plans across
the industry.
